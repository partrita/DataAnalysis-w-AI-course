[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Doing Data Analysis with AI",
    "section": "",
    "text": "Gabor‚Äôs Data Analysis with AI"
  },
  {
    "objectID": "index.html#whats-this",
    "href": "index.html#whats-this",
    "title": "Doing Data Analysis with AI",
    "section": "What‚Äôs this",
    "text": "What‚Äôs this\nThis course will equip students, who are already versed in core data analysis methods, with experience to harness AI technologies to improve productivity (yes this is classic LLM sentence). But, yeah, the idea is to help students who studied data analysis / econometrics / quant methods and want to think about how to include AI in their analytics routine, and spend time to share experiences.\nAs AI becomes more and more powerful, it is also important to provide a platform to discuss human agency in data analysis. So a key element of the course and its instructor to lead discussions on the role of AI and humans in various aspects of data analysis.\nThis is the 2025 Spring edition release\n\nFor whom?\nThis material is aimed at data science and analytics instructors who can guide their students through the material sharing the experience and domain specific examples. All of this is open source, may be modified (see rights below).\nBut, dear reader (students, practitioners, researchers, journalists and more), you can also go alone, checking ideas and practice sessions.\n\n\nAI and me\nAt the end of all classes, instructors and students should always consider these three questions.\n\nHow did AI support me do what I planned.\nHow did AI fail me: gave half-truths, buggy code, imprecise arguments\nHow did AI extend me: helped do things I could not, or gave new ideas"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Doing Data Analysis with AI",
    "section": "Course description",
    "text": "Course description\n\nContent\nThe course focuses on using large language models (LLMs) such as OpenAI‚Äôs ChatGPT, Anthropic Claude.ai, Mistral‚Äôs Le Chat, and Google‚Äôs Gemini) to carry out tasks in data analysis. It includes topics like data extraction and wrangling, data exploration and descriptive statistics, and creating reports as well as turning text to data.\nThere are three case studies that we use (1) a simulated set of data tables on hotels in Austria, (2) The World Value Survey, and (3) A series of interview textst.\nThe course material includes weekly practice assignments. It also has a learning more collection ‚Äì to see what is beyond core material with blogs, papers, and video recommendations.\n\n\nBackground: data analysis / econometrics\nYou need a background in Data Analysis / Econometrics, a good introductory course is enough. I, of course, suggest Chapters 1-12 and 19 of Data Analysis for Business, Economics and Policy (Cambridge UP, 2021). Full slideshows, data and code are open source. But consider buying the book!ü§ù\nIn particular, the course builds on Chapters 1-6 and 7-10, and 19 of Data Analysis but other Introductory Econometrics + basics of data science knowledge is ok.\n\n\nBackground: coding\nStudents are expected to have some basic coding knowledge in Python or R (Stata also fine mostly).\n\n\nRelevance\nAI is everywhere and has become essential, most analytic work will be using it. It‚Äôs like the Internet a while back. Does not solve all problems, but almost all intellectual tasks will rely on inputs from it.\n\n\nLearning Outcomes\nKey outcomes. By the end of the course, students will be able to\n\nGain experience and confidence using genAI to carry out key tasks in data analysis.\nBuild AI in coding practice including data wrangling, description and reporting and text analysis\nHave some idea of use cases when AI assistance is (1) greatly useful, (2) helpful, (3) currently problematic.\nHave some idea of use cases when AI assistance is OK to use as is vs needs strong human supervision\nHave an understanding of resources to follow for updates.\n\n\n\nTarget audience\nThis is a course aimed at 3rd (2nd?) year BA and MA students in any program with required background. Economics, Quantitative Social science, Political Science, Sociology, History. To be frank, all students shall learn data analysis and be comfortable using AI.\nBut, anyone can use it with adequate background.\n\n\nCourse Context & Trajectory\nWhat‚Äôs Coming Next:\n\nWeek 0: Using AI for Coding ‚Äì this may not be part of the curriculum\nWeek 1: LLM Review and short task to recrate a graph\nWeek 2: Use AI to understand and document the World Values Survey dataset\nWeek 3: AI-assisted report writing with WVS data\nWeek 4: Multi-table joins with Austrian hotel data\nWeeks 5-6: Text analysis of football manager interviews\nWeek 7: Developing dashboards\nWeek 7: AI as applied metrics consultant\n\nSkills Development Arc:\n\nWeeks 0:  Background prep: Writing code with AI\nWeeks 1-2: Foundation (AI concepts, data exploration)\nWeeks 3-4: Core analysis (reporting, data wrangling)\n\nWeeks 5-6: Advanced applications (text analysis, APIs)\nWeeks 5-6: Helping research and presentation\n\nAssignments\nAssignments are available for all classes\nImportant to note for assignments: * Use AI but do not submit something that was created by AI. AI is your assistant. * One of the goals of the course is to practice this."
  },
  {
    "objectID": "index.html#week01-llm-review",
    "href": "index.html#week01-llm-review",
    "title": "Doing Data Analysis with AI",
    "section": "Week01: LLM Review",
    "text": "Week01: LLM Review\nWhat are LLMs, how is the magic happening. A non-technical brief intro. How to work with LLMs? Plus ideas on applications. Includes suggested readings, podcasts, and vids to listen to.\nContent\nWhich AI? See my take on current models. As of May 2025."
  },
  {
    "objectID": "index.html#week02-data-and-code-discovery-and-documentation-with-ai",
    "href": "index.html#week02-data-and-code-discovery-and-documentation-with-ai",
    "title": "Doing Data Analysis with AI",
    "section": "Week02: Data and code discovery and documentation with AI",
    "text": "Week02: Data and code discovery and documentation with AI\nLearn how to write a clear and professional code and data documentation. LLMs are great help once you know the basics.\nCase study: World Values Survey. Data is at WVS\nContent"
  },
  {
    "objectID": "index.html#week-03-writing-reports",
    "href": "index.html#week-03-writing-reports",
    "title": "Doing Data Analysis with AI",
    "section": "Week 03: Writing Reports",
    "text": "Week 03: Writing Reports\nYou have your data and task, and need to write a short report. We compare different options with LLM, from one-shot prompt to iteration.\nCase study: World Values Survey. Data is at WVS\nContent"
  },
  {
    "objectID": "index.html#week04-data-wrangling-joining-tables",
    "href": "index.html#week04-data-wrangling-joining-tables",
    "title": "Doing Data Analysis with AI",
    "section": "Week04: Data wrangling, joining tables",
    "text": "Week04: Data wrangling, joining tables\nWhen asked about what I shall add to my textbook, David Card, the Nobel winning empirical economist told me that I shall spend time with joining tables. Here we go.\nCase study: simulated Austrian hotels. Data is at hotels\nContent"
  },
  {
    "objectID": "index.html#week05-text-as-data-1-intro-lecture",
    "href": "index.html#week05-text-as-data-1-intro-lecture",
    "title": "Doing Data Analysis with AI",
    "section": "Week05: Text as data 1 ‚Äì intro lecture",
    "text": "Week05: Text as data 1 ‚Äì intro lecture\nNo course of mine can escape football (soccer). Here we look at post-game interviews to learn basics of text analysis and apply LLMs in what they are best - context dependent learning. Two class series. First is more intro to natural language processing.\nCase study: football post-game interviews. Data is at interviews\nContent"
  },
  {
    "objectID": "index.html#week06-text-as-data-2-practice",
    "href": "index.html#week06-text-as-data-2-practice",
    "title": "Doing Data Analysis with AI",
    "section": "Week06: Text as data 2 ‚Äì practice",
    "text": "Week06: Text as data 2 ‚Äì practice\nSecond class, now we are in action. How does LLM compare to humans?\nCase study: football post-game interviews. Data is at interviews\nContent"
  },
  {
    "objectID": "index.html#learn-more",
    "href": "index.html#learn-more",
    "title": "Doing Data Analysis with AI",
    "section": "Learn more",
    "text": "Learn more\nI‚Äôm adding material to learn-more folder. You can start with the beyond page."
  },
  {
    "objectID": "index.html#rights-and-acknowledgement",
    "href": "index.html#rights-and-acknowledgement",
    "title": "Doing Data Analysis with AI",
    "section": "Rights and acknowledgement",
    "text": "Rights and acknowledgement"
  },
  {
    "objectID": "index.html#you-can-use-it-to-teach-and-learn-freely",
    "href": "index.html#you-can-use-it-to-teach-and-learn-freely",
    "title": "Doing Data Analysis with AI",
    "section": "You can use it to teach and learn freely",
    "text": "You can use it to teach and learn freely\nAttribution: B√©k√©s, G√°bor: ‚ÄúDoing Data Analysis with AI: a short course‚Äù, available at gabors-data-analysis.com/ai-course/, v1.0. 2025-06-04.\nYou can fork it from the Github Repo. github.com/gabors-data-analysis/da-w-ai/\nLicense: CC BY-NC-SA 4.0 ‚Äì share, attribute, non-commercial (contact me for corporate gigs)\nTextbook Please check out the textbook behind all this, buy it if you can. If interested teaching contact the Cambridge UP or me."
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "Doing Data Analysis with AI",
    "section": "Thanks",
    "text": "Thanks\nThanks: Developed mostly by me, G√°bor B√©k√©s Thanks a million to the two wonderful human RAs, Ms Zsuzsanna Vadle and Mr Kenneth Colombe, both Phd students. Thanks to Claude.ai that did a great deal of help in creating the simulated dataset. ChatGPT and Claude.ai helped create the slideshows and educated me on NLP. This is a beatiful example of collaboration with great young people while heavily benefiting from advanced AI.\nThanks for CEU‚Äôs teaching grant that allowed me pay people and AI.\nThanks for Quarto ‚Äì it was all drafted and written in Quarto and Rstudio by Posit."
  },
  {
    "objectID": "index.html#questions-and-suggestions",
    "href": "index.html#questions-and-suggestions",
    "title": "Doing Data Analysis with AI",
    "section": "Questions and suggestions",
    "text": "Questions and suggestions\nThis material is based my course at CEU in Vienna, Austria. Here is the Github repo\nIf you have questions or suggestions or interested to learn more, just fill in this form."
  },
  {
    "objectID": "index.html#and-now-this.",
    "href": "index.html#and-now-this.",
    "title": "Doing Data Analysis with AI",
    "section": "And now, this.",
    "text": "And now, this.\nAI use is very costly in terms of energy. Yes, it is becoming cheaper. But humanity is also using much more of it."
  },
  {
    "objectID": "weeks.html",
    "href": "weeks.html",
    "title": "Weekly Content",
    "section": "",
    "text": "Using AI for code. May not be covered in this class, as it had often been already covered in coding classes.\nContent"
  },
  {
    "objectID": "weeks.html#week01-llm-review",
    "href": "weeks.html#week01-llm-review",
    "title": "Weekly Content",
    "section": "Week01: LLM Review",
    "text": "Week01: LLM Review\nWhat are LLMs, how is the magic happening. A non-technical brief intro. How to work with LLMs? Plus ideas on applications. Includes suggested readings, podcasts, and vids to listen to.\nContent\nWhich AI? See my take on current models. As of May 2025."
  },
  {
    "objectID": "weeks.html#week02-data-and-code-discovery-and-documentation-with-ai",
    "href": "weeks.html#week02-data-and-code-discovery-and-documentation-with-ai",
    "title": "Weekly Content",
    "section": "Week02: Data and code discovery and documentation with AI",
    "text": "Week02: Data and code discovery and documentation with AI\nLearn how to write a clear and professional code and data documentation. LLMs are great help once you know the basics.\nCase study: World Values Survey. Data is at WVS\nContent"
  },
  {
    "objectID": "weeks.html#week-03-writing-reports",
    "href": "weeks.html#week-03-writing-reports",
    "title": "Weekly Content",
    "section": "Week 03: Writing Reports",
    "text": "Week 03: Writing Reports\nYou have your data and task, and need to write a short report. We compare different options with LLM, from one-shot prompt to iteration.\nCase study: World Values Survey. Data is at WVS\nContent"
  },
  {
    "objectID": "weeks.html#week04-data-wrangling-joining-tables",
    "href": "weeks.html#week04-data-wrangling-joining-tables",
    "title": "Weekly Content",
    "section": "Week04: Data wrangling, joining tables",
    "text": "Week04: Data wrangling, joining tables\nWhen asked about what I shall add to my textbook, David Card, the Nobel winning empirical economist told me that I shall spend time with joining tables. Here we go.\nCase study: simulated Austrian hotels. Data is at hotels\nContent"
  },
  {
    "objectID": "weeks.html#week05-text-as-data-1-intro-lecture",
    "href": "weeks.html#week05-text-as-data-1-intro-lecture",
    "title": "Weekly Content",
    "section": "Week05: Text as data 1 ‚Äì intro lecture",
    "text": "Week05: Text as data 1 ‚Äì intro lecture\nNo course of mine can escape football (soccer). Here we look at post-game interviews to learn basics of text analysis and apply LLMs in what they are best - context dependent learning. Two class series. First is more intro to natural language processing.\nCase study: football post-game interviews. Data is at interviews\nContent"
  },
  {
    "objectID": "weeks.html#week06-text-as-data-2-practice",
    "href": "weeks.html#week06-text-as-data-2-practice",
    "title": "Weekly Content",
    "section": "Week06: Text as data 2 ‚Äì practice",
    "text": "Week06: Text as data 2 ‚Äì practice\nSecond class, now we are in action. How does LLM compare to humans?\nCase study: football post-game interviews. Data is at interviews\nContent"
  },
  {
    "objectID": "week06/index.html",
    "href": "week06/index.html",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "",
    "text": "Week 06: Sentiment Analysis with AI\n\n\nUsing API to AI to create data from text",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#overview",
    "href": "week06/index.html#overview",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Overview",
    "text": "Overview\nContinue using text for research with AI. Still on sentiment analysis but now we focus on scaling text analysis with APIs: from 20 texts to hundreds and more.\n\nLearning Outcomes\nBy the end of the session, students will:\n\nGain hands-on experience with sentiment analysis.\nHave experience integrating NLP in research\nThink about what is ground truth\nGet a taste of building an analytical pipeline with API usage",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#using-apis",
    "href": "week06/index.html#using-apis",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Using APIs",
    "text": "Using APIs\n\nBasics and setup\nHow to use APIs\nGet an API Key (ChatGPT and Claude)\n\n\nExamples\nSimple walkthrough with GDP data ‚Äì uses World Bank and FRED APIs\nBit harder walkthrough with football data ‚Äì uses FBREF soccer data. Guess the club for example.\n\n\nMore advanced stuff\nMore advanced knowledge on APIs ‚Äì how APIs work\n\n\nMaterials\nDatasets\n\ntexts (text_id level)\ngames info (such as results, text_id level)\nclass-ratings (human, AI ratio, text_id*student level)\ndomain-rating (text_id level)\nclass-rating-aggregated (text_id level)\n\ncode",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#preparation",
    "href": "week06/index.html#preparation",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Preparation",
    "text": "Preparation\n\nDownload the combined data from Moodle\n\nNote: win, draw ‚Äì need encode loss",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#class-tasks",
    "href": "week06/index.html#class-tasks",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Class tasks",
    "text": "Class tasks\n\nDiscussion 1\n\nYour experience regarding human vs ai ratings.\nWhat was difficult and easy as human rater\n\n\n\nData Analysis\n\nTake the aggregated file and ask AI for a readme. Discuss what is in the data\nCompare human, domain lexicon and AI rating. For human and AI take the average.\nThink of an interesting comparison using AI rating\nCompare results by human and lexicon rating\n\n\n\nDiscussion 2\n\nWhat is ground truth\n\n\n\nHow to integrate AI into research\n\ncombine data with text\nthink RQ and how you‚Äôd use AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#additional-tasks-if-time-permits",
    "href": "week06/index.html#additional-tasks-if-time-permits",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Additional tasks if time permits",
    "text": "Additional tasks if time permits\n\npredict gender and result\n\nShow AI all texts and ask to predict the gender of speaker\nShow AI all texts and ask to predict the result (manager‚Äôs team won, drew, lost)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/assets/walkthrough-fbref.html",
    "href": "week06/assets/walkthrough-fbref.html",
    "title": "Walkthrough: Using soccerdata to Fetch Arsenal‚Äôs 2023‚Äì24 Match Stats",
    "section": "",
    "text": "Below is a simple, step-by-step recipe for pulling Arsenal‚Äôs match-by-match team statistics for the 2023‚Äì24 Premier League season from FBref, using the soccerdata Python client library. You never write raw HTTP requests‚Äîsoccerdata handles those for you."
  },
  {
    "objectID": "week06/assets/walkthrough-fbref.html#r-equivalent",
    "href": "week06/assets/walkthrough-fbref.html#r-equivalent",
    "title": "Walkthrough: Using soccerdata to Fetch Arsenal‚Äôs 2023‚Äì24 Match Stats",
    "section": "R equivalent",
    "text": "R equivalent\n# 1. Install and load the package\ninstall.packages(\"worldfootballR\")  # different package, same idea\nlibrary(worldfootballR)\n\n# 2. Get Arsenal match results from the Premier League 2023/24\narsenal_matches &lt;- fb_team_match_results(\n  team_url = \"https://fbref.com/en/squads/18bb7c10/Arsenal-Stats\")\n\n# 3. Inspect the data\nhead(arsenal_matches)"
  },
  {
    "objectID": "week06/assets/api-use.html",
    "href": "week06/assets/api-use.html",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "",
    "text": "Previously, we manually examined a sample of just 20 texts and tried using an LLM for sentiment analysis. How long did this take you? Would it still be doable if there were 75 texts? Likely, yes. However, imagine you have 10,000 texts to analyze for sentiment. Analyzing 10,000 texts one by one (or copying them into a tool manually) would be nearly impossible ‚Äì it would take endless hours and be prone to error. We need a way to automate and scale the process. This is where APIs come in. By leveraging an API, we can send those thousands of texts to a powerful external service that analyzes sentiment and returns results in seconds. A nice recent example of this in economics research is a recent working paper that used as data over 1,400 American life narratives from the 1930s to uncover common themes about what it means to live a meaningful life.\nOur baseline approach is for Python, but R is pretty similar (see below)"
  },
  {
    "objectID": "week06/assets/api-use.html#why-apis",
    "href": "week06/assets/api-use.html#why-apis",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "",
    "text": "Previously, we manually examined a sample of just 20 texts and tried using an LLM for sentiment analysis. How long did this take you? Would it still be doable if there were 75 texts? Likely, yes. However, imagine you have 10,000 texts to analyze for sentiment. Analyzing 10,000 texts one by one (or copying them into a tool manually) would be nearly impossible ‚Äì it would take endless hours and be prone to error. We need a way to automate and scale the process. This is where APIs come in. By leveraging an API, we can send those thousands of texts to a powerful external service that analyzes sentiment and returns results in seconds. A nice recent example of this in economics research is a recent working paper that used as data over 1,400 American life narratives from the 1930s to uncover common themes about what it means to live a meaningful life.\nOur baseline approach is for Python, but R is pretty similar (see below)"
  },
  {
    "objectID": "week06/assets/api-use.html#what-is-an-api",
    "href": "week06/assets/api-use.html#what-is-an-api",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "What is an API?",
    "text": "What is an API?\nAn API (Application Programming Interface) is like a messenger or middleman that lets two different programs talk to each other and exchange information. Instead of a person directly doing a task, you have one software program asking another program to do something on its behalf. A popular analogy is that an API is similar to a restaurant waiter:\n\nYou (the client) are sitting at a table, ready to order a meal (you have a request for information or a service).\nThe waiter (the API) takes your order and relays it to the kitchen. You don‚Äôt go into the kitchen yourself ‚Äì the waiter is the go-between.\nThe kitchen (the server) is where the work happens. The chef prepares the meal (the data or service you requested).\nThe waiter (API) returns with your meal and serves it to you. You get exactly what you ordered, without having to know how the kitchen prepared it.\n\nIn this analogy, the restaurant‚Äôs menu is like the API documentation ‚Äì it lists what you can ask for and how to ask for it. If you request something not on the menu, the waiter (API) will tell you it‚Äôs not available (an error). Similarly, an API provides a set of rules and endpoints that define what requests can be made and what responses you can expect.\nThis means you don‚Äôt need to know the complex inner workings of the server or service. You just need to know what to ask for and how to ask for it through the API. The API handles the communication, just as the waiter handles communication between you and the kitchen."
  },
  {
    "objectID": "week06/assets/api-use.html#benefits-of-using-apis-in-data-analysis",
    "href": "week06/assets/api-use.html#benefits-of-using-apis-in-data-analysis",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Benefits of Using APIs in Data Analysis",
    "text": "Benefits of Using APIs in Data Analysis\nWhy use APIs as a data analyst? Here are some key benefits:\n\nScalability: APIs let you process large volumes of data quickly. You can automate requests in code, so analyzing 10,000 texts or more becomes feasible. Instead of manually working with each piece of data, you let a server handle the heavy lifting.\nAccess to Powerful Tools: Many companies provide APIs for advanced services like sentiment analysis, language translation, image recognition, or data storage. As a data analyst, you can tap into these pre-built models and services without having to develop them from scratch.\nTime and Effort Savings: Using an API, you can perform complex tasks with just a simple request. This saves you the time of writing extensive code or doing repetitive work. For example, rather than writing your own sentiment analysis algorithm, you can send text to an API and get sentiment results immediately.\nIntegration of Data Sources: APIs allow different software and datasets to integrate. You can pull data from different sources (e.g.¬†Twitter‚Äôs API for tweets, a weather API for climate data) directly into your analysis pipeline. This marries data from multiple sources seamlessly.\nConsistency and Reliability: When you use a well-established API, you benefit from a service that‚Äôs been tested and optimized. The API will handle errors, edge cases, and updates, so you get consistent results. It‚Äôs like outsourcing a task to an expert ‚Äì you trust the API to do its job correctly."
  },
  {
    "objectID": "week06/assets/api-use.html#api-keys-and-authentication",
    "href": "week06/assets/api-use.html#api-keys-and-authentication",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "API Keys and Authentication",
    "text": "API Keys and Authentication\nMost APIs require some form of authentication to ensure that only authorized users or applications can use them. The simplest form is an API key. An API key is like a secret password or ID that you include with your API calls:\n\nYou typically get an API key by creating an account or registering an application with the API provider. For example, to use the Twitter API or OpenAI API, you‚Äôd sign up and receive a key (or token).\nThe key itself is usually a long string of characters (letters, numbers, and symbols). It‚Äôs unique to you or your application.\nYou include this key with every request. Often it goes in a request header (for instance, you might set a header Authorization: Bearer YOUR_API_KEY), or sometimes as a URL parameter (e.g., ?api_key=YOUR_API_KEY in the query string). The API documentation will tell you exactly how to include the key.\nThe server checks the key. If the key is missing or wrong, the API will usually respond with an authentication error (like a 401 Unauthorized status). If the key is valid, the server will proceed to handle your request.\nSecurity tip: Never share your API keys publicly or commit them to public repositories. They are meant to be kept secret. If someone obtains your key, they could use the API pretending to be you, which might violate usage limits or incur costs on your account.\n\nSome services use more complex authentication (like OAuth tokens which have limited scope or expiration), but an API key is the fundamental concept to understand first. It‚Äôs your access credential for using the API."
  },
  {
    "objectID": "week06/assets/api-use.html#get-an-api",
    "href": "week06/assets/api-use.html#get-an-api",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Get an API",
    "text": "Get an API\nNext, you can go and get an API key for an AI service. As a start a few dollars will be enough. Follow instructions"
  },
  {
    "objectID": "week06/assets/api-use.html#walkthrough-examples",
    "href": "week06/assets/api-use.html#walkthrough-examples",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Walkthrough examples",
    "text": "Walkthrough examples\n\nGetting GDP data from World Bank and FRED\nMore advanced Football data (python, R)"
  },
  {
    "objectID": "week06/assets/api-use.html#r",
    "href": "week06/assets/api-use.html#r",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "R",
    "text": "R\nClient libraries in other languages: While our focus is on Python, other programming languages provide similar conveniences. In R, for example, packages like httr (for making HTTP requests) and jsonlite (for parsing JSON) are commonly used to work with web APIs. Many APIs also have R packages or wrappers that function like client libraries, letting you call the API in one or two lines of R code. The core idea is the same: a client library abstracts the RESTful requests into native language functions. Regardless of language, using a client library means you can integrate an API into your data analysis or application with less hassle, letting you focus on interpreting results rather than the mechanics of HTTP."
  },
  {
    "objectID": "week06/assets/api-use.html#scaling-up-with-apis-from-20-to-10000-and-beyond",
    "href": "week06/assets/api-use.html#scaling-up-with-apis-from-20-to-10000-and-beyond",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "Scaling Up with APIs: From 20 to 10,000 and Beyond",
    "text": "Scaling Up with APIs: From 20 to 10,000 and Beyond\nThe introduction of APIs into your workflow transforms what you can accomplish:\n\nTasks that were infeasible by hand become trivial to automate. You could get results in minutes or hours rather than weeks.\nYou can harness powerful algorithms provided by industry leaders. For example, instead of developing your machine learning model, you can use Google‚Äôs vision API to tag images or OpenAI‚Äôs language API to summarize text. This means you can tackle complex problems without needing to be an expert in those specific subfields.\nYou can work with real-time and large-scale data. Want to analyze football statistics or financial market data? There are APIs to fetch those streams of information. With APIs, you are not limited to data you can collect manually; you can pull in data from all over the world programmatically.\n\nAPIs are a bridge to practically unlimited data and capabilities. They let your programs communicate with other services to get things done efficiently. As we continue this course, you‚Äôll get hands-on experience using APIs ‚Äì turning the concepts you learned here into actual data analysis tasks. Embrace this new tool in your skillset. Whenever you find yourself needing to scale up or access a specialized service, think: Is there an API for that? Chances are, the answer will be yes, and now you‚Äôll know how to use it!\nMore advanced and supplementary information."
  },
  {
    "objectID": "week05/assets/sentiment-scale.html",
    "href": "week05/assets/sentiment-scale.html",
    "title": "General Sentiment Rating Guidelines",
    "section": "",
    "text": "Please read each text carefully and rate the overall sentiment of the manager‚Äôs statement as positive or negative. Your rating should reflect the manager‚Äôs expressed tone, not your judgment of the match."
  },
  {
    "objectID": "week05/assets/sentiment-scale.html#task",
    "href": "week05/assets/sentiment-scale.html#task",
    "title": "General Sentiment Rating Guidelines",
    "section": "",
    "text": "Please read each text carefully and rate the overall sentiment of the manager‚Äôs statement as positive or negative. Your rating should reflect the manager‚Äôs expressed tone, not your judgment of the match."
  },
  {
    "objectID": "week05/assets/sentiment-scale.html#rating-scale",
    "href": "week05/assets/sentiment-scale.html#rating-scale",
    "title": "General Sentiment Rating Guidelines",
    "section": "Rating Scale:",
    "text": "Rating Scale:\n\n\n\n\n\n\n\nScore\nMeaning\n\n\n\n\n2\nStrongly positive sentiment (clear optimism, satisfaction, praise).\n\n\n1\nMildly positive sentiment (generally positive, slight reservations).\n\n\n0\nNeutral or unclear sentiment.\n\n\n-1\nMildly negative sentiment (general disappointment, frustration).\n\n\n-2\nStrongly negative sentiment (clear criticism, significant disappointment)."
  },
  {
    "objectID": "week04/index.html",
    "href": "week04/index.html",
    "title": "Week 04: Joining tables",
    "section": "",
    "text": "Week 04: Joining tables\n\n\nCombining (joining) data tables, and understanding what can go wrong with AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#about-the-class",
    "href": "week04/index.html#about-the-class",
    "title": "Week 04: Joining tables",
    "section": "About the class",
    "text": "About the class\nData wrangling is the process of preparing data for the analysis. A key task is joining data tables. It looks simple, but in practice, has many challenges. AI can help design the process, and give you code to do that.\n\nObjectives summary:\nLearn how to organize data in a tidy way, join multiple datasets, choose variables to answer a research question and create a reproducible workflow to analyze data.\n\n\nLearning Objectives\n\nLearn how to store information in a tidy way.\nWork with relational data.\nJoin tables\nUse AI to explain complex concepts\n\n\n\nBefore class\n\nBackground reading: B√©k√©s-K√©zdi (2021) Chapter 2\nDownload data-modified.zip from Here. Unzip. It is a set of csv files such as ‚Äòcities_modified‚Äô\n\nAlso available on Moodle\n\nThe data description is available here ‚Äì have a look before class",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#class-plan",
    "href": "week04/index.html#class-plan",
    "title": "Week 04: Joining tables",
    "section": "Class Plan",
    "text": "Class Plan\n\n\nü§ù Recap and dicussion (10 min)\n\n\nRecap\nDiscuss assignment 03 (20 mins by groups + 10 mins together)\n\nCreate 3-4-member groups. Each groups will read reports by an another team (1‚Äì&gt;2, 2‚Äì&gt;3, ‚Ä¶ , N‚Äì&gt;1)\nRead the other team‚Äôs submissions with a ‚Äòreader‚Äôs perspective‚Äô and take notes.\nWhich report did you like the most and why\nRank reports in terms of how much AI was involved from low to high and note suspicious examples\nNote examples where AI usage was obvious (even made things worse) vs.¬†well-integrated.\n\n\n\n\n\nü§ù Understanding challenges (20 min)\n\n\nTask 1: Use AI to understand these terms. Ask examples. (Individual)\n\ntidy data table\nrelational datasets,\n\nschema,\nprimary and foreign key\ncomposite key\n\njoining tables\n\ndifferent types of join\n1:1, 1:m\n\njoining tables in your language (python, R, Stata)\ndifficulties of generating primary keys\n\nThis is followed by a discussion.\n\n\n\n\nü§ù Using AI help join tables (30 min)\n\n\n\nTask 2: Form 2-3 groups of people using same coding language\nUse the data you downloaded to carry out joins and inspect results. Use AI but inspect.\n\n\n1:1\n\nJoin hotels and cities. Compare left, right, inner, outer joins.\n\n\nwhat happens to N?\ncompare all four join types - which hotels/cities disappear with each approach?\n\n\n\n1:m\n\nStart: Tabulate the frequency of hotels by city_hotel_counts\nCities to Hotels\n\n\none city joins to multiple hotels\nfilter on 2 cities for easier visibility\n\n\nJoin hotel and occupancy 1\n\n\nm:1\n\n\nJoin hotel and occupancy 2\n\n\nget hotel level\ntrick: aggregate\n\n\nJoin on composite key\n\n\ncreate a data table at city-year-match level showing average occupancy and tourist arrivals\ncan you include economic indicators by month?\n\n\n\nAdvice, ideas\n\ndiscuss and collect ideas from AI\nlearn to focus on key suggestions (AI can go nuanced and not important points easily)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#task-2-form-2-3-groups-of-people-using-same-coding-language",
    "href": "week04/index.html#task-2-form-2-3-groups-of-people-using-same-coding-language",
    "title": "Week 04: Joining tables",
    "section": "Task 2: Form 2-3 groups of people using same coding language",
    "text": "Task 2: Form 2-3 groups of people using same coding language\nUse the data you downloaded to carry out joins and inspect results. Use AI but inspect.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#section",
    "href": "week04/index.html#section",
    "title": "Week 04: Joining tables",
    "section": "1:1",
    "text": "1:1\n\nJoin hotels and cities. Compare left, right, inner, outer joins.\n\n\nwhat happens to N?\ncompare all four join types - which hotels/cities disappear with each approach?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#m",
    "href": "week04/index.html#m",
    "title": "Week 04: Joining tables",
    "section": "1:m",
    "text": "1:m\n\nStart: Tabulate the frequency of hotels by city_hotel_counts\nCities to Hotels\n\n\none city joins to multiple hotels\nfilter on 2 cities for easier visibility\n\n\nJoin hotel and occupancy 1\n\n\nm:1\n\n\nJoin hotel and occupancy 2\n\n\nget hotel level\ntrick: aggregate\n\n\nJoin on composite key\n\n\ncreate a data table at city-year-match level showing average occupancy and tourist arrivals\ncan you include economic indicators by month?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#advice-ideas",
    "href": "week04/index.html#advice-ideas",
    "title": "Week 04: Joining tables",
    "section": "Advice, ideas",
    "text": "Advice, ideas\n\ndiscuss and collect ideas from AI\nlearn to focus on key suggestions (AI can go nuanced and not important points easily)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#home-assignment",
    "href": "week04/index.html#home-assignment",
    "title": "Week 04: Joining tables",
    "section": "Home Assignment",
    "text": "Home Assignment\nSuggested assignment [/assignments/assignment_04]",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#end-of-week-discussion-points",
    "href": "week04/index.html#end-of-week-discussion-points",
    "title": "Week 04: Joining tables",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nHow useful was AI in teaching skills?\nHow useful was AI in actually joining tables?\nHow can you debug what AI did in terms of executing code?\nWhen joins produced unexpected row counts, how did you diagnose the problem? What AI prompts helped most?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week03/index.html",
    "href": "week03/index.html",
    "title": "Week 03: Reporting your data analysis",
    "section": "",
    "text": "Week 03: Reporting your data analysis\n\n\nCreating inputs and organizing a short data analytics report with AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Writing Reports"
    ]
  },
  {
    "objectID": "week03/index.html#objectives",
    "href": "week03/index.html#objectives",
    "title": "Week 03: Reporting your data analysis",
    "section": "Objectives",
    "text": "Objectives\n\nSummary:\nHow to organize a short data analytics report? The job includes choosing and creating relevant plots, running regression. The class will exlopre how we can use AI to assist in these tasks, and get the distinction between using AI as input vs.¬†output in report writing.\n\n\nDetails\n\nUnderstand how to connect an empirical question to data\nCreate relevant visualizations and tables using AI.\nLearn to critically assess reports with the help of AI tools. We‚Äôll especially focus on half-truths: convincing, looks okay, is okay-ish, but not 100% true",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Writing Reports"
    ]
  },
  {
    "objectID": "week03/index.html#preparation-before-class",
    "href": "week03/index.html#preparation-before-class",
    "title": "Week 03: Reporting your data analysis",
    "section": "Preparation BEFORE class",
    "text": "Preparation BEFORE class\n\nBackground reading: B√©k√©s-K√©zdi (2021) Chapters 3-4, 7-10. Pay special attention to Chapter 4‚Äôs discussion of good vs.¬†bad graphs and Chapter 10 on interpreting regression coefficients precisely.\nDownload the WVS_GDP_merged_data.csv. This is an aggregated, cleaned subset of the 7th Wave of WVS dataset merged with GDP data from World Bank\n\n\nThe data\n\nThis is aggregated data: country level\nYear: Wave 7 of the WVS ‚Äì survey was conducted at different years.\nCombined with World Bank data: at year when survey was conducted\nGDP: level USD, level USD PPP, level USD PPP per capita.\npopulation",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Writing Reports"
    ]
  },
  {
    "objectID": "week03/index.html#class-plan",
    "href": "week03/index.html#class-plan",
    "title": "Week 03: Reporting your data analysis",
    "section": "Class plan",
    "text": "Class plan\n\n\nü§ù Review on readmes (30 min)\n\n\n\nReview Readme assignment\n\nQ+A\nCommon AI errors in data documentation?\nVerify, verify, verify (AI is good but rarely perfect)\n\n\n\nStarting presentation about prompting\nslideshow\n\n\nHow does a good report look like?\n\nHow to write a good short report: structure\ngood graphs and tables\n\nMake sure precise language. Recap on causal language.\n\n\n\n\nProblems with AI generated reports.\nUse AI as input (like advanced google search) not as output writer, because\n\nCreates ‚Äúaverage‚Äù / generic / bland / repetitive text\nConvincing but not precise\nNot your style and not your exact plan\nToo broad (like adds further research)\nnot precise enough, especially using causal language inappropriately.\n\n\n\n\n\n\nü§ù No AI vs AI prompting strategies to create a report (50 min)\n\n\n\nNO AI\nForm 2-3 member groups freely\n\nEach group: Choose one these pre-defined research questions:\n\n\nIs there a relationship between income level and trust?\nIs there a relationship between income level and happiness?\nIs there a relationship between income level and gender attitudes?\n\n\nChoose the relevant variables to answer your question (you can use AI to understand variables like in week2 (Use your Week 2 AI skills to understand complex variable definitions.)\nDesign a plan for a report on the topic: list of exhibits (graphs, tables). Do not write code (yet)\nDiscuss plans\n\n\n\nAI 1\nTry get a report with a single prompt. * Hint 1: translate your plan into a prompt using ideas from the intro. * Hint 2: Look for impressive-looking but problematic results.\n\n\nAI 2\n\nShowcase an iterative process where key exhibits are created\nHint 3: get AI to create precisely the exhibits you designed, not what it thinks you need.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Writing Reports"
    ]
  },
  {
    "objectID": "week03/index.html#end-of-week-discussion-points",
    "href": "week03/index.html#end-of-week-discussion-points",
    "title": "Week 03: Reporting your data analysis",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nCompare single and multi-step approach generating reports?\nHow good is AI in creating good enough vs exactly as planned graphs?\nWhat is happiness? :-)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Writing Reports"
    ]
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html",
    "href": "week03/assets/trust_income_report_long.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#motivation",
    "href": "week03/assets/trust_income_report_long.html#motivation",
    "title": "",
    "section": "1. Motivation",
    "text": "1. Motivation\nTrust lubricates social and economic interaction. Classical growth models treat trust as an informal institution that lowers transaction costs; empirical work often finds that high‚Äëtrust societies grow faster and innovate more. Here we ask a simpler descriptive question: are richer countries also more trusting?"
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#data",
    "href": "week03/assets/trust_income_report_long.html#data",
    "title": "",
    "section": "2. Data",
    "text": "2. Data\n\nCoverage: 64 country‚Äësurveys from WVS Wave¬†7 (2017‚Äë2022).\nIncome: GDP per capita, PPP‚Äëadjusted USD (World Bank).\nTrust:\n‚Ä¢ Baseline ‚Äì share answering ‚ÄúMost people can be trusted‚Äù (Q57).\n‚Ä¢ Alternative ‚Äì average of Q59‚ÄìQ63 (trust in neighbours, acquaintances, first‚Äëtime met, other religion, other nationality), reverse‚Äëcoded."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#exploratory-pattern",
    "href": "week03/assets/trust_income_report_long.html#exploratory-pattern",
    "title": "",
    "section": "3. Exploratory pattern",
    "text": "3. Exploratory pattern\n\n\n\nTrust‚Äëincome scatter\n\n\nBoth the LOESS smoother and the dashed OLS line suggest a positive association. Annotated points mark extreme income positions (rich: MAC, SGP, NLD; poor: ETH, ZWE, TJK) and trust outliers."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#regression-analysis",
    "href": "week03/assets/trust_income_report_long.html#regression-analysis",
    "title": "",
    "section": "4. Regression analysis",
    "text": "4. Regression analysis\n\nBaseline trust\nŒ≤‚ÇÅ¬†=¬†0.104 (SE¬†0.018, p¬†=¬†0.000, R¬≤¬†=¬†0.35)\n\n\nAlternative trust index\nŒ≤‚ÇÅ¬†=¬†0.137 (SE¬†0.030, p¬†=¬†0.000, R¬≤¬†=¬†0.25)"
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#interpretation",
    "href": "week03/assets/trust_income_report_long.html#interpretation",
    "title": "",
    "section": "5. Interpretation",
    "text": "5. Interpretation\nA one‚Äëunit rise in log‚Äëincome (‚âà‚ÄØ2.7√ó GDP per capita) is associated with a 0.104-point increase in the share trusting (scale¬†0‚Äì1). While not large, this accounts for roughly 35% of cross‚Äëcountry variation. Channels could include better governance, education and formal institutions in high‚Äëincome settings, which foster generalised trust. Reverse causality‚Äîtrust as a driver of growth‚Äîremains plausible."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#limitations-robustness",
    "href": "week03/assets/trust_income_report_long.html#limitations-robustness",
    "title": "",
    "section": "6. Limitations & robustness",
    "text": "6. Limitations & robustness\n\nCross‚Äësection only; cannot infer causality.\nSmall N; high‚Äëtrust outlier (CHN) influences slope.\nAggregate averages hide within‚Äëcountry disparities."
  },
  {
    "objectID": "week03/assets/trust_income_report_long.html#short-assignment-prompt",
    "href": "week03/assets/trust_income_report_long.html#short-assignment-prompt",
    "title": "",
    "section": "7. Short assignment prompt",
    "text": "7. Short assignment prompt\nFor students:\n\nReplicate the scatter plot; label one additional country of your choice.\nRe‚Äëestimate the regression adding a regional fixed effect (e.g.¬†Europe vs.¬†rest). How does Œ≤‚ÇÅ change?\nPropose two mechanisms linking income and trust and outline an empirical strategy to test one of them.\nCritique the trust measures used here.\n\n\nPrepared 21¬†Apr¬†2025"
  },
  {
    "objectID": "week03/assets/analysis_notebook.html",
    "href": "week03/assets/analysis_notebook.html",
    "title": "Income and Trust: Analysis Notebook",
    "section": "",
    "text": "# wd temp\nsetwd(\"C:/Users/bekes/Documents/GitHub/\")\npath =\"da-w-ai/data/VWS/\"\ndf &lt;- read_csv(paste0(path, \"WVS_GDP_merged_data.csv\"))\n\nRows: 66 Columns: 97\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): B_COUNTRY_ALPHA, iso3c\ndbl (95): A_YEAR, Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, Q11, Q12, Q13, Q1...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf &lt;- df %&gt;%\n  mutate(baseline_trust = 2 - Q57,\n         alt_trust = 4 - rowMeans(select(., Q59:Q63), na.rm=TRUE) + 1,\n         log_gdppc = log(GDP_USD_PPP_per_capita))\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nsummary(lm(baseline_trust ~ log_gdppc, data = df))\n\n\nCall:\nlm(formula = baseline_trust ~ log_gdppc, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.23286 -0.09800 -0.01321  0.05304  0.44896 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.80042    0.17794  -4.498 3.13e-05 ***\nlog_gdppc    0.10390    0.01806   5.751 3.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1262 on 61 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.3516,    Adjusted R-squared:  0.341 \nF-statistic: 33.08 on 1 and 61 DF,  p-value: 3.044e-07\n\nsummary(lm(alt_trust ~ log_gdppc, data = df))\n\n\nCall:\nlm(formula = alt_trust ~ log_gdppc, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53529 -0.12054  0.00505  0.12603  0.45564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0880     0.2975   3.657 0.000533 ***\nlog_gdppc     0.1372     0.0302   4.544 2.67e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2109 on 61 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.2529,    Adjusted R-squared:  0.2406 \nF-statistic: 20.64 on 1 and 61 DF,  p-value: 2.669e-05"
  },
  {
    "objectID": "week02/index.html",
    "href": "week02/index.html",
    "title": "Week02: Discovery and documentation",
    "section": "",
    "text": "Week02: Discovery and documentation\n\n\nData discovery and data and code documentation with AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#summary",
    "href": "week02/index.html#summary",
    "title": "Week02: Discovery and documentation",
    "section": "Summary",
    "text": "Summary\nSometimes data is large and discovery is hard. Sometimes you need to write data documentation. LLMs can help. You will learn how to write a clear and professional README. We use a cleaned subset of the 7th Wave of the World Values Survey (WVS). We‚Äôll also talk some tech on documentation. We‚Äôll use AI as a research assistantto bravely face a codebook with hundreds of variables.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#learning-objectives",
    "href": "week02/index.html#learning-objectives",
    "title": "Week02: Discovery and documentation",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nUnderstand how to document a new dataset using as an example th WVS 7th wave data.\nCreate a README that describes data.\nLearn to refine documentation by incorporating iterative feedback from peers and AI tools.\nDevelop skills in using AI to translate complex materials into accessible documentation",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#preparation-before-class",
    "href": "week02/index.html#preparation-before-class",
    "title": "Week02: Discovery and documentation",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\nüìö Required Reading\n\n\n\nBackground reading: B√©k√©s-K√©zdi (2021) Chapters 1-3, in particular core background info. Focus on Chapter 2 sections on data structure and variable types - this becomes crucial when documenting data.\nSome discussion of data types Data Management in Large-Scale Education Research by Crystal Lewis\n\n\n\n\n\nüìä Data Setup\n\n\nAccess the VWS dataset\n\nData: WVS_random_subset.csv - random subset (N=2000) - covering all countries\nDownload its official codebook documentation\nTake 10 minutes to browse the data structure before class - note what confuses you about variable names and definitions.\n\nIf you prefer datasets are also at OSF, Gabors Data Analysis / World Values Survey",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#review-assignment-01",
    "href": "week02/index.html#review-assignment-01",
    "title": "Week02: Discovery and documentation",
    "section": "Review Assignment 01",
    "text": "Review Assignment 01\n\nFollow instructions.\nHow to get close to original, different ways\nWhy do an app? What to expect from an app\n\nstreamlit\nshinyapps",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#i.-background",
    "href": "week02/index.html#i.-background",
    "title": "Week02: Discovery and documentation",
    "section": "I. Background",
    "text": "I. Background\n\nAbout Markdown\n\nEditor in R, Python Quarto\nOnline Markdown editor\nAlso: Pandoc\n\n\n\nWhat is a good readme?\nSome examples for reproduction package\n\nB√©k√©s-K√©zdi (2021) Hotels dataset ‚Äì show basics\nKoren-Pet≈ë (2021) Business disruptions from social distancing as PDF\nSome ideas on readme: Makereadme, Social Science Editors\n\nKey ingredients\n\nOverview of project\nlicense\nAll datasets (data tables) separately discussed\nAll key variables described (name, content, type, coverage (% share missing)\n\nmaybe also: source, extension (csv / xlsx/ parquet)\n\n\n\n\nWhat is a variable dictionary (also called codebook)\n\nmore details of a dataset, often as xlsx\nmetric (euro, %), meaning of values if categorical\nmaybe even mean, min, max\n\nExamples\n\nB√©k√©s-K√©zdi (2021) Bisnode dataset variables\nReif (2022) illinois-wellness-data",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#ii.-work-on-data",
    "href": "week02/index.html#ii.-work-on-data",
    "title": "Week02: Discovery and documentation",
    "section": "II. Work on data",
    "text": "II. Work on data\n\nNo AI\n\nDownload and look at the Random Subset data\nStart collecting some info on the data without AI\nStart thinking about an interesting research question (find \\(y\\) and \\(x\\))\n\n\n\nAI: let AI teach you also about\n\nStart asking for skeleton readme, ask about advice\nDiscussion\n\n\n\nAI: Learning and idea generation\n\nTell AI about your plan and need for a readme\n\nexperiment with one-shot vs interaction\n\nDiscussion\n\n\n\nCyborg mode: create a readme with AI\n\nUpload the codebook + random subset data\nGet AI to design a README TEMPLATE for this task.\nGet a draft\nUnderstand and edit draft\n\n\n\nIII additional idea\n\nSometimes, complicated projects have extensive folder structure. Use A to design a folder structure",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#end-of-week-discussion-points",
    "href": "week02/index.html#end-of-week-discussion-points",
    "title": "Week02: Discovery and documentation",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\nEnd of Week Reflection:\n\nWhat was the biggest contribution of AI?\nFirst result vs after iterations ‚Äì what did improve?\nHow do you feel about learning from AI vs human instructor? Pros and cons?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html",
    "title": "World Values Survey Data Analysis",
    "section": "",
    "text": "This repository contains data and code for analyzing the World Values Survey (WVS) dataset. The project involves data cleaning, processing, and merging GDP data with survey responses."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#overview",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#overview",
    "title": "World Values Survey Data Analysis",
    "section": "",
    "text": "This repository contains data and code for analyzing the World Values Survey (WVS) dataset. The project involves data cleaning, processing, and merging GDP data with survey responses."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#repository-structure",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#repository-structure",
    "title": "World Values Survey Data Analysis",
    "section": "Repository Structure",
    "text": "Repository Structure\nüìÇ osf-vwvs/                   # Root directory\n ‚îú‚îÄ‚îÄ üìÇ clean/                 # Processed datasets  \n ‚îÇ    ‚îú‚îÄ‚îÄ WVS_GDP_merged_data.csv    # Merged WVS data with GDP  \n ‚îÇ    ‚îú‚îÄ‚îÄ WVS_random_subset.csv      # Random subset of WVS data  \n ‚îÇ    ‚îú‚îÄ‚îÄ WVS_subset.csv             # Subset of WVS data  \n ‚îÇ  \n ‚îú‚îÄ‚îÄ üìÇ documentation/          # Documentation and metadata  \n ‚îÇ    ‚îú‚îÄ‚îÄ codebook.pdf          # Variable definitions and coding instructions  \n ‚îÇ  \n ‚îú‚îÄ‚îÄ üìÇ raw/                    # Raw data files  \n ‚îÇ    ‚îú‚îÄ‚îÄ WVS_Cross-National_Wave_7_csv_v6_0.csv  # Original WVS dataset  \n ‚îÇ  \n ‚îú‚îÄ‚îÄ üìÇ code/                   # Scripts for data processing  \n ‚îÇ    ‚îú‚îÄ‚îÄ cleaning.R            # Script to clean and process WVS data  \n ‚îÇ  \n ‚îú‚îÄ‚îÄ README.md                  # Project description and instructions"
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#data-description",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#data-description",
    "title": "World Values Survey Data Analysis",
    "section": "Data Description",
    "text": "Data Description\n\nSource: The WVS dataset is publicly available at World Values Survey.\n\nProcessed Data: The clean/ folder contains merged and subsetted datasets used for analysis.\n\nRaw Data: The raw/ folder holds the original dataset in .csv format.\n\nDocumentation: The codebook.pdf explains variable definitions and survey methodology."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#code-and-reproducibility",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#code-and-reproducibility",
    "title": "World Values Survey Data Analysis",
    "section": "Code and Reproducibility",
    "text": "Code and Reproducibility\n\nMain script: code/cleaning.R cleans and processes the raw WVS data.\nRequirements:\n\nR (version X.X.X)\nRequired packages: tidyverse, haven\n\nRunning the code:\n\nPlace the raw data in the raw/ folder.\nRun cleaning.R to generate the processed datasets."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#license-and-citation",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#license-and-citation",
    "title": "World Values Survey Data Analysis",
    "section": "License and Citation",
    "text": "License and Citation\n\nThis project follows the MIT License.\nIf using this data or code, please cite the original WVS dataset."
  },
  {
    "objectID": "week02/assets/chatgpt-2025-vws-README-v1.html#contact",
    "href": "week02/assets/chatgpt-2025-vws-README-v1.html#contact",
    "title": "World Values Survey Data Analysis",
    "section": "Contact",
    "text": "Contact\nFor questions or collaboration, contact: [Your Name] at [Your Email]."
  },
  {
    "objectID": "week01/assets/which-ai.html",
    "href": "week01/assets/which-ai.html",
    "title": "Which AI model shall we chose?",
    "section": "",
    "text": "In what follows, here is my personal take as of date:2025-05-22\nGenerative AI based on Large Language Models (genAI) is great for many tasks. In this course we only focus on aspects of Data Analysis:"
  },
  {
    "objectID": "week01/assets/which-ai.html#different-ai-providers-and-their-models",
    "href": "week01/assets/which-ai.html#different-ai-providers-and-their-models",
    "title": "Which AI model shall we chose?",
    "section": "Different AI providers and their models",
    "text": "Different AI providers and their models\n\nOpenAI ChatGPT\nThe main current models are 4o, o3, and 4.1\nHere‚Äôs an updated, compact guide including o3, o4-mini-high, GPT-4.5, and Deep Research:\nHere ChatGPT‚Äôs quick guide to when and why to use which model\n\nüîç Model Overview\n\n\n\n\n\n\n\n\nModel\nStrengths\nUse When\n\n\n\n\nGPT-4-turbo (4o)\nFast, accurate, handles long prompts and code well\nDefault for coding, EDA, modeling, teaching\n\n\nGPT-4.5\nSlightly better reasoning and math; not always faster\nMore complex logic, multi-step planning\n\n\nGPT-4 (base)\nStable, reliable for structured work\nYou need consistent responses (e.g., templates)\n\n\no3\nCompact, efficient, more creative but can be fuzzy\nBrainstorming, creative prompt design\n\n\no4-mini-high\nLightweight, fast, good for quick checks or when resources are limited\nInstant feedback, code sketching\n\n\nDeep Research\nAccess to full documents, citations, deep factual retrieval\nLiterature reviews, technical deep-dives\n\n\n\n\n\n\n‚úÖ Best Model by Task\n\n\n\n\n\n\n\n\nTask\nBest Model(s)\nNotes\n\n\n\n\nDesigning analysis\n4o / GPT-4.5\nHandles multi-step reasoning well\n\n\nWriting code (R, Python, SQL)\n4o / GPT-4.5 / o4-mini-high\nUse 4o for tidyverse-heavy tasks; o4-mini for quick draft\n\n\nData wrangling\n4o\nVery good with dplyr, data.table, pandas\n\n\nExploratory data analysis\n4o + code interpreter\nVisuals, summaries, and diagnostics\n\n\nModeling (ML, regressions)\nGPT-4.5 / 4o\nClear, structured models and diagnostics\n\n\nCausal inference\nGPT-4.5 / 4o\nHandles DiD, IV, RDD, matching logic well\n\n\nCreating tables and graphs\n4o / GPT-4.5\nKnows LaTeX, Markdown, ggplot2, matplotlib formatting\n\n\nWriting reports / slides\n4o / GPT-4 / o3\n4o for clarity, o3 for more creative text generation\n\n\nLiterature search / citations\nDeep Research\nFinds, summarizes, and cites academic papers\n\n\n\n\n\nAnthropic Claude\nThe main current model is Claude Sonnet 3.7.\nKey tools * Projects: organize files, allow inquiry. One example is full codebase.\n\n\nOthers\nThere are many other models, but I have much less experience."
  },
  {
    "objectID": "week01/assets/which-ai.html#free-vs-pro",
    "href": "week01/assets/which-ai.html#free-vs-pro",
    "title": "Which AI model shall we chose?",
    "section": "Free vs Pro?",
    "text": "Free vs Pro?\nThe current free models are great for many tasks such as coding, idea generation.\n\nChatGPT\nThe free version offers: access to GPT‚Äë4.1 mini, real-time data from the web with search. Plus * Limited access to GPT‚Äë4o, OpenAI o4-mini, and deep research * Limited access to file uploads, data analysis,\nThe Plus version offers\n\naccess to reasoning models (OpenAI o3, OpenAI o4-mini, and OpenAI o4-mini-high)\naccess Deep Research\nhigher limits on advanced features: file uploads, and data analysis\naccess to a research preview of GPT‚Äë4.5\naccess to GPT‚Äë4.1, a model optimized for coding tasks\ncan create and use projects, tasks,\n\n\n\nClaude\nThe free model can be used for chat and data analysis.\nThe paid tier for Claude\n\nMore usage ‚Äì for details see limits\naccess to Projects to organize chats and documents\nweb access\nextended thinking for complex work"
  },
  {
    "objectID": "week01/assets/which-ai.html#other-cool-stuff-i-use",
    "href": "week01/assets/which-ai.html#other-cool-stuff-i-use",
    "title": "Which AI model shall we chose?",
    "section": "Other cool stuff I use",
    "text": "Other cool stuff I use\n\nNotebook LM\n[Google‚Äôs Notebook LM] (https://notebooklm.google/) is able to ‚Äúunderstand‚Äù and summarize any material (such as research paper) and relate it to other topics. Can create fun audo summaries like a podcast. Here is one on a research paper of mine on cultural homophily.\n\n\nGithub Copilot\nGithub Copilot goes inside your code editor such as Rstudio, VSCode, Jupyter Notebook and helps writing code. Great to write frequent stuff like loops or graphs.\nIt has an Education ‚Äì free access for students: GithubEducation\n\n\nCursor\nCursor AI is the most popular AI code editor, I have very limited experience, but is favored by software developers."
  },
  {
    "objectID": "week01/assets/which-ai.html#feedback",
    "href": "week01/assets/which-ai.html#feedback",
    "title": "Which AI model shall we chose?",
    "section": "Feedback",
    "text": "Feedback\nDear Reader. I have limited experience. Suggestions are welcome, please post an issue."
  },
  {
    "objectID": "not-shared/hotel-data-readme.html",
    "href": "not-shared/hotel-data-readme.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "not-shared/hotel-data-readme.html#info",
    "href": "not-shared/hotel-data-readme.html#info",
    "title": "",
    "section": "Info",
    "text": "Info\nThis dataset contains realistic data on hotels across Austria.\n\nThis dataset was generated programmatically with the generate_austrian_hotels_data.R script to ensure realistic relationships between variables while maintaining privacy.\n\nThe scipt was writen by Claude AI, Sonnet 3.7, 2025-03-15, and reviwed and approved by Gabor 2025-03-17\n\nThe dataset consists of multiple related tables that can be combined.\nThe data patterns are based on typical hotel industry metrics but do not represent actual hotels."
  },
  {
    "objectID": "not-shared/hotel-data-readme.html#dataset-overview",
    "href": "not-shared/hotel-data-readme.html#dataset-overview",
    "title": "",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe dataset includes hotels across Austrian cities with data on occupancy, pricing, tourism statistics, and economic indicators.\n\nFiles\nAll files are located in the data/raw/ directory:\n\n\n\n\n\n\n\n\n\nFile\nDescription\nRows\nKey Columns\n\n\n\n\nhotels.csv\nBasic hotel information\n200\nhotel_id (PK)\n\n\ncities.csv\nCity information\n10\ncity (PK)\n\n\nmonthly_occupancy.csv\nMonthly hotel performance metrics\n~3,800\nhotel_id, month, year\n\n\ncity_tourism.csv\nMonthly tourism statistics by city\n240\ncity, month, year\n\n\neconomic_indicators.csv\nMonthly economic indicators\n24\nmonth, year\n\n\nreviews.csv\nHotel guest reviews\n~1,700\nreview_id (PK), hotel_id (FK)\n\n\namenities.csv\nList of possible hotel amenities\n10\namenity_id (PK)\n\n\nhotel_amenities.csv\nHotel-amenity relationships\n~1,000\nhotel_id, amenity_id\n\n\n\n## Schema Details\n### hotels.csv Information about individual hotels.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nPrimary key\n\n\nhotel_name\ncharacter\nHotel name\n\n\ncity\ncharacter\nCity where hotel is located\n\n\nstar_rating\ninteger\nHotel quality rating (3-5 stars)\n\n\nrooms\ninteger\nNumber of rooms in the hotel\n\n\nyear_built\ninteger\nYear the hotel was built\n\n\n\n### cities.csv Information about Austrian cities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\ncity\ncharacter\nCity name (primary key)\n\n\nprovince\ncharacter\nAustrian province\n\n\npopulation\ninteger\nCity population\n\n\ntourism_rank\ninteger\nTourism popularity rank (1 = highest)\n\n\n\n### monthly_occupancy.csv Monthly hotel performance metrics.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\noccupancy_rate\nnumeric\nPercentage of rooms occupied (0.0-1.0)\n\n\navg_daily_rate\nnumeric\nAverage price per night in EUR\n\n\nrevenue_per_room\nnumeric\nRevenue per available room (RevPAR)\n\n\n\n### city_tourism.csv Monthly tourism statistics for each city.\n\n\n\nColumn\nType\nDescription\n\n\n\n\ncity\ncharacter\nCity name\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\ntourist_arrivals\ninteger\nNumber of tourists arriving\n\n\nevent_days\ninteger\nNumber of event days in the month\n\n\navg_stay_length\nnumeric\nAverage length of stay in days\n\n\n\n### economic_indicators.csv Monthly economic indicators for Austria.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\ninflation_rate\nnumeric\nMonthly inflation rate (decimal)\n\n\nunemployment\nnumeric\nUnemployment rate (decimal)\n\n\nconsumer_confidence\nnumeric\nConsumer confidence index\n\n\n\n### reviews.csv Hotel guest reviews.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nreview_id\ninteger\nPrimary key\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\nrating\nnumeric\nRating (1.0-5.0)\n\n\nreview_date\ndate\nDate of the review\n\n\n\n### amenities.csv List of possible hotel amenities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\namenity_id\ninteger\nPrimary key\n\n\namenity_name\ncharacter\nName of the amenity\n\n\n\n### hotel_amenities.csv Many-to-many relationship between hotels and amenities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\namenity_id\ninteger\nForeign key to amenities.csv"
  },
  {
    "objectID": "data/VWS/README_WVS.html",
    "href": "data/VWS/README_WVS.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "data/VWS/README_WVS.html#overview",
    "href": "data/VWS/README_WVS.html#overview",
    "title": "",
    "section": "Overview",
    "text": "Overview\nThis script cleans and subsets World Values Survey (WVS) Wave 7 data, generates a random subsample, aggregates by country & year, and merges with World Bank GDP indicators."
  },
  {
    "objectID": "data/VWS/README_WVS.html#prerequisites",
    "href": "data/VWS/README_WVS.html#prerequisites",
    "title": "",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPackages:\n\nosfr (download from OSF)\n\ndplyr (data manipulation)\n\nreadr (CSV I/O)\n\nWDI (World Bank API)"
  },
  {
    "objectID": "data/VWS/README_WVS.html#directory-structure",
    "href": "data/VWS/README_WVS.html#directory-structure",
    "title": "",
    "section": "Directory Structure",
    "text": "Directory Structure\nproject-root/\n‚îú‚îÄ data/\n‚îÇ  ‚îú‚îÄ raw/        ‚Üê input CSVs\n‚îÇ  ‚îî‚îÄ clean/      ‚Üê outputs\n‚îî‚îÄ scripts/\n   ‚îî‚îÄ cleaning.R ‚Üê this script"
  },
  {
    "objectID": "data/VWS/README_WVS.html#input",
    "href": "data/VWS/README_WVS.html#input",
    "title": "",
    "section": "Input",
    "text": "Input\n\ndata/raw/WVS_Cross-National_Wave_7_csv_v6_0.csv\nDownloaded automatically from OSF (ID: 36dgb)."
  },
  {
    "objectID": "data/VWS/README_WVS.html#output",
    "href": "data/VWS/README_WVS.html#output",
    "title": "",
    "section": "Output",
    "text": "Output\n\nWVS_subset.csv\nSelected variables and respondents, wave 1‚Äì7.\nWVS_random_subset2000.csv\nRandom sample of 2 000 respondents (‚âà per country).\nWVS_GDP_merged_data.csv\nAggregated (mean & mode) by country & year for wave 7, merged with GDP & population (2017‚Äì2023)."
  },
  {
    "objectID": "data/VWS/README_WVS.html#processing-steps",
    "href": "data/VWS/README_WVS.html#processing-steps",
    "title": "",
    "section": "Processing Steps",
    "text": "Processing Steps\n\nSetup\n\nClear environment (rm(list=ls()))\n\nLoad libraries\n\nDefine data_in and data_out folders\n\nImport & Subset\n\nDownload raw CSV via OSF\n\nSelect key demographics (country codes, interview date, weights) and survey items (Q1‚ÄìQ89, Q260‚ÄìQ290)\n\nSave to WVS_subset.csv\nNote: This file contains answers from all respondents from the data.\n\nRandom Subsample\n\nIn this step, we create a random subsample to reduce sample size.\nSeed: 20250124\n\nSample ~2 000 respondents stratified by country\n\nCount the resulting number of respondents in each country\nSave to WVS_random_subset2000.csv\n\nAggregate & Clean\n\nIn this step, we aggregate the full data (step 2 data) to country-level, then join with GDP data.\nRecode negative codes (‚Äì1‚Ä¶‚Äì5) to NA\n\nCount the number of respondents in each country\nCompute country‚Äìyear means for numeric items, modes for categorical\n\nDownload GDP & population (2017‚Äì2023) via WDI\n\nMerge on ISO3 country code & year\n\nSave to WVS_GDP_merged_data.csv"
  },
  {
    "objectID": "data/VWS/README_WVS.html#usage",
    "href": "data/VWS/README_WVS.html#usage",
    "title": "",
    "section": "Usage",
    "text": "Usage\nRscript scripts/cleaning.R\nEnsure your working directory is set to project root.\nRaw data and outputs will live under data/raw and data/clean."
  },
  {
    "objectID": "data/VWS/README_WVS.html#contact",
    "href": "data/VWS/README_WVS.html#contact",
    "title": "",
    "section": "Contact",
    "text": "Contact\ngabors-data-analysis.com | MA (BA) Data Analysis with AI course"
  },
  {
    "objectID": "data/interviews/index.html",
    "href": "data/interviews/index.html",
    "title": "Interview Analysis Code",
    "section": "",
    "text": "Code examples for sentiment analysis of football manager interviews, supporting both R and Python workflows.\nRelated to: Week 5-6 (Text as Data)"
  },
  {
    "objectID": "data/interviews/index.html#overview",
    "href": "data/interviews/index.html#overview",
    "title": "Interview Analysis Code",
    "section": "",
    "text": "Code examples for sentiment analysis of football manager interviews, supporting both R and Python workflows.\nRelated to: Week 5-6 (Text as Data)"
  },
  {
    "objectID": "data/interviews/index.html#api-setup",
    "href": "data/interviews/index.html#api-setup",
    "title": "Interview Analysis Code",
    "section": "API Setup",
    "text": "API Setup\n\napi-key.R\nTemplate for setting up OpenAI API credentials in R.\n# Set your API key\nSys.setenv(OPENAI_API_KEY = \"sk-....\")\nImportant: Copy to my-openai-api-key.R and add to .gitignore"
  },
  {
    "objectID": "data/interviews/index.html#r-scripts",
    "href": "data/interviews/index.html#r-scripts",
    "title": "Interview Analysis Code",
    "section": "R Scripts",
    "text": "R Scripts\n\nsentiment-analysis.R\nComplete sentiment analysis workflow using OpenAI API.\nFeatures: - API key setup and validation - Batch processing with progress tracking - Sentiment classification function (-2 to +2 scale) - Error handling and retry logic - Comparison between multiple API runs\nKey functions: - classify_text(): Sends text to OpenAI for sentiment rating - Handles API errors with exponential backoff - Saves results to CSV for further analysis\n\n\ndomain_lexicon.r\nCreates football-specific sentiment lexicon.\nOutput: domain_lexicon.csv with: - 100+ football-specific terms - Positive terms (goals, win, excellent): scores 1.2-1.9 - Negative terms (lose, poor, mistake): scores -1.9 to -1.2 - Neutral terms (match, team, play): score 0"
  },
  {
    "objectID": "data/interviews/index.html#python-scripts",
    "href": "data/interviews/index.html#python-scripts",
    "title": "Interview Analysis Code",
    "section": "Python Scripts",
    "text": "Python Scripts\n\nclassify_manager_sentiment.py\nPython equivalent of R sentiment analysis.\nFeatures: - OpenAI API integration using openai library - Logging and progress tracking with tqdm - Robust error handling - Same rating guidelines as R version\nDependencies:\npip install openai pandas tqdm python-dotenv\n\n\nmanager_sentiment_api.md\nDocumentation and code template for Python API usage."
  },
  {
    "objectID": "data/interviews/index.html#usage-notes",
    "href": "data/interviews/index.html#usage-notes",
    "title": "Interview Analysis Code",
    "section": "Usage Notes",
    "text": "Usage Notes\n\nAPI Key Management\n\nStore keys in environment variables\nNever commit keys to version control\nBudget ~$5 for course exercises\n\n\n\nRate Limiting\n\nBoth scripts include retry logic\nExponential backoff for failed requests\nMonitor usage through OpenAI dashboard\n\n\n\nReproducibility\n\nSet temperature=0 for consistent results\nSave intermediate results\nDocument model versions used"
  },
  {
    "objectID": "data/interviews/index.html#output-files",
    "href": "data/interviews/index.html#output-files",
    "title": "Interview Analysis Code",
    "section": "Output Files",
    "text": "Output Files\nScripts generate: - manager_sentiment_results.csv: Individual text ratings - sentiment_comparison.csv: Comparison between runs - classification.log: Detailed processing logs"
  },
  {
    "objectID": "data/interviews/index.html#integration-with-course-data",
    "href": "data/interviews/index.html#integration-with-course-data",
    "title": "Interview Analysis Code",
    "section": "Integration with Course Data",
    "text": "Integration with Course Data\nThese scripts work with: - /data/interviews/interview-texts-only.xlsx (input) - /data/interviews/domain_lexicon.csv (lexicon) - Student rating files for comparison analysis"
  },
  {
    "objectID": "data/austria-hotels/index.html",
    "href": "data/austria-hotels/index.html",
    "title": "Austrian Hotels Dataset",
    "section": "",
    "text": "This dataset contains realistic simulated data on hotels across Austria, designed for practicing data wrangling and table joins. The dataset consists of multiple related tables that can be combined using various join operations.\nUsed in: Week 4 (Joining Tables)\nGenerated by: Claude AI (Sonnet 3.7) with realistic relationships between variables"
  },
  {
    "objectID": "data/austria-hotels/index.html#overview",
    "href": "data/austria-hotels/index.html#overview",
    "title": "Austrian Hotels Dataset",
    "section": "",
    "text": "This dataset contains realistic simulated data on hotels across Austria, designed for practicing data wrangling and table joins. The dataset consists of multiple related tables that can be combined using various join operations.\nUsed in: Week 4 (Joining Tables)\nGenerated by: Claude AI (Sonnet 3.7) with realistic relationships between variables"
  },
  {
    "objectID": "data/austria-hotels/index.html#dataset-structure",
    "href": "data/austria-hotels/index.html#dataset-structure",
    "title": "Austrian Hotels Dataset",
    "section": "Dataset Structure",
    "text": "Dataset Structure\nThe dataset includes 8 related tables with hotels across Austrian cities, covering occupancy, pricing, tourism statistics, and economic indicators.\n\nCore Tables\n\n\n\n\n\n\n\n\n\nFile\nDescription\nRows\nKey Columns\n\n\n\n\nhotels.csv\nBasic hotel information\n200\nhotel_id (PK)\n\n\ncities.csv\nCity information\n10\ncity (PK)\n\n\nmonthly_occupancy.csv\nMonthly hotel performance metrics\n~3,800\nhotel_id, month, year\n\n\ncity_tourism.csv\nMonthly tourism statistics by city\n240\ncity, month, year\n\n\neconomic_indicators.csv\nMonthly economic indicators\n24\nmonth, year\n\n\nreviews.csv\nHotel guest reviews\n~1,700\nreview_id (PK), hotel_id (FK)\n\n\namenities.csv\nList of possible hotel amenities\n10\namenity_id (PK)\n\n\nhotel_amenities.csv\nHotel-amenity relationships\n~1,000\nhotel_id, amenity_id"
  },
  {
    "objectID": "data/austria-hotels/index.html#key-relationships",
    "href": "data/austria-hotels/index.html#key-relationships",
    "title": "Austrian Hotels Dataset",
    "section": "Key Relationships",
    "text": "Key Relationships\n\nOne-to-One: Hotels ‚ÜîÔ∏é Cities (through city name)\nOne-to-Many: Hotels ‚Üí Monthly Occupancy, Hotels ‚Üí Reviews\nMany-to-Many: Hotels ‚ÜîÔ∏é Amenities (through hotel_amenities)\nComposite Keys: Monthly data requires (hotel_id, month, year) or (city, month, year)"
  },
  {
    "objectID": "data/austria-hotels/index.html#documentation",
    "href": "data/austria-hotels/index.html#documentation",
    "title": "Austrian Hotels Dataset",
    "section": "Documentation",
    "text": "Documentation\n\nhotel-data-readme.md - Detailed schema documentation with column descriptions and data types"
  },
  {
    "objectID": "data/austria-hotels/index.html#learning-objectives",
    "href": "data/austria-hotels/index.html#learning-objectives",
    "title": "Austrian Hotels Dataset",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis dataset allows students to practice: - Inner, left, right, and full joins - One-to-one and one-to-many relationships - Composite key joins - Data aggregation after joins - Handling missing values in joins"
  },
  {
    "objectID": "data/austria-hotels/index.html#sample-research-questions",
    "href": "data/austria-hotels/index.html#sample-research-questions",
    "title": "Austrian Hotels Dataset",
    "section": "Sample Research Questions",
    "text": "Sample Research Questions\n\nHow do hotel prices vary by city and season?\nWhat‚Äôs the relationship between amenities and guest ratings?\nHow do economic indicators affect hotel occupancy rates?\nWhich cities have the highest tourism-to-hotel capacity ratios?"
  },
  {
    "objectID": "code/interviews/index.html",
    "href": "code/interviews/index.html",
    "title": "Interview Analysis Code",
    "section": "",
    "text": "Code examples for sentiment analysis of football manager interviews, supporting both R and Python workflows.\nRelated to: Week 5-6 (Text as Data)"
  },
  {
    "objectID": "code/interviews/index.html#overview",
    "href": "code/interviews/index.html#overview",
    "title": "Interview Analysis Code",
    "section": "",
    "text": "Code examples for sentiment analysis of football manager interviews, supporting both R and Python workflows.\nRelated to: Week 5-6 (Text as Data)"
  },
  {
    "objectID": "code/interviews/index.html#api-setup",
    "href": "code/interviews/index.html#api-setup",
    "title": "Interview Analysis Code",
    "section": "API Setup",
    "text": "API Setup\n\napi-key.R\nTemplate for setting up OpenAI API credentials in R.\n# Set your API key\nSys.setenv(OPENAI_API_KEY = \"sk-....\")\nImportant: Copy to my-openai-api-key.R and add to .gitignore"
  },
  {
    "objectID": "code/interviews/index.html#r-scripts",
    "href": "code/interviews/index.html#r-scripts",
    "title": "Interview Analysis Code",
    "section": "R Scripts",
    "text": "R Scripts\n\nsentiment-analysis.R\nComplete sentiment analysis workflow using OpenAI API.\nFeatures: - API key setup and validation - Batch processing with progress tracking - Sentiment classification function (-2 to +2 scale) - Error handling and retry logic - Comparison between multiple API runs\nKey functions: - classify_text(): Sends text to OpenAI for sentiment rating - Handles API errors with exponential backoff - Saves results to CSV for further analysis\n\n\ndomain_lexicon.r\nCreates football-specific sentiment lexicon.\nOutput: domain_lexicon.csv with: - 100+ football-specific terms - Positive terms (goals, win, excellent): scores 1.2-1.9 - Negative terms (lose, poor, mistake): scores -1.9 to -1.2 - Neutral terms (match, team, play): score 0"
  },
  {
    "objectID": "code/interviews/index.html#python-scripts",
    "href": "code/interviews/index.html#python-scripts",
    "title": "Interview Analysis Code",
    "section": "Python Scripts",
    "text": "Python Scripts\n\nclassify_manager_sentiment.py\nPython equivalent of R sentiment analysis.\nFeatures: - OpenAI API integration using openai library - Logging and progress tracking with tqdm - Robust error handling - Same rating guidelines as R version\nDependencies:\npip install openai pandas tqdm python-dotenv\n\n\nmanager_sentiment_api.md\nDocumentation and code template for Python API usage."
  },
  {
    "objectID": "code/interviews/index.html#usage-notes",
    "href": "code/interviews/index.html#usage-notes",
    "title": "Interview Analysis Code",
    "section": "Usage Notes",
    "text": "Usage Notes\n\nAPI Key Management\n\nStore keys in environment variables\nNever commit keys to version control\nBudget ~$5 for course exercises\n\n\n\nRate Limiting\n\nBoth scripts include retry logic\nExponential backoff for failed requests\nMonitor usage through OpenAI dashboard\n\n\n\nReproducibility\n\nSet temperature=0 for consistent results\nSave intermediate results\nDocument model versions used"
  },
  {
    "objectID": "code/interviews/index.html#output-files",
    "href": "code/interviews/index.html#output-files",
    "title": "Interview Analysis Code",
    "section": "Output Files",
    "text": "Output Files\nScripts generate: - manager_sentiment_results.csv: Individual text ratings - sentiment_comparison.csv: Comparison between runs - classification.log: Detailed processing logs"
  },
  {
    "objectID": "code/interviews/index.html#integration-with-course-data",
    "href": "code/interviews/index.html#integration-with-course-data",
    "title": "Interview Analysis Code",
    "section": "Integration with Course Data",
    "text": "Integration with Course Data\nThese scripts work with: - /data/interviews/interview-texts-only.xlsx (input) - /data/interviews/domain_lexicon.csv (lexicon) - Student rating files for comparison analysis"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Course Assignments",
    "section": "",
    "text": "Assignments are available for all classes\nImportant to note for assignments:  * Use AI but do not submit something that was created by AI. AI is your assistant. * One of the goals of the course is to practice this."
  },
  {
    "objectID": "assignments.html#assignment-1-reproduce-an-ft-graph",
    "href": "assignments.html#assignment-1-reproduce-an-ft-graph",
    "title": "Course Assignments",
    "section": "Assignment 1: Reproduce an FT graph",
    "text": "Assignment 1: Reproduce an FT graph\nLook at this graph from Financial Times. Your task is to recreate this figure using real-world data.\nAssignment Details"
  },
  {
    "objectID": "assignments.html#assignment-2-creating-a-readme-doc",
    "href": "assignments.html#assignment-2-creating-a-readme-doc",
    "title": "Course Assignments",
    "section": "Assignment 2: Creating a readme doc",
    "text": "Assignment 2: Creating a readme doc\nIn this assignment, you will create a readme document by asking for AI assistance.\nAssignment Details"
  },
  {
    "objectID": "assignments.html#assignment-3-create-a-report",
    "href": "assignments.html#assignment-3-create-a-report",
    "title": "Course Assignments",
    "section": "Assignment 3: Create a report",
    "text": "Assignment 3: Create a report\nContinue with the research question you had chosen in class. Use AI iteratively to help you create a report.\nAssignment Details"
  },
  {
    "objectID": "assignments.html#assignment-4-join-tables-analysis",
    "href": "assignments.html#assignment-4-join-tables-analysis",
    "title": "Course Assignments",
    "section": "Assignment 4: Join tables analysis",
    "text": "Assignment 4: Join tables analysis\nCreate groups of 2-3 people who use the same coding language. Ask AI to give you a list of research questions that can be answered using many (at least 4) of the hotel data sets provided.\nAssignment Details"
  },
  {
    "objectID": "assignments.html#assignment-5-text-sentiment-analysis",
    "href": "assignments.html#assignment-5-text-sentiment-analysis",
    "title": "Course Assignments",
    "section": "Assignment 5: Text sentiment analysis",
    "text": "Assignment 5: Text sentiment analysis\nStudent-Specific Text Sentiment Analysis ‚Äì Compare manual ratings with AI-generated ratings.\nAssignment Details"
  },
  {
    "objectID": "assignments.html#assignment-7-simulation-app",
    "href": "assignments.html#assignment-7-simulation-app",
    "title": "Course Assignments",
    "section": "Assignment 7: Simulation app",
    "text": "Assignment 7: Simulation app\nCreate a simulation app of an important statistical phenomenon.\nAssignment Details"
  },
  {
    "objectID": "assignments/assignment_05.html",
    "href": "assignments/assignment_05.html",
    "title": "Student-Specific Sentiment Analysis",
    "section": "",
    "text": "Task\nStudent-Specific Sentiment Analysis of a series of texts ‚Äì Compare manual ratings with AI-generated ratings.\nSteps:\n\nDownload the text file of interviews. Randonly generate a series of 25 numbers between 1 and 121 (without replacement). Those will your interviews to rate. Filter those interviews and save the file as lastname_firstname_sentiment_rated.xlsx.\nGo through your interviews and manually rate managers‚Äô sentiment (25 total).\nInput full manager interviews (without your rating) into the AI of choice and obtain and compare AI-generated sentiment ratings to initial manual assessments.\nFor both exercises, incorporate the guidelines HERE\nCompare your manual and AI generated ratings, and write one paragraph summarizing similarities and deviation.\n\n\n\nSubmit:\n\nA file with your manual ratings and AI generated ratings: lastname_firstname_sentiment_rated.xlsx.\nA text file with the one paragraph summarizing similarities and deviation: lastname_firstname_sentiment_eval.txt/",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "assignments/assignment_03.html",
    "href": "assignments/assignment_03.html",
    "title": "Creating a report",
    "section": "",
    "text": "Continue with the research question you had chosen in class.\nUse AI iteratively to help you create a report.\nImportant\n\nAI is your assistant, use it as input not as output. Don‚Äôt submit an AI-generated report as such, always author, review, edit.\n\n\nTasks\nOption A (easy)\n\nThink in terms of two variables (\\(x\\) and \\(y\\)).\n\nPick a single variable or a combined index of variables\nPick a GDP variable\nThink about a causal link.\n\nCreate one carefully designed graph to illustrate the relationship (Graph 1)\nAt each step, explain your choices/decisions (ie why you chose a certain variable)\nCreate a Conclusion paragraph where you summarize your work and results in 80-100 words.\n\nOption B (advanced)\n\nIn addition to the tasks in Option A, create another graph to show heterogeneity of country size groups using population (Graph 2)\nRun a regression and interpret the coefficient.\n\n\n\nTo submit\n\nSubmission 1: Submit a maximum 1-1.5 page report in .pdf format (including exhibits) (lastname_firstname_dawai_week03_report.pdf) (12p)\nSubmission 2: Submit your code (or aprovide a link). (4p)\nSubmission 3: What advice would you give to a fellow data analysis student on using AI to create a report? (lastname_firstname_dawai_week01_advice.txt) (4p)\n\nList 2‚Äì3 pieces of advice based on your own experience using AI for this specific task.\n\n\nImportant:\n\nUpload your report to the student folder called Reports at moodle or similar service if applicable. This is for the next class\nImportant: Do not use AI to help you generate the advice. We want to hear your personal examples and reflections, not AI-generated suggestions.\nNext class in the first 20 mins each group will read another groups report and discuss the good and bad aspects of the report.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/assignment_01.html",
    "href": "assignments/assignment_01.html",
    "title": "Reproduce an FT graph",
    "section": "",
    "text": "Look at this graph from Financial Times. Your task is to recreate this figure using real-world data.\n\nOption A (easy)\nUse AI as your assistant to find real-world data that matches or closely resembles the one used in the FT graph, and recreate the same chart using that data.\nwhat to submit\n\nGraph: A PDF version of your recreated graph, name it: ‚Äúlastname_firstname_dawai_week01_ftfigure.pdf‚Äù\nData: A CSV file with the data used to create the graph, with the name (‚Äúlastname_firstname_dawai_week01_ftdata.pdf‚Äù)\n\n\n\nOption B (advanced)\nUse AI as your assistant to build an interactive app (dashboard) that mimics the FT graph.\n\nMinimum requirement: the you shall be able to set dates, and hover around values\nIdeal: get the app update data and graph dynamically.\ncould be upload new data\nbest: automatic via API\n\n\n\nGrading\nSuccess Criteria: - Data accuracy: Real Treasury yield and tariff announcement data - Design fidelity: Similar visual style, annotations, and layout - Code quality: Reproducible workflow with clear documentation\nWhat we‚Äôre evaluating: - Problem-solving approach with AI assistance - Quality of human oversight and iteration - Documentation of AI usage and limitations\nwhat to submit\n\nlink to app\nyou can use tools like steamlit (Python), shinyapps (R), etc",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment_02.html",
    "href": "assignments/assignment_02.html",
    "title": "Assignment 2: Creating a readme doc",
    "section": "",
    "text": "Task\nIn this assignment, you will create a readme document by asking for AI assistance.\n\nFirst, consider the VWS Survey we used in class: Codebook, sample data\nSecond, choose a research question using the VWS data. It can be broad or specific. Ask AI for ideas.\nWhat variables would you use to investigate the research question? Pick the most relevant ones (min 8, max 25)\nConsidering these variables only create a single README.md (in markdown). In the readme, include:\n\nOption A (easy)\n\nkey info on the dataset itself (you need to figure out what this key info is based on the lecture and examples)\nkey variables description\ntable with descriptive statistics for the 5 most important selected variables\n\nOption B (advanced)\n\nin addition to the points in Option A, describe the folder structure you would use to work with this data\n\n\n\nTo submit\n\nSubmission 1: a finalized README.md file and code. (lastname_firstname_dawai_week01_readme.md) (16p)\nSubmission 2: What advice would you give to a fellow data analysis student on using AI to create README documents? (lastname_firstname_dawai_week01_advice.txt) (4p)\n\nList 2‚Äì3 pieces of advice based on your own experience using AI for this specific task.\n\n\nImportant notice\n\nDo not use AI to help you generate the advice. We want to hear your personal examples and reflections, not AI-generated suggestions.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment_04.html",
    "href": "assignments/assignment_04.html",
    "title": "Research presentation after joining tables",
    "section": "",
    "text": "Create groups of 2-3 people who use the same coding language.\n\nTask\nAsk AI to give you a list of research questions that can be answered using many (at least 4) of the hotel data sets provided. Iterate with AI to find an interesting question using a persona. * Write code that joins the required data\nOption A (easy)\n\nWrite code to do the analysis (conditional comparison / some simple regression)\nProduce a single figure that shows the answer (or a simple version of the answer) to the research question (e.g.¬†scatterplot and regression line, or boxplot, etc) Create a min. 6 - max. 10 pages slide show to illustrate the research question, the merging process, the figure and the conclusion.\n\nFor example, you can use the following slide show structure, each bit is 1-2 slides:\n\nResearch question and why/for whom you picked it\nData and variables\nDetailed steps of joining tables to get to the work data (here be specific, include schema, keys used, join types, 2-4 slides)\nSteps of creating your variables for analysis\nResult and interpretation\n\nOption B (advanced)\n\nIn addition to the tasks in Option A, choose another real-world dataset that could be merged with the hotel data.\nChoose an additional question that can be answered using the hotel data and the newly merged dataset.\nAdd two slides where you 1) describe how you joined the two datasets and 2) include a figure that shows your finding\n\n\n\nSubmit:\n\nSlideshow in pdf format: ‚Äúlastname1_lastname2_lastname3_slides.pdf‚Äù\nReproducible code ‚Äúlastname1_lastname2_lastname3_code.pdf‚Äù",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignments/assignment_07.html",
    "href": "assignments/assignment_07.html",
    "title": "Simulation app",
    "section": "",
    "text": "Create a simulation app of an important statistical phenomenon.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 7"
    ]
  },
  {
    "objectID": "code/index.html",
    "href": "code/index.html",
    "title": "Code Used for case studies",
    "section": "",
    "text": "Code for sentiment analysis of football manager interviews.\nLocation: code/interviews\nFiles include:\n\n\n\nScripts:\n\nDomain lexicon creation in R\nscript in R requires API key setup in R\nScript in Python ‚Äì make sure you set your own API key",
    "crumbs": [
      "Home",
      "Resources",
      "Code"
    ]
  },
  {
    "objectID": "code/index.html#interview-analysis",
    "href": "code/index.html#interview-analysis",
    "title": "Code Used for case studies",
    "section": "",
    "text": "Code for sentiment analysis of football manager interviews.\nLocation: code/interviews\nFiles include:\n\n\n\nScripts:\n\nDomain lexicon creation in R\nscript in R requires API key setup in R\nScript in Python ‚Äì make sure you set your own API key",
    "crumbs": [
      "Home",
      "Resources",
      "Code"
    ]
  },
  {
    "objectID": "code/index.html#data-analysis-templates",
    "href": "code/index.html#data-analysis-templates",
    "title": "Code Examples",
    "section": "Data Analysis Templates",
    "text": "Data Analysis Templates\nAdditional code examples will be added as the course progresses.",
    "crumbs": [
      "Home",
      "Resources",
      "Code"
    ]
  },
  {
    "objectID": "data/austria-hotels/hotel-data-readme.html",
    "href": "data/austria-hotels/hotel-data-readme.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "data/austria-hotels/hotel-data-readme.html#info",
    "href": "data/austria-hotels/hotel-data-readme.html#info",
    "title": "",
    "section": "Info",
    "text": "Info\nThis dataset contains realistic data on hotels across Austria.\n\nThis dataset was generated programmatically with the generate_austrian_hotels_data.R script to ensure realistic relationships between variables while maintaining privacy.\n\nThe scipt was writen by Claude AI, Sonnet 3.7, 2025-03-15, and reviwed and approved by Gabor 2025-03-17\n\nThe dataset consists of multiple related tables that can be combined.\nThe data patterns are based on typical hotel industry metrics but do not represent actual hotels."
  },
  {
    "objectID": "data/austria-hotels/hotel-data-readme.html#dataset-overview",
    "href": "data/austria-hotels/hotel-data-readme.html#dataset-overview",
    "title": "",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe dataset includes hotels across Austrian cities with data on occupancy, pricing, tourism statistics, and economic indicators.\n\nFiles\nAll files are located in the data/raw/ directory:\n\n\n\n\n\n\n\n\n\nFile\nDescription\nRows\nKey Columns\n\n\n\n\nhotels.csv\nBasic hotel information\n200\nhotel_id (PK)\n\n\ncities.csv\nCity information\n10\ncity (PK)\n\n\nmonthly_occupancy.csv\nMonthly hotel performance metrics\n~3,800\nhotel_id, month, year\n\n\ncity_tourism.csv\nMonthly tourism statistics by city\n240\ncity, month, year\n\n\neconomic_indicators.csv\nMonthly economic indicators\n24\nmonth, year\n\n\nreviews.csv\nHotel guest reviews\n~1,700\nreview_id (PK), hotel_id (FK)\n\n\namenities.csv\nList of possible hotel amenities\n10\namenity_id (PK)\n\n\nhotel_amenities.csv\nHotel-amenity relationships\n~1,000\nhotel_id, amenity_id\n\n\n\n## Schema Details\n### hotels.csv Information about individual hotels.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nPrimary key\n\n\nhotel_name\ncharacter\nHotel name\n\n\ncity\ncharacter\nCity where hotel is located\n\n\nstar_rating\ninteger\nHotel quality rating (3-5 stars)\n\n\nrooms\ninteger\nNumber of rooms in the hotel\n\n\nyear_built\ninteger\nYear the hotel was built\n\n\n\n### cities.csv Information about Austrian cities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\ncity\ncharacter\nCity name (primary key)\n\n\nprovince\ncharacter\nAustrian province\n\n\npopulation\ninteger\nCity population\n\n\ntourism_rank\ninteger\nTourism popularity rank (1 = highest)\n\n\n\n### monthly_occupancy.csv Monthly hotel performance metrics.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\noccupancy_rate\nnumeric\nPercentage of rooms occupied (0.0-1.0)\n\n\navg_daily_rate\nnumeric\nAverage price per night in EUR\n\n\nrevenue_per_room\nnumeric\nRevenue per available room (RevPAR)\n\n\n\n### city_tourism.csv Monthly tourism statistics for each city.\n\n\n\nColumn\nType\nDescription\n\n\n\n\ncity\ncharacter\nCity name\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\ntourist_arrivals\ninteger\nNumber of tourists arriving\n\n\nevent_days\ninteger\nNumber of event days in the month\n\n\navg_stay_length\nnumeric\nAverage length of stay in days\n\n\n\n### economic_indicators.csv Monthly economic indicators for Austria.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nmonth\ninteger\nMonth (1-12)\n\n\nyear\ninteger\nYear (2023-2024)\n\n\ninflation_rate\nnumeric\nMonthly inflation rate (decimal)\n\n\nunemployment\nnumeric\nUnemployment rate (decimal)\n\n\nconsumer_confidence\nnumeric\nConsumer confidence index\n\n\n\n### reviews.csv Hotel guest reviews.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nreview_id\ninteger\nPrimary key\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\nrating\nnumeric\nRating (1.0-5.0)\n\n\nreview_date\ndate\nDate of the review\n\n\n\n### amenities.csv List of possible hotel amenities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\namenity_id\ninteger\nPrimary key\n\n\namenity_name\ncharacter\nName of the amenity\n\n\n\n### hotel_amenities.csv Many-to-many relationship between hotels and amenities.\n\n\n\nColumn\nType\nDescription\n\n\n\n\nhotel_id\ninteger\nForeign key to hotels.csv\n\n\namenity_id\ninteger\nForeign key to amenities.csv"
  },
  {
    "objectID": "data/index.html",
    "href": "data/index.html",
    "title": "Course Datasets",
    "section": "",
    "text": "There are three main case studies used throughout the course:",
    "crumbs": [
      "Home",
      "Resources",
      "Data"
    ]
  },
  {
    "objectID": "data/index.html#simulated-austrian-hotels",
    "href": "data/index.html#simulated-austrian-hotels",
    "title": "Course Datasets",
    "section": "1. Simulated Austrian Hotels",
    "text": "1. Simulated Austrian Hotels\nA realistic simulated dataset of hotels across Austria for practicing data wrangling and table joins. Contains 8 related tables with hotels, cities, occupancy, tourism, and economic data.\nUsed in: Week 4 (Joining Tables)\nKey features: Multiple join types, one-to-many relationships, composite keys\nFiles: 200 hotels across 10 Austrian cities with 2 years of monthly data\n‚Üí Explore Austrian Hotels Data",
    "crumbs": [
      "Home",
      "Resources",
      "Data"
    ]
  },
  {
    "objectID": "data/index.html#world-values-survey-wvs",
    "href": "data/index.html#world-values-survey-wvs",
    "title": "Course Datasets",
    "section": "2. World Values Survey (WVS)",
    "text": "2. World Values Survey (WVS)\nThe 7th Wave of the World Values Survey dataset, cleaned and merged with World Bank GDP data. Available both as individual responses and country-level aggregations.\nUsed in: Week 2 (Data Documentation), Week 3 (Report Writing)\nKey features: International survey data, economic indicators, multiple aggregation levels\nFiles: Individual responses (~2,000 sample) and country-year summaries\n‚Üí Explore WVS Data",
    "crumbs": [
      "Home",
      "Resources",
      "Data"
    ]
  },
  {
    "objectID": "data/index.html#football-manager-interviews",
    "href": "data/index.html#football-manager-interviews",
    "title": "Course Datasets",
    "section": "3. Football Manager Interviews",
    "text": "3. Football Manager Interviews\nPost-match interview texts from football managers for sentiment analysis and NLP practice. Includes multiple rating systems for comparison and validation.\nUsed in: Week 5 (Text as Data I), Week 6 (Text as Data II)\nKey features: Sentiment analysis, API integration, human vs AI comparison\nFiles: Interview texts, domain lexicons, rating comparisons\n‚Üí Explore Interview Data",
    "crumbs": [
      "Home",
      "Resources",
      "Data"
    ]
  },
  {
    "objectID": "data/index.html#data-access",
    "href": "data/index.html#data-access",
    "title": "Course Datasets",
    "section": "Data Access",
    "text": "Data Access\nAll datasets are included in the course repository. Each folder contains: - Raw and processed data files - Detailed documentation and codebooks\n- Example code for data processing - README files with usage instructions",
    "crumbs": [
      "Home",
      "Resources",
      "Data"
    ]
  },
  {
    "objectID": "data/index.html#technical-notes",
    "href": "data/index.html#technical-notes",
    "title": "Course Datasets",
    "section": "Technical Notes",
    "text": "Technical Notes\n\nData formats: CSV, Excel (.xlsx), R scripts\nAll code tested with R (tidyverse) and Python (pandas)\nReproducible workflows with documented processing steps\nAPI examples require OpenAI account (budget ~$5 for course)",
    "crumbs": [
      "Home",
      "Resources",
      "Data"
    ]
  },
  {
    "objectID": "data/VWS/index.html",
    "href": "data/VWS/index.html",
    "title": "World Values Survey (WVS) Data",
    "section": "",
    "text": "This folder contains cleaned and processed data from the 7th Wave of the World Values Survey, merged with World Bank GDP indicators. The WVS is a global research project that explores people‚Äôs values and beliefs across different countries and cultures.\nUsed in: Week 2 (Data Documentation), Week 3 (Report Writing)"
  },
  {
    "objectID": "data/VWS/index.html#overview",
    "href": "data/VWS/index.html#overview",
    "title": "World Values Survey (WVS) Data",
    "section": "",
    "text": "This folder contains cleaned and processed data from the 7th Wave of the World Values Survey, merged with World Bank GDP indicators. The WVS is a global research project that explores people‚Äôs values and beliefs across different countries and cultures.\nUsed in: Week 2 (Data Documentation), Week 3 (Report Writing)"
  },
  {
    "objectID": "data/VWS/index.html#files",
    "href": "data/VWS/index.html#files",
    "title": "World Values Survey (WVS) Data",
    "section": "Files",
    "text": "Files\n\nProcessed Data\n\nWVS_random_subset2000.csv - Random subset of 2,000 respondents stratified by country\nWVS_GDP_merged_data.csv - Country-level aggregated data merged with GDP and population indicators\nWVS_subset.csv - Selected variables from full dataset (all respondents, waves 1-7)\n\n\n\nDocumentation\n\ncodebook.pdf - Official WVS codebook with variable definitions and survey methodology\nREADME_WVS.md - Data processing documentation and variable descriptions\n\n\n\nCode\n\ncleaning.R - R script that processes raw WVS data and creates the cleaned datasets"
  },
  {
    "objectID": "data/VWS/index.html#data-structure",
    "href": "data/VWS/index.html#data-structure",
    "title": "World Values Survey (WVS) Data",
    "section": "Data Structure",
    "text": "Data Structure\n\nWVS_random_subset2000.csv\n\nObservations: ~2,000 individual respondents\nVariables: Demographics (country, interview date, weights) + survey items (Q1-Q89, Q260-Q290)\nPurpose: Manageable sample for exploration and practice\n\n\n\nWVS_GDP_merged_data.csv\n\nObservations: Country-year level (Wave 7 only)\nVariables: Aggregated survey responses + World Bank indicators\nKey variables:\n\nB_COUNTRY_ALPHA: ISO3 country code\nA_YEAR: Survey year (varies by country, 2017-2023)\nQ1-Q89: Aggregated survey responses (means for numeric, modes for categorical)\nGDP_USD_PPP_per_capita: GDP per capita in PPP terms\nPopulation: Country population"
  },
  {
    "objectID": "data/VWS/index.html#usage-notes",
    "href": "data/VWS/index.html#usage-notes",
    "title": "World Values Survey (WVS) Data",
    "section": "Usage Notes",
    "text": "Usage Notes\n\nNegative codes (-1 to -5) in original data have been recoded as NA\nRandom sampling uses seed 20250124 for reproducibility\nGDP data covers 2017-2023 to match survey timing variations\nSee README_WVS.md for detailed processing steps"
  },
  {
    "objectID": "data/VWS/index.html#research-applications",
    "href": "data/VWS/index.html#research-applications",
    "title": "World Values Survey (WVS) Data",
    "section": "Research Applications",
    "text": "Research Applications\nCommon research questions using this data: - Relationship between income level and trust/happiness - Cultural differences in gender attitudes - Economic development and social values"
  },
  {
    "objectID": "learn-more/beyond.html",
    "href": "learn-more/beyond.html",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "",
    "text": "Learning more\n\n\nBeyond: Suggested readings and resources to learn more\nG√°bor‚Äôs collection of recommended readings, listening. Wide variety from practical to business and nerdy stuff.",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "learn-more/beyond.html#basics",
    "href": "learn-more/beyond.html#basics",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "Basics",
    "text": "Basics\n\nCore readings\n\nEthan Mollick ‚ÄúCo-Intelligence: Living and Working with AI‚Äù Penguin Random House 2024\nAnton Korinek ‚ÄúGenerative AI for Economic Research: Use Cases and Implications for Economists,‚Äù Journal of Economic Literature 61(4) December 2024 Update 1‚Äì74\n\n\n\nImportant reviews\n\nReview of LLMs by Simon Willison\nMachines of Loving Grace Dario Amodei\n\n\n\nPrompting\n\nAI Frontiers in Plain English: Prompt Engineering guide from Google with LM Notebook Part 1. LLM output configurations + others\n\n\n\nUnderstanding LLMs\n\nGlossary of LLM terms Glossary of LLM Terms\nFinancial Times: How AI Large Language Models Work\nThe Economist: How Large Language Models Work\nThinking like AI\nWhat‚Äôs an LLM context window and why is it getting larger? IBM research on context window\n\n\n\nAI and business / management\n\nStrategy in business Build a winning AI strategy, HBR 2023\nInterview with Rafella Sadun on Reskilling workforce with AI from MIT Sloan Review",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "learn-more/beyond.html#additional-readings",
    "href": "learn-more/beyond.html#additional-readings",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "Additional readings",
    "text": "Additional readings\n\nConsequence of AI\nHow NLP was killed by Transformers/ LLMs in Quant magazine 2025 April\n\n\nBlogs, newsletters\n\nBlog post by Posit Text Summarization, Translation, and Classification using LLMs: mall does it all\nSimon Willis blog post LLM and coding\nEthan Mollick substack: One useful thing\nAlpha Signal newsletter\nHow Andrej Karpathy is adopting AI assisted coding.\nBlog by Anthropic Claude explains ‚Äì cool collection on ideas for coding with Claude\n\n\n\nVideo Resources on AI\n\nAndrej Karpathy Introduction to Large Language Models ‚Äì 1hs overview, a great start\nAndrej Karpathy Deep Dive into LLMs like ChatGPT ‚Äì 3hs comprehensive updated version of the Intro video\nAndrej Karpathy: ‚ÄúLet‚Äôs build GPT: from scratch, in code, spelled out‚Äù\nInterview with a great Sendhil Mullainathan on direction AI",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "learn-more/beyond.html#deeper-stuff-on-ai",
    "href": "learn-more/beyond.html#deeper-stuff-on-ai",
    "title": "Beyond: suggested readings and resources to learn more",
    "section": "Deeper stuff on AI",
    "text": "Deeper stuff on AI\n\nArtificial intelligence learns to reason in Science",
    "crumbs": [
      "Home",
      "Resources",
      "Learn More"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "I‚Äôm adding material to learn-more folder. You can start with the beyond page."
  },
  {
    "objectID": "resources.html#learn-more",
    "href": "resources.html#learn-more",
    "title": "Resources",
    "section": "",
    "text": "I‚Äôm adding material to learn-more folder. You can start with the beyond page."
  },
  {
    "objectID": "resources.html#data",
    "href": "resources.html#data",
    "title": "Resources",
    "section": "Data",
    "text": "Data\nCourse datasets are available in the data folder.\nThere are three case studies that we use: 1. A simulated set of data tables on hotels in Austria 2. The World Value Survey 3. A series of interview texts\nData Overview"
  },
  {
    "objectID": "resources.html#code",
    "href": "resources.html#code",
    "title": "Resources",
    "section": "Code",
    "text": "Code\nCode examples and templates are available in the code folder.\nCode Overview"
  },
  {
    "objectID": "week01/index.html",
    "href": "week01/index.html",
    "title": "Week 01: LLM Review",
    "section": "",
    "text": "Week 01: LLM Review\n\n\nIntroduction to Large Language Models and their applications in data analysis",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#learning-objectives",
    "href": "week01/index.html#learning-objectives",
    "title": "Week 01: LLM Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, students will:\n\nUnderstand core concepts and architecture behind large language models (LLMs)\nLearn how to incorporate AI into data analysis workflows effectively\nDistinguish between ‚ÄúCyborg‚Äù and ‚ÄúCentaur‚Äù approaches to AI collaboration\nExperience the ‚Äújagged frontier‚Äù of LLM capabilities through hands-on practice\nCritically assess capabilities and limitations of AI tools in academic contexts\nDevelop foundational prompt engineering skills for data analysis tasks",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#class-materials",
    "href": "week01/index.html#class-materials",
    "title": "Week 1: LLM Review",
    "section": "Class Materials",
    "text": "Class Materials\n\n\nüìä Slideshow\n\n\nLLM Concepts Presentation\nKey Topics Covered:\n\nWhat are Large Language Models?\nThe Transformer architecture and tokenization\nCyborg vs Centaur approaches to AI collaboration\nThe ‚Äújagged frontier‚Äù of AI capabilities\nPrompt engineering fundamentals\nAI as bs generator and Brandolini‚Äôs Law\n\n\n\n\n\nüìö Required Reading\n\n\n\nWhich AI Model to Choose?\nUpdated guide for 2025\nReview the FT graph for class activity (see below)\n\nOptional Background:\n\nEthan Mollick: ‚ÄúCo-Intelligence: Living and Working with AI‚Äù (Chapters 1-2)\n\n\n\n\n\nüéØ Class Activity\n\n\nThe Financial Times Challenge\nTake a look at this excellent Financial Times visualization showing the market reaction to Trump‚Äôs tariff announcements.\nYour Mission:\nReproduce this chart as accurately as possible in the shortest time using AI assistance.\nLearning Goals:\n\nExperience AI-assisted data visualization\nPractice prompt engineering for specific tasks\nUnderstand the balance between human direction and AI execution, risks of relying too much on AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#assignment",
    "href": "week01/index.html#assignment",
    "title": "Week 01: LLM Review",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nAssignment 1: Reproduce the FT Graph\n\n\n\nDue: Before Week 2\nChoose your approach:\n\nOption A (Standard): Use AI to find data and recreate the visualization with same design principles\nOption B (Advanced): Build an interactive dashboard that updates dynamically\n\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#preparation",
    "href": "week01/index.html#preparation",
    "title": "Week 1: LLM Review",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nBefore Class\n\n\n\n\nNo specific preparation required for Week 1\nCome ready to discuss your current experience with AI tools\nBring examples of where you‚Äôve encountered AI in your work/studies",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#discussion-questions",
    "href": "week01/index.html#discussion-questions",
    "title": "Week 01: LLM Review",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nEnd of Week Reflection:\n\nPersonal AI Experience: How have you already incorporated AI into your routine? Which model feels most natural to you?\nError Management: How do you currently deal with AI hallucinations or imperfect answers? What strategies emerged during the FT graph exercise?\nThe Jagged Frontier: What tasks do you expect AI to excel at? Where do you think it will struggle? Did the visualization exercise match your expectations?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#tools-and-resources",
    "href": "week01/index.html#tools-and-resources",
    "title": "Week 1: LLM Review",
    "section": "Tools and Resources",
    "text": "Tools and Resources\nRecommended AI Platforms for this course:\n\nChatGPT 4o/o1 - Excellent for coding and data analysis\nClaude 3.5 Sonnet - Great for research and writing tasks\n\nGitHub Copilot - For integrated coding assistance\n\nGetting Started:\n\nMost tasks can be accomplished with free tiers\nConsider paid subscriptions for intensive work ($20/month typical)\nSee AI Model Comparison Guide for detailed recommendations",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#week-1-outcomes",
    "href": "week01/index.html#week-1-outcomes",
    "title": "Week 1: LLM Review",
    "section": "Week 1 Outcomes",
    "text": "Week 1 Outcomes\nBy completing Week 1, you should:\n\n‚úÖ Understand what LLMs can and cannot do reliably\n‚úÖ Have experience with AI-assisted data visualization\n\n‚úÖ Recognize the importance of human oversight in AI workflows\n‚úÖ Be prepared to use AI as a collaborative tool throughout the course\n\n\n\n\n\n\n\n\nAcademic Integrity Note\n\n\n\nThis course teaches you to use AI as a powerful assistant while maintaining your responsibility as the analyst and author. Always verify AI outputs, cite your methods, and ensure you understand the analysis you‚Äôre presenting.\n\n\nNext Week: Week 2 - Data Discovery and Documentation where we‚Äôll use AI to understand and document complex datasets.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week02/assets/da-background.html",
    "href": "week02/assets/da-background.html",
    "title": "Data Analysis Background",
    "section": "",
    "text": "This is based on B√©k√©s-K√©zdi: Data Analysis for Business, Econoomics, and Policy (2021, Cambridge University Press), Chapter 02 Preparing data for analysis\nSlideshow is available here: Chapter 02 slides"
  },
  {
    "objectID": "week02/assets/da-background.html#variables",
    "href": "week02/assets/da-background.html#variables",
    "title": "Data Analysis Background",
    "section": "Variables",
    "text": "Variables\n\nVariable types\n\ncontinuous or discrete or qualitative\nbinary\nflow and stock\n\n\n\nEncoding\n\nbinary\nnumeric\nstring (text)\ncategorical / factor (maybe ordinal)\n\n\n\nMeaning of variable values\n\nwhat are they measuring\nunit of measurement\nsource\n\n\n\nOther Information\n\ndescriptive statistics\ncoverage / share of missing values"
  },
  {
    "objectID": "week03/assets/analysis_notebook (1).html",
    "href": "week03/assets/analysis_notebook (1).html",
    "title": "Income and Trust: Analysis Notebook",
    "section": "",
    "text": "setwd(\"C:/Users/bekes/Documents/GitHub/\")\npath =\"da-w-ai/data/VWS/\"\ndf &lt;- read_csv(paste0(path, \"WVS_GDP_merged_data.csv\"))\n\nRows: 66 Columns: 97\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): B_COUNTRY_ALPHA, iso3c\ndbl (95): A_YEAR, Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, Q11, Q12, Q13, Q1...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf &lt;- df %&gt;%\n  mutate(baseline_trust = 2 - Q57,\n         alt_trust = 4 - rowMeans(select(., Q59:Q63), na.rm=TRUE) + 1,\n         log_gdppc = log(GDP_USD_PPP_per_capita))\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range (`stat_smooth()`).\nRemoved 3 rows containing non-finite outside the scale range (`stat_smooth()`).\n\n\nWarning: Failed to fit group -1.\nCaused by error in `terms.formula()`:\n! argument is not a valid model\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nsummary(lm(baseline_trust ~ log_gdppc, data = df))\n\n\nCall:\nlm(formula = baseline_trust ~ log_gdppc, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.23286 -0.09800 -0.01321  0.05304  0.44896 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.80042    0.17794  -4.498 3.13e-05 ***\nlog_gdppc    0.10390    0.01806   5.751 3.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1262 on 61 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.3516,    Adjusted R-squared:  0.341 \nF-statistic: 33.08 on 1 and 61 DF,  p-value: 3.044e-07\n\nsummary(lm(alt_trust ~ log_gdppc, data = df))\n\n\nCall:\nlm(formula = alt_trust ~ log_gdppc, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53529 -0.12054  0.00505  0.12603  0.45564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0880     0.2975   3.657 0.000533 ***\nlog_gdppc     0.1372     0.0302   4.544 2.67e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2109 on 61 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.2529,    Adjusted R-squared:  0.2406 \nF-statistic: 20.64 on 1 and 61 DF,  p-value: 2.669e-05"
  },
  {
    "objectID": "week03/assets/trust_income_report.html",
    "href": "week03/assets/trust_income_report.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "week03/assets/trust_income_report.html#introduction",
    "href": "week03/assets/trust_income_report.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nWe examine whether national income levels are associated with interpersonal trust. Trust is central to economic performance and social cohesion, and understanding its determinants can inform development policy."
  },
  {
    "objectID": "week03/assets/trust_income_report.html#data-measures",
    "href": "week03/assets/trust_income_report.html#data-measures",
    "title": "",
    "section": "Data & Measures",
    "text": "Data & Measures\n\nIncome: GDP per capita, PPP (constant USD, log‚Äëscaled).\nBaseline trust: share answering ‚ÄúMost people can be trusted‚Äù (Q57).\nAlternative trust index: reverse‚Äëcoded average of trust in neighbours, people known personally, first‚Äëtime met, other religion, other nationality (Q59‚ÄìQ63)."
  },
  {
    "objectID": "week03/assets/trust_income_report.html#descriptive-pattern",
    "href": "week03/assets/trust_income_report.html#descriptive-pattern",
    "title": "",
    "section": "Descriptive Pattern",
    "text": "Descriptive Pattern\n\n\n\nBaseline trust vs income"
  },
  {
    "objectID": "week03/assets/trust_income_report.html#regression-results",
    "href": "week03/assets/trust_income_report.html#regression-results",
    "title": "",
    "section": "Regression Results",
    "text": "Regression Results\n\nBaseline trust\nŒ≤‚ÇÅ¬†=¬†0.104 (SE¬†0.018, p¬†=¬†0.000)\n\n\nAlternative trust\nŒ≤‚ÇÅ¬†=¬†0.137 (SE¬†0.030, p¬†=¬†0.000)"
  },
  {
    "objectID": "week03/assets/trust_income_report.html#discussion",
    "href": "week03/assets/trust_income_report.html#discussion",
    "title": "",
    "section": "Discussion",
    "text": "Discussion\nThe positive coefficient indicates that richer countries tend to report higher levels of interpersonal trust. A one‚Äëlog (‚âà¬†2.7√ó) increase in GDP per capita is associated with an average increase of 0.104 points in the baseline trust measure (on a 0‚Äë1 scale). Results hold for the broader five‚Äëitem trust index."
  },
  {
    "objectID": "week03/assets/trust_income_report.html#conclusion",
    "href": "week03/assets/trust_income_report.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nHigher income levels correlate with greater trust. While causality cannot be established here, the pattern supports theories linking economic prosperity to social capital."
  },
  {
    "objectID": "week03/ideas-good-report.html",
    "href": "week03/ideas-good-report.html",
    "title": "Some ideas on good report",
    "section": "",
    "text": "What is a good data analysis presentation"
  },
  {
    "objectID": "week03/ideas-good-report.html#three-types-of-reserach-questions",
    "href": "week03/ideas-good-report.html#three-types-of-reserach-questions",
    "title": "Some ideas on good report",
    "section": "Three types of reserach questions",
    "text": "Three types of reserach questions\n\nPatterns of association ‚Äì focus on pattern discovery\nCausal question ‚Äì identify the effect of an intervention\n\nExperiment\nObservational data\n\nBuild a predictive model ‚Äì focus on selection, understand how model works"
  },
  {
    "objectID": "week03/ideas-good-report.html#let-us-focus-on-association",
    "href": "week03/ideas-good-report.html#let-us-focus-on-association",
    "title": "Some ideas on good report",
    "section": "Let us focus on association",
    "text": "Let us focus on association\nThink questions like\n\nAre people working more hours, make more money/hour?\nHow much less does a car worth when having 1000km more in odometer?\nWhat is the relationship between hotel prices and location / amenities?\n\nCausal would be\n\nIn an intervention, we make some people work more, will they have a high w/h?\n\nPrediction would be\n\nHere is wage data, let us build a model for hourly wage"
  },
  {
    "objectID": "week03/ideas-good-report.html#what-to-include-in-a-report",
    "href": "week03/ideas-good-report.html#what-to-include-in-a-report",
    "title": "Some ideas on good report",
    "section": "What to include in a report",
    "text": "What to include in a report\nConsider a few-page report on an association with some possibility for causal analysis (later)\nKey parts of the report\n\nIntroduction (why interesting)\nResearch question\nData description\n\nsource\nvariables\n\nMethods\n\nwhat method you use (e.g.¬†cross section OLS)\nregression you estimate precisely\n\nResults\n\nInclude exhibits (think about graphs, tables)\nDecide on main result and focus on explaining it (e.g.¬†\\(x\\) coefficient)\n\nHint: think for the reader ‚Äì what would help them understand?\n\n\nConclusion\n\nShort: 1-2 para\nDiscussion to summarize result (in plain English)\nPlace result in context\n\nis it causal\nexternal validity\n\n(no need for future research)"
  },
  {
    "objectID": "week04/assets/presentation-plan.html",
    "href": "week04/assets/presentation-plan.html",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "",
    "text": "Data table joins combine information from multiple tables based on matching values.\nKey Terms:\n\nPrimary Key: Unique identifier for each row (e.g., hotel_id)\nForeign Key: Column that references a primary key in another table\nJoin Key: Column(s) used to match rows between tables\n\nCommon Join Types:\n\nInner Join: Returns only matching rows\nLeft Join: Returns all rows from left table, matching rows from right\nRight Join: Returns all rows from right table, matching rows from left\nFull Join: Returns all rows from both tables"
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-1-key-vocabulary",
    "href": "week04/assets/presentation-plan.html#slide-1-key-vocabulary",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "",
    "text": "Data table joins combine information from multiple tables based on matching values.\nKey Terms:\n\nPrimary Key: Unique identifier for each row (e.g., hotel_id)\nForeign Key: Column that references a primary key in another table\nJoin Key: Column(s) used to match rows between tables\n\nCommon Join Types:\n\nInner Join: Returns only matching rows\nLeft Join: Returns all rows from left table, matching rows from right\nRight Join: Returns all rows from right table, matching rows from left\nFull Join: Returns all rows from both tables"
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-2-more-vocabulary",
    "href": "week04/assets/presentation-plan.html#slide-2-more-vocabulary",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 2: More Vocabulary",
    "text": "Slide 2: More Vocabulary\nRelationship Types:\n\nOne-to-One: Each row in Table A matches exactly one row in Table B\n\nExample: Hotel details and hotel star ratings\n\nOne-to-Many: Each row in Table A matches multiple rows in Table B\n\nExample: Hotel to guest reviews\n\nMany-to-Many: Multiple rows in Table A match multiple rows in Table B\n\nExample: Hotels and amenities (each hotel has many amenities, each amenity exists in many hotels)\n\n\nTidy Data Principles: - Each variable forms a column - Each observation forms a row - Each type of observational unit forms a table"
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-3-one-to-one-join-perfect-match",
    "href": "week04/assets/presentation-plan.html#slide-3-one-to-one-join-perfect-match",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 3: One-to-One Join (Perfect Match)",
    "text": "Slide 3: One-to-One Join (Perfect Match)\nScenario: Two tables with exactly the same hotels\nTable A: Austrian Hotels | hotel_id | hotel_name | city | |‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äì| | 1 | Hotel Sacher | Vienna | | 2 | Hotel Imperial | Vienna | | 3 | Schloss Fuschl Resort | Salzburg | | 4 | Grand Hotel Wien | Vienna | | 5 | Hotel Goldener Hirsch | Salzburg |\nTable B: Hotel Ratings | hotel_id | stars | avg_price_eur | |‚Äî‚Äî‚Äî-|‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî| | 1 | 5 | 450 | | 2 | 5 | 420 | | 3 | 5 | 380 | | 4 | 5 | 350 | | 5 | 5 | 320 |\nInner Join Result (same as Left, Right, and Full Join in this case):\nhotels_with_ratings &lt;- inner_join(hotels, ratings, by = \"hotel_id\")"
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-4-one-to-one-join-partial-match",
    "href": "week04/assets/presentation-plan.html#slide-4-one-to-one-join-partial-match",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 4: One-to-One Join (Partial Match)",
    "text": "Slide 4: One-to-One Join (Partial Match)\nScenario: Some hotels appear in one table but not the other\nTable A: Austrian Hotels | hotel_id | hotel_name | city | |‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äì| | 1 | Hotel Sacher | Vienna | | 2 | Hotel Imperial | Vienna | | 3 | Schloss Fuschl Resort | Salzburg | | 4 | Grand Hotel Wien | Vienna | | 5 | Hotel Goldener Hirsch | Salzburg |\nTable B: Boutique Hotels | hotel_id | is_boutique | room_count | |‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî| | 1 | TRUE | 150 | | 2 | TRUE | 138 | | 5 | TRUE | 70 | | 6 | TRUE | 45 | | 7 | TRUE | 62 |\nDifferent Join Types:\nInner Join (only matching hotel_ids):\nboutique_overlap &lt;- inner_join(hotels, boutique, by = \"hotel_id\")\n# Returns hotels 1, 2, and 5\nLeft Join (all hotels from Table A):\nall_hotels_boutique_info &lt;- left_join(hotels, boutique, by = \"hotel_id\")\n# Returns hotels 1-5, with NULL for boutique info for 3 and 4\nRight Join (all boutique hotels):\nall_boutique_hotel_info &lt;- right_join(hotels, boutique, by = \"hotel_id\")\n# Returns hotels 1, 2, 5, 6, 7 with NULL for hotel info for 6 and 7\nFull Join (all hotels from both tables):\nall_hotels_combined &lt;- full_join(hotels, boutique, by = \"hotel_id\")\n# Returns hotels 1-7, with NULLs where information is missing"
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-5-one-to-one-join-summary",
    "href": "week04/assets/presentation-plan.html#slide-5-one-to-one-join-summary",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 5: One-to-One Join summary",
    "text": "Slide 5: One-to-One Join summary\nInner Join: Only keeps rows that exist in both tables (intersection) Left Join: Keeps all rows from the left table, adds matching data from right Right Join: Keeps all rows from the right table, adds matching data from left Full Join: Keeps all rows from both tables (union)\nEconomic Insight: Join type selection impacts analysis conclusions. For example, calculating average room prices would differ based on which hotels are included in the final dataset."
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-6-one-to-many-join",
    "href": "week04/assets/presentation-plan.html#slide-6-one-to-many-join",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 6: One-to-Many Join",
    "text": "Slide 6: One-to-Many Join\nScenario: Each hotel has multiple guest reviews\nTable A: Austrian Hotels | hotel_id | hotel_name | city | |‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äì| | 1 | Hotel Sacher | Vienna | | 2 | Hotel Imperial | Vienna | | 3 | Schloss Fuschl Resort | Salzburg |\nTable B: Guest Reviews | review_id | hotel_id | rating | review_date | |‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî-| | 101 | 1 | 4.8 | 2023-06-15 | | 102 | 1 | 4.7 | 2023-07-22 | | 103 | 1 | 4.9 | 2023-08-05 | | 104 | 2 | 4.6 | 2023-06-10 | | 105 | 2 | 4.8 | 2023-07-15 | | 106 | 3 | 4.5 | 2023-08-20 |\nJoin Result:\nhotels_with_reviews &lt;- left_join(hotels, reviews, by = \"hotel_id\")\nThis creates a table with 6 rows (one for each review) where hotel information is duplicated for hotels with multiple reviews."
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-7-one-to-many-join-visualization",
    "href": "week04/assets/presentation-plan.html#slide-7-one-to-many-join-visualization",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 7: One-to-Many Join Visualization",
    "text": "Slide 7: One-to-Many Join Visualization\nKey Points: - The resulting table has more rows than the ‚Äúone‚Äù table - Information from the ‚Äúone‚Äù table gets duplicated for each matching row in the ‚Äúmany‚Äù table - Common use: parent-child relationships in data (e.g., cities to buildings, companies to employees)\nData Analysis Impact: - Be careful with aggregations after a one-to-many join"
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-8-join-with-composite-keys",
    "href": "week04/assets/presentation-plan.html#slide-8-join-with-composite-keys",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 8: Join with Composite Keys",
    "text": "Slide 8: Join with Composite Keys\nScenario: Hotels with seasonal pricing\nTable A: Austrian Hotels | hotel_id | hotel_name | city | |‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äì| | 1 | Hotel Sacher | Vienna | | 2 | Hotel Imperial | Vienna | | 3 | Schloss Fuschl Resort | Salzburg |\nTable B: Seasonal Hotel Pricing | hotel_id | season | avg_price_eur | occupancy_rate | |‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚Äî-| | 1 | Summer | 520 | 0.92 | | 1 | Winter | 480 | 0.85 | | 1 | Christmas | 650 | 0.98 | | 2 | Summer | 490 | 0.88 | | 2 | Winter | 450 | 0.82 | | 3 | Summer | 420 | 0.95 | | 3 | Winter | 550 | 0.90 |\nComposite Key Join:\nseasonal_hotel_data &lt;- left_join(hotels, seasonal_prices, by = c(\"hotel_id\"))\nThis creates an incorrect result with duplicated rows. Instead, we need both the hotel_id and season to uniquely identify a record in the pricing table.\nProper Use Case: When joining hotel occupancy data that varies by both hotel and date/season."
  },
  {
    "objectID": "week04/assets/presentation-plan.html#slide-9-key-takeaways",
    "href": "week04/assets/presentation-plan.html#slide-9-key-takeaways",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "Slide 9: Key Takeaways",
    "text": "Slide 9: Key Takeaways\n\nChoose the right join type based on your analytical needs:\n\nInner join for strict matching\nLeft/right join when you need to preserve all records from one table\nFull join when you need all possible data\n\nUnderstand your data relationships:\n\nOne-to-one: Simple matching\nOne-to-many: Creates duplication of the ‚Äúone‚Äù side\nMany-to-many: Requires careful handling to avoid combinatorial explosion\n\nComposite keys are essential for:\n\neconomic data at geography* time level\n\nData integrity is crucial:\n\nCheck for unexpected NULLs after joining\nValidate row counts before and after joins\nConsider foreign key constraints in database design"
  },
  {
    "objectID": "week04/assets/presentation-plan.html#ai",
    "href": "week04/assets/presentation-plan.html#ai",
    "title": "Joining Data Tables: A Visual Guide with Austrian Hotels",
    "section": "AI:",
    "text": "AI:\nThis is done with Claude Sonnet 3.7\n‚ÄúI am working on a presentation for economics students on ways of joining tidy data tables. Have 1 or 2 slides on vocabulary. Then 3 slides on 1:1. First, two tables exact same rows. Then second is fewer but full overlap. Third no full overlap, ie some rows only in 1 some in 2. Show what alternative types of join do. Then do 1 to many. Then join on composite. Create a nice example to carry though. One idea is that rows are hotels in Austria. Nice graphs are useful. No limit in slides, just focus on clarity and make it pretty. Can use tikz.‚Äù"
  },
  {
    "objectID": "week05/assets/sentiment-guidelines.html",
    "href": "week05/assets/sentiment-guidelines.html",
    "title": "Guidelines for Rating:",
    "section": "",
    "text": "Positive mentions include praise, satisfaction with performance, optimistic outlook, and appreciation.\nNegative mentions include frustration, disappointment, criticism of performance, or external conditions.\nNeutral statements are neither positive nor negative.\n\n\nExamples:\n\n‚ÄúI‚Äôm extremely proud of how the team played today. Fantastic performance by everyone.‚Äù ‚Üí +2\n‚ÄúWe played well, but there are still areas to improve.‚Äù ‚Üí +1\n‚ÄúIt was a tough match, evenly balanced, nothing much to say.‚Äù ‚Üí 0\n‚ÄúWe weren‚Äôt at our best; it was a frustrating game.‚Äù ‚Üí -1\n‚ÄúI‚Äôm very disappointed. Our performance was unacceptable.‚Äù ‚Üí -2\n\n\n\nFinal Notes:\n\nUse 0 if unsure or if sentiment is mixed without clear dominance."
  },
  {
    "objectID": "week06/assets/api-advanced.html",
    "href": "week06/assets/api-advanced.html",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "",
    "text": "Before diving into APIs, it‚Äôs essential to understand how web communication works at its most basic level. The web runs on HTTP (HyperText Transfer Protocol), a simple, text-based language that lets one computer or application (the client) ask another computer (the server) for data or actions. Whenever you browse a site or call a web service, your client sends an HTTP request and the server answers with an HTTP response.\nAn HTTP request consists of a URL‚Äîthe address of the resource or endpoint you want‚Äîand an HTTP method, which tells the server what you‚Äôd like to do. The two most common methods are GET (to fetch data, like loading a webpage) and POST (to submit data, such as filling in a form). By combining the URL and method (e.g.¬†GET https://example.com/data), the client makes its intentions clear.\nWhen the server receives that request, it processes whatever work is needed‚Äîretrieving files, querying a database, or running a service‚Äîand then sends back a response. Each response begins with a status code (a number indicating success or failure, such as 200 OK for success or 404 Not Found when something is missing) followed by the body containing the actual data or content. For webpages this is usually HTML; for data services (APIs), it‚Äôs often JSON (JavaScript Object Notation).\nThis back-and-forth interaction is called the request-response cycle, and it‚Äôs entirely stateless‚Äîeach request is handled on its own, with no memory of previous requests. That stateless design makes HTTP simple and highly scalable, allowing any client that speaks HTTP (from browsers to Python scripts) to interoperate with any server that understands HTTP. When you move on to APIs, you‚Äôll leverage this same cycle to automate large-scale data retrieval and processing."
  },
  {
    "objectID": "week06/assets/api-advanced.html#http-client-server-communication-and-the-request-response-cycle",
    "href": "week06/assets/api-advanced.html#http-client-server-communication-and-the-request-response-cycle",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "",
    "text": "Before diving into APIs, it‚Äôs essential to understand how web communication works at its most basic level. The web runs on HTTP (HyperText Transfer Protocol), a simple, text-based language that lets one computer or application (the client) ask another computer (the server) for data or actions. Whenever you browse a site or call a web service, your client sends an HTTP request and the server answers with an HTTP response.\nAn HTTP request consists of a URL‚Äîthe address of the resource or endpoint you want‚Äîand an HTTP method, which tells the server what you‚Äôd like to do. The two most common methods are GET (to fetch data, like loading a webpage) and POST (to submit data, such as filling in a form). By combining the URL and method (e.g.¬†GET https://example.com/data), the client makes its intentions clear.\nWhen the server receives that request, it processes whatever work is needed‚Äîretrieving files, querying a database, or running a service‚Äîand then sends back a response. Each response begins with a status code (a number indicating success or failure, such as 200 OK for success or 404 Not Found when something is missing) followed by the body containing the actual data or content. For webpages this is usually HTML; for data services (APIs), it‚Äôs often JSON (JavaScript Object Notation).\nThis back-and-forth interaction is called the request-response cycle, and it‚Äôs entirely stateless‚Äîeach request is handled on its own, with no memory of previous requests. That stateless design makes HTTP simple and highly scalable, allowing any client that speaks HTTP (from browsers to Python scripts) to interoperate with any server that understands HTTP. When you move on to APIs, you‚Äôll leverage this same cycle to automate large-scale data retrieval and processing."
  },
  {
    "objectID": "week06/assets/api-advanced.html#how-apis-work-client-server-interaction-requests-and-responses",
    "href": "week06/assets/api-advanced.html#how-apis-work-client-server-interaction-requests-and-responses",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "How APIs Work: Client-Server Interaction, Requests, and Responses",
    "text": "How APIs Work: Client-Server Interaction, Requests, and Responses\nAt its core, an API works through a client-server interaction over the internet. Let‚Äôs break down the key concepts:\n\nClient and Server: The client is the part that sends a request (this would be your code or application), and the server is the part that receives the request and provides a response (this is the API‚Äôs service, often running on a remote server or in the cloud). In our context, you (or your Python script) are the client, and the API provider‚Äôs system is the server.\nRequest: A request is the message the client sends to the server asking for some action or data. Think of it as filling out an order form or making a specific query. A request typically includes:\n\nAn endpoint URL (the address of the API and the specific service you want).\nA method/verb (often one of the HTTP methods like GET to retrieve data or POST to send data).\nParameters or data (any additional information the server needs, such as the text you want analyzed or a query like a city name for a weather API).\nHeaders including an API key if required (more on API keys soon).\n\nResponse: After the server receives your request and processes it, it sends back a response. The response contains:\n\nStatus code ‚Äì a number that tells you if the request was successful (e.g.¬†200 OK), or if something went wrong (e.g.¬†404 Not Found, 401 Unauthorized).\nData ‚Äì the information you asked for, often in a structured format like JSON (a common text-based data format) or XML. For example, if you requested sentiment analysis, the response data might be a sentiment score or label for your text.\nMetadata or messages ‚Äì sometimes additional info, like how long the request took or usage details.\n\n\n Client-Server Communication: Your application (client) sends an HTTP request to an API‚Äôs server (for example, asking for sentiment analysis on some text). The server then processes that request and sends back an HTTP response containing the result (for instance, the sentiment score). This request-response cycle is the foundation of how we use APIs.\n\nAPI Endpoint: An endpoint is a specific address (URL) that you hit to access a particular service or data from an API. It‚Äôs like a function or feature on the server that you can invoke. For example, a sentiment analysis API might have an endpoint like /analyzeSentiment that you call to get a sentiment result. Each endpoint usually corresponds to one type of task or data.\nAPI Documentation: Because you can‚Äôt see the ‚Äúkitchen‚Äù (the server‚Äôs internal code or database), the API documentation is your guide to what you can request and how to format those requests. It typically lists all available endpoints, what parameters they accept, what kind of output they return, and examples. Good documentation is like a user manual for the API."
  },
  {
    "objectID": "week06/assets/api-advanced.html#rest-apis-the-common-way-to-communicate",
    "href": "week06/assets/api-advanced.html#rest-apis-the-common-way-to-communicate",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "REST APIs: The Common Way to Communicate",
    "text": "REST APIs: The Common Way to Communicate\nWhen people talk about web APIs, they are often referring to REST APIs. REST stands for Representational State Transfer, which is a style of designing networked applications. You don‚Äôt need to remember the term, but here‚Äôs what it means in practice:\n\nUses HTTP: REST APIs use the same protocol your web browser uses ‚Äì HTTP. This means you‚Äôre often calling URLs (web addresses) just like loading a webpage, but instead of a webpage, you get data.\nEndpoints and Resources: In a REST API, different URLs (endpoints) represent different resources or services. For example, GET https://api.openweathermap.org/data/2.5/weather?q=London might retrieve weather data for London. Here, the endpoint /data/2.5/weather is a resource for weather info, and q=London is a parameter specifying the city.\nHTTP Methods (Verbs): REST APIs make use of HTTP methods such as:\n\nGET ‚Äì Retrieve data (for example, get information or fetch results; e.g., get the sentiment of a text or fetch a list of comments).\nPOST ‚Äì Send data or create a new resource (for example, submit a new text to be analyzed, or add a new entry to a database via the API).\nPUT/PATCH ‚Äì Update an existing resource (for example, update a record in a database).\nDELETE ‚Äì Delete a resource.\n\nIn our sentiment analysis example, you might use a GET request if the text is included in the URL or a POST request if you are sending the text in the request body. The key idea is that the method indicates what action you want to perform on the resource.\nStateless Communication: REST APIs are stateless, meaning each request is independent. The server doesn‚Äôt retain information about your previous requests. This is like each request being a separate, self-contained transaction. For instance, if you call the sentiment analysis API twice with two different texts, the server doesn‚Äôt remember the first text when processing the second ‚Äì you need to send all the information it needs each time. The stateless design makes it easier to scale and ensures that each server can handle any request without needing to know what came before.\n\nREST APIs are popular because they are simple, scalable, and use existing web standards. Almost any programming environment can send HTTP requests, so REST makes it easy to interact with services from different languages and platforms. When you hear about APIs from providers like Google, Twitter, or OpenAI, they are usually implemented as RESTful APIs."
  },
  {
    "objectID": "week06/assets/api-advanced.html#other-api-styles-graphql-and-websockets",
    "href": "week06/assets/api-advanced.html#other-api-styles-graphql-and-websockets",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "Other API Styles: GraphQL and WebSockets",
    "text": "Other API Styles: GraphQL and WebSockets\nREST is the most common approach, but it‚Äôs not the only pattern you might encounter. Here are two other API styles, just for your awareness:\n\nGraphQL: GraphQL is an alternative approach where instead of multiple specific endpoints, you have a single endpoint that can handle flexible queries. You send a query describing exactly what data you want, and you get back precisely that data. This can reduce the number of requests needed for complex apps. For example, if a REST API required you to call one endpoint for user info and another for user posts, a GraphQL API could allow you to get both in a single request by specifying that in the query. It‚Äôs powerful for complex data fetching, but also a bit more advanced in usage. (In this course we won‚Äôt be using GraphQL, but it‚Äôs good to know it exists.)\nWebSockets (Real-Time APIs): A WebSocket provides a continuous two-way connection between client and server, allowing data to be sent in real time. This isn‚Äôt a request-response model; it‚Äôs more like an open channel. WebSockets are useful for applications like live chat, streaming data updates, or multiplayer games ‚Äì anywhere you want instant, ongoing data flow. For instance, if you were tracking sentiment on a live stream of tweets, a WebSocket connection could stream new analyses continuously. This is more specialized, so we‚Äôll stick to the request/response style in our work, but you might encounter WebSocket APIs in other contexts (e.g., real-time stock price feeds).\n\nFor most data analysis tasks (like fetching data or sending data for analysis), you‚Äôll be using REST APIs, as they cover the majority of use cases and are easier to get started with.\nGreat, I‚Äôll put together a conceptual overview of how API client libraries (especially in Python, with a mention of R) work, including when and why you‚Äôd use them, and how they simplify HTTP requests behind the scenes. I‚Äôll be back shortly with the draft section you can add to your markdown."
  },
  {
    "objectID": "week06/assets/api-advanced.html#api-client-libraries",
    "href": "week06/assets/api-advanced.html#api-client-libraries",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "API Client Libraries",
    "text": "API Client Libraries\nWhen working with web APIs, you often have the choice of using API client libraries instead of crafting raw HTTP requests. An API client library (sometimes called an SDK or helper library) is a package provided in a specific programming language that wraps the API‚Äôs functionality in convenient functions or classes. In Python especially, many major services offer official or community-maintained libraries to simplify API usage. These libraries act as a language-specific abstraction layer over the API ‚Äì they break the API‚Äôs usual language-agnostic nature to make the developer experience smoother. In practice, this means you can interact with the API using Python objects and methods, rather than manually formatting HTTP requests and parsing responses.\nWhy use a client library? Client libraries simplify and reduce the code you need to write for common API tasks. Instead of manually assembling URLs, query parameters, headers, and JSON payloads for each request, you can call a function or method provided by the library and let it handle those details. For example, a library might provide a method like get_user_tweets(\"Alice\") or openai.ChatCompletion.create(...) ‚Äì behind the scenes, that method will construct the proper HTTP request (with the correct endpoint and parameters), send it, and parse the result for you. In other words, the library ‚Äúabstracts away the HTTP requests and offers more convenient interfaces‚Äù to work with the API. This abstraction typically covers:\n\nRequest formatting and transport: The library builds the correct HTTP requests (URLs, methods, headers, body) and sends them using an HTTP client, so you don‚Äôt have to manually use tools like requests or curl. It also often manages details like authentication headers (API keys, tokens) for you after an initial setup.\nResponse parsing and serialization: Data returned from the API (usually in JSON) is automatically parsed into Python data structures or objects. Similarly, when you provide data to send (like a dictionary or object), the library serializes it to JSON. This spares you from manual JSON formatting.\nError handling and reliability: Good client libraries include error handling logic. They might raise clear exceptions for error responses (rather than you checking HTTP status codes yourself) and handle common issues like rate limiting or retries. This means your code can focus on what you want to do with the API, and the library handles the low-level communication details.\nIdiomatic interface: The library‚Äôs functions and classes are designed to feel natural in the given language. For instance, Python libraries will return Python objects (like dictionaries or custom classes) and use Python naming conventions. This makes the API ‚Äúsimple and intuitive to use‚Äù in that language, as opposed to treating everything as raw text or HTTP mechanics."
  },
  {
    "objectID": "week06/assets/api-advanced.html#making-an-api-call-a-conceptual-walkthrough",
    "href": "week06/assets/api-advanced.html#making-an-api-call-a-conceptual-walkthrough",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "Making an API Call: A Conceptual Walkthrough",
    "text": "Making an API Call: A Conceptual Walkthrough\nLet‚Äôs put all this into a concrete example without diving into code. How would you use an API for a task, say, analyzing sentiment on a large number of texts? Here‚Äôs the high-level process:\n\nFind a Suitable API & Read the Documentation: First, you‚Äôd identify an API that offers sentiment analysis (or whatever task you need). This could be a cloud service like Azure Cognitive Services, IBM Watson NLU, or OpenAI‚Äôs API. You‚Äôd read the documentation to find out the endpoint for sentiment analysis, what inputs it expects (maybe it requires a piece of text or a list of texts, and possibly a language or other settings), and what the output looks like.\nObtain Access (API Key): You sign up for the service to get your API key (or other credentials). For example, you might create a free account and receive a key like abcd1234... that identifies you.\nConstruct the Request: Using the information from the docs, you prepare your API request. For example:\n\nDecide on the HTTP method: sentiment analysis might require a POST request because you‚Äôre sending data (the text).\nDetermine the endpoint URL: e.g., https://api.some-service.com/v1/sentiment.\nPrepare the data format: the API might require JSON. For instance, you might need to send {\"text\": \"I love this product!\"} in the body of the request. If you have multiple texts, maybe it allows an array of texts.\nInclude your API key as instructed (maybe in a header or as a parameter).\n\nSend the Request (Client side): Now you send the request from your client (which could be a Python script, a command-line tool like curl, or an app like Postman for testing). This is when your program reaches out over the internet to the API‚Äôs server with your request details.\nReceive the Response (Server side): The API‚Äôs server processes your input. It runs the sentiment analysis on the text you sent. Then it sends back a response. Let‚Äôs say the response is a JSON object like: {\"sentiment\": \"positive\", \"confidence\": 0.95}, along with an HTTP status code 200 (meaning success). If something was wrong (e.g., missing the API key or the text was too long), you might get an error response instead of explaining what went wrong.\nIntegrate the Results: Your code receives this response data. Now you can use it in your analysis. For instance, your program can take the \"sentiment\": \"positive\" value and record that this particular review was positive. You might loop through all 10,000 reviews, call the API for each, and collect the results. In the end, you could calculate statistics (like 60% of reviews are positive, 30% neutral, 10% negative, etc.) or visualize the data.\n\nThroughout this process, the heavy work (the actual sentiment computation) is done by the API provider‚Äôs servers. Your job is to correctly send requests and process the responses. In practice, you‚Äôd likely write a small script to automate steps 3‚Äì6 so that you can handle many texts sequentially or in parallel.\nEven without writing code here, hopefully, you can see the pattern: find API -&gt; get access -&gt; request -&gt; response -&gt; use data. Once you learn to do this, you can apply it to countless situations, not just sentiment analysis.\nPlease see our api-use.md file for fundamental information."
  },
  {
    "objectID": "week06/assets/api-advanced.html#more",
    "href": "week06/assets/api-advanced.html#more",
    "title": "Supplementary Information for Introduction to APIs",
    "section": "More",
    "text": "More\nListen to Alice Evans explain all things HTTP on this podcast."
  },
  {
    "objectID": "week06/assets/get-ai-api-key.html",
    "href": "week06/assets/get-ai-api-key.html",
    "title": "How to get an API key: OpenAI ChatGPT and Anthropic Claude",
    "section": "",
    "text": "Below is a comprehensive, step-by-step guide for you to obtain and securely store API keys for both OpenAI ChatGPT and Anthropic Claude. It assumes you already have active accounts but need to know exactly where to navigate in each console to generate keys, fund your projects, and follow best practices for management.\nYou‚Äôll need to spulrge a minimum of $5 to get started. That‚Äôs enough for the purpose of this course."
  },
  {
    "objectID": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-openai-chatgpt",
    "href": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-openai-chatgpt",
    "title": "How to get an API key: OpenAI ChatGPT and Anthropic Claude",
    "section": "How to get an API key: OpenAI ChatGPT",
    "text": "How to get an API key: OpenAI ChatGPT\n\nLog in to the OpenAI Platform Go to https://platform.openai.com/account/api-keys and sign in with your ChatGPT credentials if prompted.\nCreate a Project Navigate to https://platform.openai.com/settings/organization/projects and click + Create, enter a project name (e.g., ‚ÄúDA w AI Course‚Äù), then click Create.\nSet up Billing (Add Prepaid Credits) In the left-hand menu under Billing, click Add payment method, enter your credit card details, then purchase the minimum $5 to fund your project.\nGenerate Your API Key Within your project, go to the API Keys tab, click + Create new secret key, give it a descriptive name (e.g., ‚ÄúCourse Project Key‚Äù), and click Create.\nCopy and Secure Your Key The full secret key string will be displayed only once‚Äîcopy it immediately and store it in a password manager or secure vault. You will not be able to view it again.\n\n\nBest Practices\n\nRotate keys every 90 days and revoke any that are no longer in use to limit exposure if compromised.\nMonitor your usage via the Usage dashboard at https://platform.openai.com/account/usage or the Usage tab under Settings.\nNever commit API keys to version control‚Äîadd them to .gitignore or use secret-management tools"
  },
  {
    "objectID": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-anthropic-claude",
    "href": "week06/assets/get-ai-api-key.html#how-to-get-an-api-key-anthropic-claude",
    "title": "How to get an API key: OpenAI ChatGPT and Anthropic Claude",
    "section": "How to get an API key: Anthropic Claude",
    "text": "How to get an API key: Anthropic Claude\n\nLog in to the Anthropic Console Navigate to https://console.anthropic.com and sign in with your Claude account credentials.\nAccess the API Keys Section Click the key icon in the left-hand navigation.\nGenerate a New API Key On the API Keys page, click + Create Key, enter a descriptive name (e.g., ‚ÄúCourse Key‚Äù), and click Add.\nCopy and Secure Your Key The full API key string is shown only once‚Äîcopy it immediately and store it in a secure vault. You will not be able to view it again.\nSet Up Billing or Claim Free Credits In the left navigation, select Billing. If you have trial credits, you may need to verify your phone number to claim them; otherwise, under Buy credits purchase the minimum $5 to fund your usage.\n\n\nBest Practices\n\nRotate keys periodically and delete any unused or compromised keys to maintain security.\nNever expose keys in public repositories‚Äîuse environment variables or secret managers and add any config files with keys to .gitignore.\nMonitor your credit balance and API usage in Billing to avoid unexpected costs."
  },
  {
    "objectID": "week06/assets/walkthrough-wb-fred.html",
    "href": "week06/assets/walkthrough-wb-fred.html",
    "title": "Getting GDP data",
    "section": "",
    "text": "We‚Äôll show two options: World Bank and FRED"
  },
  {
    "objectID": "week06/assets/walkthrough-wb-fred.html#world-bank-gdp-per-capita-via-wb-api",
    "href": "week06/assets/walkthrough-wb-fred.html#world-bank-gdp-per-capita-via-wb-api",
    "title": "Getting GDP data",
    "section": "World Bank GDP per capita via WB API",
    "text": "World Bank GDP per capita via WB API\nAPI-based, no key needed, works in both R and Python.\n\nSame data source (World Bank).\nOfficial packages in Python, R\nNo messy HTML scraping.\nClear structure: select indicator ‚Üí countries ‚Üí year ‚Üí get table.\n\n\nüìà GDP per capita (constant 2015 US$), code: NY.GDP.PCAP.KD\n\n\n\nPython (using wbdata or pandas-datareader):\nimport wbdata\nimport pandas as pd\nfrom datetime import datetime\n\n# Set countries and indicator\ncountries = ['USA', 'HUN', 'DEU']\nindicator = {'NY.GDP.PCAP.KD': 'GDP_per_capita'}\n\n# Get data\ndf = wbdata.get_dataframe(indicator, country=countries, data_date=datetime(2021, 1, 1))\nprint(df.head())\n\n\n\nR (using wbstats):\ninstall.packages(\"wbstats\")\nlibrary(wbstats)\n\n# Set indicator and countries\ngdp_data &lt;- wb(indicator = \"NY.GDP.PCAP.KD\", country = c(\"US\", \"HU\", \"DE\"),\n               startdate = 2021, enddate = 2021)\n\nhead(gdp_data)"
  },
  {
    "objectID": "week06/assets/walkthrough-wb-fred.html#fred",
    "href": "week06/assets/walkthrough-wb-fred.html#fred",
    "title": "Getting GDP data",
    "section": "FRED",
    "text": "FRED\nFRED (Federal Reserve Economic Data) provides economic time series (e.g.¬†GDP, inflation, interest rates).\n\nYou need an API key, but the process is straightforward\nworks in both R and Python with identical logic in both languages: set key ‚Üí request series ‚Üí get dataframe.\n\n\n\nüîë Get API Key\n\nSign up at https://fred.stlouisfed.org/\nGo to your account settings to get your API key.\nCopy your API key, you‚Äôll need it with code\n\n\n\n\n‚úÖ Example: U.S. GDP per capita (A939RC0Q052SBEA)\nDetails at FRED site\n\n\n\nPython (using fredapi)\nfrom fredapi import Fred\n\nfred = Fred(api_key='your_api_key_here')\n\n# GDP per capita\ngdp_pc = fred.get_series('A939RC0Q052SBEA')\nprint(gdp_pc.tail())\n\n\n\nR (using fredr)\ninstall.packages(\"fredr\")\nlibrary(fredr)\n\nfredr_set_key(\"your_api_key_here\")\n\n# GDP per capita\ngdp_pc &lt;- fredr(series_id = \"A939RC0Q052SBEA\")\nhead(gdp_pc)"
  },
  {
    "objectID": "week07/index.html",
    "href": "week07/index.html",
    "title": "Creating dashboards and online simulation apps",
    "section": "",
    "text": "TBA",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 7: Dashboards & Apps"
    ]
  },
  {
    "objectID": "week07/index.html#part-i-dashboard-to-illustrate-economics",
    "href": "week07/index.html#part-i-dashboard-to-illustrate-economics",
    "title": "Creating dashboards and online simulation apps",
    "section": "Part I Dashboard to illustrate economics",
    "text": "Part I Dashboard to illustrate economics\nTBA",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 7: Dashboards & Apps"
    ]
  },
  {
    "objectID": "week07/index.html#part-ii-simulation-app-for-a-statistical-property",
    "href": "week07/index.html#part-ii-simulation-app-for-a-statistical-property",
    "title": "Creating dashboards and online simulation apps",
    "section": "Part II Simulation app for a statistical property",
    "text": "Part II Simulation app for a statistical property\nTBA",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 7: Dashboards & Apps"
    ]
  },
  {
    "objectID": "week05/index.html",
    "href": "week05/index.html",
    "title": "Week 05: using text as data",
    "section": "",
    "text": "Week 05: using text as data\n\n\nTurning a series of short texts into tabular data: humans vs AI",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#overview",
    "href": "week05/index.html#overview",
    "title": "Week 05: using text as data",
    "section": "Overview",
    "text": "Overview\nIn this lesson, students will be introduced to sentiment analysis, specifically applied to evaluating general positivity or negativity in football managers‚Äô statements about match outcomes.\n\nLearning Outcomes\nBy the end of the session, students will:\n\nUnderstand core NLP concepts: tokenization, preprocessing, bag-of-words, and feature extraction in practical context.\nGain hands-on experience with sentiment analysis.\nUnderstand the complexities and limitations of sentiment analysis.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#assignment-review",
    "href": "week05/index.html#assignment-review",
    "title": "Week 05: using text as data",
    "section": "Assignment review",
    "text": "Assignment review\n\nFancy graphs != good graphs (good graph &lt;- careful design)\nPrecise interpretation &gt;&gt; BS\nLess is more\nShow only what you understand deeply",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#lecture-nlp-basics",
    "href": "week05/index.html#lecture-nlp-basics",
    "title": "Week 05: using text as data",
    "section": "Lecture: NLP basics",
    "text": "Lecture: NLP basics\n\nTopic: Introduction to Sentiment Analysis\nKey points:\n\nImportance of text analysis and its applications\nIntroduction to Natural Language Processing (NLP): definition and applications\nKey concepts in text analysis:\n\nTokenization\nPreprocessing techniques\nFeature extraction\n\nSentiment analysis: detecting emotion and tone in text\nPractical examples from football managers‚Äô post-match interviews\nLimitations and challenges in text analysis, emphasizing contextual interpretation and ambiguity\n\n\nSlides\ndomain lexicon",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#practical-activity",
    "href": "week05/index.html#practical-activity",
    "title": "Week 05: using text as data",
    "section": "Practical Activity",
    "text": "Practical Activity\n\nManual vs AI Sentiment Analysis Activity\n\nObjective: Practice manually rating football manager statements as positive or negative.\nSteps:\n\nReview general sentiment rating scale provided HERE\nIndividually analyze and rate 5 provided test statements from student_test.csv.\nNow use AI to rate them.\nTry have a better domain lexicon.\n\n\nDiscuss experience, how AI helps, what could go wrong.\n\n\nPrediction of score\n\nModeling choices of results\nThink about how you would do it first\nCheck how AI thinks about, rate the examples and look at explanations\ntake the 5 examples, and compare your predictions vs the AI predictions\n\n\n\nDiscussion: Validation and Sentiment Analysis\n\nObjective: Discuss validation techniques used in sentiment analysis.\nTopics for discussion:\n\nDifferences between manual and AI ratings\nGround Truth\nIntroduction to validation methods:\nIf ground truth ‚Äì can do confusion maztric, calculate accuracy\nIf no ground truth ‚Äì measure agreement between humans and AI. test difference.\n\nAI is average, but‚Ä¶\nAI with persona?\nAI biased ?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#end-of-week-discussion-points",
    "href": "week05/index.html#end-of-week-discussion-points",
    "title": "Week 05: using text as data",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nHow precise is AI in sentiment analysis?\nHow did you compare to AI in terms of scores? How did any difference make you feel?\nCan you think of a past project where AI could have helped you upgrade it?\nDid AI provide consistent explanations for its ratings? When did its reasoning seem flawed? Did you feel it was mostly OK / half-truths?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week06/assets/api-use.html#whats-different-in-r",
    "href": "week06/assets/api-use.html#whats-different-in-r",
    "title": "Introduction to APIs: Why and when use them, and how to get started",
    "section": "What‚Äôs different in R",
    "text": "What‚Äôs different in R\nClient libraries in other languages: While our focus is on Python, other programming languages provide similar conveniences.\nIn R, packages like httr (for making HTTP requests) and jsonlite (for parsing JSON) are commonly used to work with web APIs. Many APIs also have R packages or wrappers that function like client libraries, letting you call the API in one or two lines of R code.\nThe core idea is the same: a client library abstracts the RESTful requests into native language functions. Regardless of language, using a client library means you can integrate an API into your data analysis or application with less hassle, letting you focus on interpreting results rather than the mechanics of HTTP."
  },
  {
    "objectID": "assignments/assignment_06.html",
    "href": "assignments/assignment_06.html",
    "title": "Gender guessing Analysis Pipeline",
    "section": "",
    "text": "Task\nCreate a similar pipeline we used in class but now with the aim to decide if the coach is a man or a women. In this example dataset, men teams have men coach and women teams have women coach.\nPick a few texts and do it yourself\nGet AI explain it‚Äôs decisions\n\n\nSubmit:\n\nA file with AI generated ratings: lastname_firstname_geneder_text.xlsx.\nA short para on why this could be easy or hard lastname_firstname_geneder_text.txt.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 6"
    ]
  },
  {
    "objectID": "assignments/assignment_08.html",
    "href": "assignments/assignment_08.html",
    "title": "Design an IV",
    "section": "",
    "text": "Task: Design an IV\nThink about an identification problem and interate to find a good IV. Explain why it may work and why could it fail.\nDiscuss what data would be needed and where it would come.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 8"
    ]
  },
  {
    "objectID": "assignments.html#assignment-5-text-analysis-guessing-gener",
    "href": "assignments.html#assignment-5-text-analysis-guessing-gener",
    "title": "Course Assignments",
    "section": "Assignment 5: Text analysis: guessing gener",
    "text": "Assignment 5: Text analysis: guessing gener\nCreate pipeline for guessing the speaker‚Äôs gender\nAssignment Details"
  },
  {
    "objectID": "index.html#week07-creating-simulations-with-apps-and-dashboards",
    "href": "index.html#week07-creating-simulations-with-apps-and-dashboards",
    "title": "Doing Data Analysis with AI",
    "section": "Week07: Creating simulations with apps and dashboards",
    "text": "Week07: Creating simulations with apps and dashboards\nTBA"
  },
  {
    "objectID": "index.html#week08-ai-as-research-companion",
    "href": "index.html#week08-ai-as-research-companion",
    "title": "Doing Data Analysis with AI",
    "section": "Week08: AI as research companion",
    "text": "Week08: AI as research companion\nTBA"
  },
  {
    "objectID": "weeks.html#week07-creating-simulations-with-apps-and-dashboards",
    "href": "weeks.html#week07-creating-simulations-with-apps-and-dashboards",
    "title": "Weekly Content",
    "section": "Week07: Creating simulations with apps and dashboards",
    "text": "Week07: Creating simulations with apps and dashboards\nTBA"
  },
  {
    "objectID": "weeks.html#week08-ai-as-research-companion",
    "href": "weeks.html#week08-ai-as-research-companion",
    "title": "Weekly Content",
    "section": "Week08: AI as research companion",
    "text": "Week08: AI as research companion\nTBA"
  },
  {
    "objectID": "assignments.html#assignment-6-text-analysis-guessing-gender-in-text",
    "href": "assignments.html#assignment-6-text-analysis-guessing-gender-in-text",
    "title": "Course Assignments",
    "section": "Assignment 6: Text analysis: guessing gender in text",
    "text": "Assignment 6: Text analysis: guessing gender in text\nCreate pipeline for guessing the speaker‚Äôs gender.\nAssignment Details"
  },
  {
    "objectID": "assignments.html#assignment-8-design-an-iv",
    "href": "assignments.html#assignment-8-design-an-iv",
    "title": "Course Assignments",
    "section": "Assignment 8: Design an IV",
    "text": "Assignment 8: Design an IV\nThink about an identification problem and interate to find a good IV.\nAssignment Details"
  },
  {
    "objectID": "week06/index.html#interview-analysis",
    "href": "week06/index.html#interview-analysis",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Interview Analysis",
    "text": "Interview Analysis\nCode for sentiment analysis of football manager interviews, here: interview scripts\n\nDomain lexicon creation in R\nscript in R requires API key setup in R\nScript in Pythin ‚Äì make sure you set your own API key",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#assignment-review",
    "href": "week06/index.html#assignment-review",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Assignment review",
    "text": "Assignment review\n\nReview of transformation options\n\nlexicon based counting numbers ‚Äì&gt; generate a transparent script\nMachine-learning classifiers, n-grams etc.\nLLM (transformer-based) one-shot: treating the LLM like a giant classifier: you hand it raw text and ask classes of sentiment. (Deep contextual understanding‚Äîword embeddings, attention across the whole sentence etc decide the sentiment.) No separate sentiment lexicon; it‚Äôs all encoded in the model weights.\nYou can also take a pretrained LLM and continue training it on thousands of labeled review. See our example guidelines HERE\n\n\n\n\nLet us discuss the concept ground truth (again): AI vs humans vs ‚Äútruth‚Äù",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week01/index.html#preparation-before-class",
    "href": "week01/index.html#preparation-before-class",
    "title": "Week 01: LLM Review",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\nüìö No Required Reading\n\n\nThis is Week 1 - come ready to explore and discuss!\nReflection Preparation:\n\nThink about your current experience with AI tools (if any)\nConsider examples where you‚Äôve encountered AI in your work/studies\nIdentify one data analysis task you find time-consuming or repetitive\n\nOptional Background:\n\nEthan Mollick: ‚ÄúCo-Intelligence: Living and Working with AI‚Äù (Chapters 1-2)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#class-material",
    "href": "week01/index.html#class-material",
    "title": "Week 01: LLM Review",
    "section": "Class Material",
    "text": "Class Material\n\n\nüìä Core LLM Concepts (60 min)\n\n\nSlideshow: LLM Concepts and Applications\nKey Topics Covered:\n\nWhat are LLMs? Statistical models predicting next tokens from massive training data\nThe Transformer Revolution: How 2017‚Äôs ‚ÄúAttention is All You Need‚Äù changed everything\nContext Windows: why this matters for data analysis\nTraining Process: Resources and human feedback\nCollaborative Frameworks: How to integrate human and AI work\n\n\n\n\n\nüîß Hands-on: FT Graph Challenge (25 min)\n\n\nThe Challenge:\nRecreate the Financial Times tariff/yield visualization as accurately as possible using AI assistance.\n\nNote the sophisticated design: multiple data series, annotations, professional styling\nConsider: What would you need to recreate this manually vs.¬†with AI assistance?\n\nLearning Approach:\n\nAnalyze the original - what data sources, what design elements?\nUse AI to find data and generate initial code\nIterate and refine based on results",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week01/index.html#background-tools-and-resources",
    "href": "week01/index.html#background-tools-and-resources",
    "title": "Week 01: LLM Review",
    "section": "Background, Tools and Resources",
    "text": "Background, Tools and Resources\nü§ñ Recommended for Data Analysis:\n\n\n\n\n\n\n\n\n\nPlatform\nStrengths\nBest For\nCost\n\n\n\n\nChatGPT 4o/o1\nExcellent coding, data analysis tools\nR/Python code, statistical methods\n$20/month\n\n\nClaude 4 Sonnet\nGreat reasoning, document analysis\nResearch, writing, complex prompts\n$20/month\n\n\nGitHub Copilot\nIntegrated coding assistance\nReal-time code completion\n$10/month\n\n\n\nFree Tier Capabilities:\n\nMost course tasks achievable with free versions\nPaid subscriptions recommended for intensive work ‚Äì maybe try during this course\nStart free, upgrade if you hit limits\n\nDetailed Comparison:\nAI Model Selection Guide\n\nAcademic Integrity and AI use\nCourse Philosophy:\n\nAI as Assistant: Use AI to enhance your capabilities, not replace your thinking\nMaintain Authority: You remain responsible for all outputs and interpretations\nVerify Everything: Always validate AI suggestions, especially statistical claims\nDocument Usage: Keep track of how AI helped ‚Äì useful for methodology sections\n\nRed Lines:\n\nNever submit unverified AI output as your own work\nAlways understand the analysis you‚Äôre presenting\nCite AI assistance appropriately (we‚Äôll discuss standards as they evolve)\n\nThe Goal:\nBecome a more capable data analyst who can leverage AI tools effectively while maintaining scientific rigor.\nNext Week:\nWeek 2 - Data Discovery and Documentation where we‚Äôll use AI to understand and document complex datasets.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week08/index.html",
    "href": "week08/index.html",
    "title": "AI as Research companion",
    "section": "",
    "text": "We will cover two topics: Creating difference-in-differences models, and thinking about finding instrumental variables. Both are key topics in empirical work in applied economics, political science, and beyond.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 8: AI Research Companion"
    ]
  },
  {
    "objectID": "week08/index.html#tasks",
    "href": "week08/index.html#tasks",
    "title": "AI as Research companion",
    "section": "",
    "text": "We will cover two topics: Creating difference-in-differences models, and thinking about finding instrumental variables. Both are key topics in empirical work in applied economics, political science, and beyond.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 8: AI Research Companion"
    ]
  },
  {
    "objectID": "week08/index.html#did-design",
    "href": "week08/index.html#did-design",
    "title": "AI as Research companion",
    "section": "DiD design",
    "text": "DiD design\nDifference-in-differences (DiD) models",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 8: AI Research Companion"
    ]
  },
  {
    "objectID": "week08/index.html#iv-design",
    "href": "week08/index.html#iv-design",
    "title": "AI as Research companion",
    "section": "IV design",
    "text": "IV design\nIV",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 8: AI Research Companion"
    ]
  },
  {
    "objectID": "index.html#detailed-overview",
    "href": "index.html#detailed-overview",
    "title": "Doing Data Analysis with AI",
    "section": "Detailed overview",
    "text": "Detailed overview\n\nWeek00: AI for coding\nUsing AI for code. May not be covered in this class, as it had often been already covered in coding classes.\nContent\n\n\nWeek01: LLM Review\nWhat are LLMs, how is the magic happening. A non-technical brief intro. How to work with LLMs? Plus ideas on applications. Includes suggested readings, podcasts, and vids to listen to.\nContent\nWhich AI?\nSee my take on current models. As of May 2025.\n\n\nWeek02: Data and code discovery and documentation with AI\nLearn how to write a clear and professional code and data documentation. LLMs are great help once you know the basics.\nCase study: World Values Survey. Data is at WVS\nContent\n\n\nWeek 03: Writing Reports\nYou have your data and task, and need to write a short report. We compare different options with LLM, from one-shot prompt to iteration.\nCase study: World Values Survey. Data is at WVS\nContent\n\n\nWeek04: Data wrangling, joining tables\nJoining data tables is maybe the most basic yet rather complicated part of data wrangling.\nCase study: simulated Austrian hotels. Data is at hotels\nContent\n\n\nWeek05: Text as data 1 ‚Äì intro lecture\nNo course of mine can escape football (soccer). Here we look at post-game interviews to learn basics of text analysis and apply LLMs in what they are best - context dependent learning. Two class series. First is more intro to natural language processing.\nCase study: football post-game interviews. Data is at interviews\nContent\n\n\nWeek06: Text as data 2 ‚Äì practice\nSecond class, now we are in action. How does LLM compare to humans?\nCase study: football post-game interviews. Data is at interviews\nContent\n\n\nWeek07: Creating simulations with apps and dashboards\nTBA\n\n\nWeek08: AI as research companion\nTBA"
  },
  {
    "objectID": "code/creating-code-sentiment-analysis.html",
    "href": "code/creating-code-sentiment-analysis.html",
    "title": "creating-code-sentiment-analysis",
    "section": "",
    "text": "Here are some prompts we created. Follow them. Need debugging.\nFirst download interview-texts-only.xlsx from data/interviews\n\nWrite a Python script for sentiment analysis of post-match manager quotes using OpenAI‚Äôs GPT API. Follow these detailed specifications:\n\n\nClassify each quote on a sentiment scale from -2 to 2 based on the manager‚Äôs tone, not the game result.\nThe script should process the xlsx file and produce a new CSV with sentiment scores.\n\n\n\nLoad interview-texts-only.xlsx .\nUse utf-8 encoding.\nSkip malformed lines when reading.\n\n\n\nUse OpenAI‚Äôs gpt-4.1-2025-04-14 model (or fallback-compatible GPT-4 model).\nConstruct a detailed prompt that:"
  },
  {
    "objectID": "code/creating-code-sentiment-analysis.html#writing-code-to-run-sentiment-analysis-via-apis.",
    "href": "code/creating-code-sentiment-analysis.html#writing-code-to-run-sentiment-analysis-via-apis.",
    "title": "creating-code-sentiment-analysis",
    "section": "",
    "text": "Here are some prompts we created. Follow them. Need debugging.\nFirst download interview-texts-only.xlsx from data/interviews\n\nWrite a Python script for sentiment analysis of post-match manager quotes using OpenAI‚Äôs GPT API. Follow these detailed specifications:\n\n\nClassify each quote on a sentiment scale from -2 to 2 based on the manager‚Äôs tone, not the game result.\nThe script should process the xlsx file and produce a new CSV with sentiment scores.\n\n\n\nLoad interview-texts-only.xlsx .\nUse utf-8 encoding.\nSkip malformed lines when reading.\n\n\n\nUse OpenAI‚Äôs gpt-4.1-2025-04-14 model (or fallback-compatible GPT-4 model).\nConstruct a detailed prompt that:"
  },
  {
    "objectID": "code/creating-code-sentiment-analysis.html#task",
    "href": "code/creating-code-sentiment-analysis.html#task",
    "title": "creating-code-sentiment-analysis",
    "section": "Task:",
    "text": "Task:\nPlease read each text carefully and rate the overall sentiment of the manager‚Äôs statement as positive or negative. Your rating should reflect the manager‚Äôs expressed tone, not your judgment of the match."
  },
  {
    "objectID": "code/creating-code-sentiment-analysis.html#rating-scale",
    "href": "code/creating-code-sentiment-analysis.html#rating-scale",
    "title": "creating-code-sentiment-analysis",
    "section": "Rating Scale:",
    "text": "Rating Scale:\n\n\n\n\n\n\n\nScore\nMeaning\n\n\n\n\n2\nStrongly positive sentiment (clear optimism, satisfaction, praise).\n\n\n1\nMildly positive sentiment (generally positive, slight reservations).\n\n\n0\nNeutral or unclear sentiment.\n\n\n-1\nMildly negative sentiment (general disappointment, frustration).\n\n\n-2\nStrongly negative sentiment (clear criticism, significant disappointment).\n\n\n\n\nFinal Notes:\n\nUse 0 if unsure or if sentiment is mixed without clear dominance.\n\n\n\nRetry up to 3 times if API call fails, using exponential backoff (e.g., wait 1s, 2s, 4s).\n\nIf the response is not an integer, return None.\n\n\nüîê API Authentication\n\n\nUse the openai Python package (v1+).\n\nLoad API key securely from .env file with key OPENAI_API_KEY.\n\nIf the key is missing, exit with an error.\n\n\nüß™ Processing & Output\n\n\nIterate through all rows using tqdm for progress.\n\nFor each row, call the GPT model and collect the result.\n\nSave results (text_id, score) to manager_sentiment_results.csv.\n\nAt end, print and log score frequency table.\n\n\nü™µ Logging\n\n\nUse Python‚Äôs logging module.\n\nConfigure it to:\n\n\nLog to both console (stdout) and to file classification.log (overwrite mode).\n\nUse format: [timestamp] [level] [message]\n\nSet level to INFO.\n\n\n\nLog key events:\n\n\nStart of API call\n\nAPI responses\n\nNumber of rows processed\n\nMissing columns\n\nFinal distribution of scores\n\n\n\n\nüì¶ Dependencies\n\n\npandas\n\ntqdm\n\nopenai\n\ndotenv\n\n\nüîÅ Extras\n\n\nUse functions for:\n\n\nLoading CSV,\n\nClassifying text,\n\nError handling.\n\n\n\nEnsure script can be run directly with #!/usr/bin/env python3 at the top.\n\n\n\nLet me know if you want me to generate the exact prompt message string or turn this into a README section."
  },
  {
    "objectID": "week01/0index.html",
    "href": "week01/0index.html",
    "title": "Week 1: LLM Review",
    "section": "",
    "text": "Week 1: LLM Review\n\n\nIntroduction to Large Language Models and their applications in data analysis"
  },
  {
    "objectID": "week01/0index.html#learning-objectives",
    "href": "week01/0index.html#learning-objectives",
    "title": "Week 1: LLM Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, students will:\n\nUnderstand core concepts and architecture behind large language models (LLMs)\nLearn how to incorporate AI into data analysis workflows\n\nCritically assess capabilities and limitations of AI tools in academic contexts\nExperience the ‚Äújagged frontier‚Äù of LLM capabilities through hands-on practice"
  },
  {
    "objectID": "week01/0index.html#class-materials",
    "href": "week01/0index.html#class-materials",
    "title": "Week 1: LLM Review",
    "section": "Class Materials",
    "text": "Class Materials\n\n\nüìä Slideshow\n\n\nLLM Concepts Presentation\nKey Topics Covered:\n\nWhat are Large Language Models?\nThe Transformer architecture and tokenization\nCyborg vs Centaur approaches to AI collaboration\nThe ‚Äújagged frontier‚Äù of AI capabilities\nPrompt engineering fundamentals\nAI as bs generator and Brandolini‚Äôs Law\n\n\n\n\n\nüìö Required Reading\n\n\n\nWhich AI Model to Choose?\nUpdated guide for 2025\nReview the FT graph for class activity (see below)\n\nOptional Background:\n\nEthan Mollick: ‚ÄúCo-Intelligence: Living and Working with AI‚Äù (Chapters 1-2)\n\n\n\n\n\nüéØ Class Activity\n\n\nThe Financial Times Challenge\nTake a look at this excellent Financial Times visualization showing the market reaction to Trump‚Äôs tariff announcements.\nYour Mission:\nReproduce this chart as accurately as possible in the shortest time using AI assistance.\nLearning Goals:\n\nExperience AI-assisted data visualization\nPractice prompt engineering for specific tasks\nUnderstand the balance between human direction and AI execution, risks of relying too much on AI"
  },
  {
    "objectID": "week01/0index.html#assignment",
    "href": "week01/0index.html#assignment",
    "title": "Week 1: LLM Review",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nAssignment 1: Reproduce the FT Graph\n\n\n\nDue: Before Week 2\nChoose your approach:\n\nOption A (Standard): Use AI to find data and recreate the visualization\nOption B (Advanced): Build an interactive dashboard that updates dynamically\n\nFull Assignment Details"
  },
  {
    "objectID": "week01/0index.html#preparation",
    "href": "week01/0index.html#preparation",
    "title": "Week 1: LLM Review",
    "section": "Preparation",
    "text": "Preparation\n\n\n\n\n\n\nBefore Class\n\n\n\n\nNo specific preparation required for Week 1\nCome ready to discuss your current experience with AI tools\nBring examples of where you‚Äôve encountered AI in your work/studies"
  },
  {
    "objectID": "week01/0index.html#discussion-questions",
    "href": "week01/0index.html#discussion-questions",
    "title": "Week 1: LLM Review",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nConsider these questions as you engage with the materials:\n\nPersonal AI Experience: How have you already incorporated AI into your routine? Which model feels most natural to you?\nError Management: How do you currently deal with AI hallucinations or imperfect answers?\nThe Jagged Frontier: What tasks do you expect AI to excel at? Where do you think it will struggle?"
  },
  {
    "objectID": "week01/0index.html#tools-and-resources",
    "href": "week01/0index.html#tools-and-resources",
    "title": "Week 1: LLM Review",
    "section": "Tools and Resources",
    "text": "Tools and Resources\nRecommended AI Platforms for this course:\n\nChatGPT 4o/o1 - Excellent for coding and data analysis\nClaude 3.5 Sonnet - Great for research and writing tasks\n\nGitHub Copilot - For integrated coding assistance\n\nGetting Started:\n\nMost tasks can be accomplished with free tiers\nConsider paid subscriptions for intensive work ($20/month typical)\nSee AI Model Comparison Guide for detailed recommendations"
  },
  {
    "objectID": "week01/0index.html#week-1-outcomes",
    "href": "week01/0index.html#week-1-outcomes",
    "title": "Week 1: LLM Review",
    "section": "Week 1 Outcomes",
    "text": "Week 1 Outcomes\nBy completing Week 1, you should:\n\n‚úÖ Understand what LLMs can and cannot do reliably\n‚úÖ Have experience with AI-assisted data visualization\n\n‚úÖ Recognize the importance of human oversight in AI workflows\n‚úÖ Be prepared to use AI as a collaborative tool throughout the course\n\n\n\n\n\n\n\n\nAcademic Integrity Note\n\n\n\nThis course teaches you to use AI as a powerful assistant while maintaining your responsibility as the analyst and author. Always verify AI outputs, cite your methods, and ensure you understand the analysis you‚Äôre presenting.\n\n\nNext Week: Week 2 - Data Discovery and Documentation where we‚Äôll use AI to understand and document complex datasets."
  },
  {
    "objectID": "week01/index.html#the-final-bit-to-think-about",
    "href": "week01/index.html#the-final-bit-to-think-about",
    "title": "Week 01: LLM Review",
    "section": "The final bit to think about",
    "text": "The final bit to think about\nIt was an AI (Claude Sonnet 4.0) that suggest to include the last bit on Academic Integrity and it wrote many lines such as having red lines.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week02/index.html#class-material",
    "href": "week02/index.html#class-material",
    "title": "Week02: Discovery and documentation",
    "section": "Class Material",
    "text": "Class Material\n\n\nüìà Assignment Review (10 min)\n\n\n\nFollow instructions.\nHow to get close to original, different ways\nWhy do an app? What to expect from an app\n\nstreamlit\nshinyapps\n\nHow was AI assistance helpful?\n\n\n\n\n\nüìñ Documentation Fundamentals (20 min)\n\n\n\nAbout Markdown\n\nEditor in R, Python Quarto\nOnline Markdown editor\nAlso: Pandoc\n\n\n\nWhat is a good readme?\nSome examples for reproduction package\n\nB√©k√©s-K√©zdi (2021) Hotels dataset ‚Äì show basics\nKoren-Pet≈ë (2021) Business disruptions from social distancing as PDF\nSome ideas on readme: Makereadme, Social Science Editors\n\nKey ingredients\n\nOverview of project\nlicense\nAll datasets (data tables) separately discussed\nAll key variables described (name, content, type, coverage (% share missing)\n\nmaybe also: source, extension (csv / xlsx/ parquet)\n\nData lineage ‚Äúprovenance‚Äù : source ‚Üí processing ‚Üí final structure\n\n\n\nWhat is a variable dictionary (also called codebook)\n\nmore details of a dataset, often as xlsx\nmetric (euro, %), meaning of values if categorical\nmaybe even mean, min, max\n\nExamples\n\nB√©k√©s-K√©zdi (2021) Bisnode dataset variables\nReif (2022) illinois-wellness-data\n\nOh, but there is one we created we created in Week00\n\nOn earnings data, by Claude\n\n\n\n\n\n\nü§ù Hands-on Documentation Workshop (50 min)\n\n\n\nNo AI\n\nDownload and look at the Random Subset data\nStart collecting some info on the data without AI\nStart thinking about an interesting research question (find \\(y\\) and \\(x\\))\nIdentify 3 variables that seem important but are unclear from names alone.\n\n\n\nAI: let AI teach you also about\n\nStart asking for skeleton readme, ask about advice\nTest AI‚Äôs understanding: ‚ÄúExplain the difference between Q6 and Q7 in simple terms‚Äù - this reveals whether AI actually understands the codebook.\nDiscussion\n\n\n\nAI: Learning and idea generation\n\nTell AI about your plan and need for a readme\n\nexperiment with one-shot vs interaction\n\nDiscussion\n\n\n\nCyborg mode: create a readme with AI\n\nUpload the codebook + random subset data\nGet AI to design a README TEMPLATE for this task.\nGet a draft\nFocus on the ‚ÄúVariables‚Äù section - this is where AI excels at summarizing complex definitions while you provide oversight for accuracy.\nUnderstand and edit draft\n\n\n\nIII additional idea\n\nSometimes, complicated projects have extensive folder structure. Use A to design a folder structure",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#discussion-questions",
    "href": "week02/index.html#discussion-questions",
    "title": "Week02: Discovery and documentation",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nEnd of Week Reflection:\n\nWhat was the biggest contribution of AI?\nFirst result vs after iterations ‚Äì what did improve?\nHow do you feel about learning from AI vs human instructor? Pros and cons?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#assignment",
    "href": "week02/index.html#assignment",
    "title": "Week02: Discovery and documentation",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nAssignment 2: Creating Documentation\n\n\n\nDue: Before Week 3\nChoose a research question using the WVS data and create professional documentation focusing on relevant variables.\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#background-tools-and-resources",
    "href": "week02/index.html#background-tools-and-resources",
    "title": "Week02: Discovery and documentation",
    "section": "Background, Tools and Resources",
    "text": "Background, Tools and Resources\nWVS Data Specifics:\n\nCheck how AI understands nuances of encoding\nReview survey timing and discuss consequences\n\nAI-Assisted Documentation Workflow, use AI to:\n\nconvert dense codebook language into accessible descriptions.\nsuggest folder structures for complex projects.\ncheck consistency across variable descriptions\n\nAlways verify technical details, because AI makes some mistakes.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week02/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week02/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week02: Discovery and documentation",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nWe (Zsuzsi and me) first developed this material in August 2024. At that time, there were many hiccups in variable understanding and selection. I was gonna suggest careful human oversight. By the time of first teaching it in February 2025, AI got extremely good at reading a 400 page codebook.\nAI suggested the point ‚ÄúTest AI‚Äôs understanding‚Äù",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 2: Data Documentation"
    ]
  },
  {
    "objectID": "week01/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week01/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 01: LLM Review",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nI needed to rewrite, edit the slideshow every other month. Bloody hell, this course material is tricky. (While Gosset‚Äôs t-test has been around since 1908‚Ä¶)\nIt was an AI (Claude Sonnet 4.0) that suggest to include the last bit on Academic Integrity and it wrote many lines such as having red lines. Hahh.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 1: LLM Review"
    ]
  },
  {
    "objectID": "week02/index - Copy.html",
    "href": "week02/index - Copy.html",
    "title": "Week02: Discovery and documentation",
    "section": "",
    "text": "Week02: Discovery and documentation\n\n\nData discovery and data and code documentation with AI"
  },
  {
    "objectID": "week02/index - Copy.html#summary",
    "href": "week02/index - Copy.html#summary",
    "title": "Week02: Discovery and documentation",
    "section": "Summary",
    "text": "Summary\nSometimes data is large and discovery is hard. Sometimes you need to write data documentation. LLMs can help. You will learn how to write a clear and professional README. We use a cleaned subset of the 7th Wave of the World Values Survey (WVS). We‚Äôll also talk some tech on documentation."
  },
  {
    "objectID": "week02/index - Copy.html#learning-objectives",
    "href": "week02/index - Copy.html#learning-objectives",
    "title": "Week02: Discovery and documentation",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nUnderstand how to document a new dataset using as an example th WVS 7th wave data.\nCreate a README that describes data.\nLearn to refine documentation by incorporating iterative feedback from peers and AI tools."
  },
  {
    "objectID": "week02/index - Copy.html#preparation-before-class",
    "href": "week02/index - Copy.html#preparation-before-class",
    "title": "Week02: Discovery and documentation",
    "section": "Preparation BEFORE class",
    "text": "Preparation BEFORE class\n\nReading and review\n\nBackground reading: B√©k√©s-K√©zdi (2021) Chapters 1-3, in particular core background info\nSome discussion of data types Data Management in Large-Scale Education Research by Crystal Lewis\n\n\n\nGet data and info:\nAccess the VWS dataset 1. Data: WVS_random_subset.csv - random subset (N=2000) - covering all countries 2. Download its official codebook documentation\nIf you prefer datasets are also at OSF, Gabors Data Analysis / World Values Survey"
  },
  {
    "objectID": "week02/index - Copy.html#review-assignment-01",
    "href": "week02/index - Copy.html#review-assignment-01",
    "title": "Week02: Discovery and documentation",
    "section": "Review Assignment 01",
    "text": "Review Assignment 01\n\nFollow instructions.\nHow to get close to original, different ways\nWhy do an app? What to expect from an app\n\nstreamlit\nshinyapps"
  },
  {
    "objectID": "week02/index - Copy.html#i.-background",
    "href": "week02/index - Copy.html#i.-background",
    "title": "Week02: Discovery and documentation",
    "section": "I. Background",
    "text": "I. Background\n\nAbout Markdown\n\nEditor in R, Python Quarto\nOnline Markdown editor\nAlso: Pandoc\n\n\n\nWhat is a good readme?\nSome examples for reproduction package\n\nB√©k√©s-K√©zdi (2021) Hotels dataset ‚Äì show basics\nKoren-Pet≈ë (2021) Business disruptions from social distancing as PDF\nSome ideas on readme: Makereadme, Social Science Editors\n\nKey ingredients\n\nOverview of project\nlicense\nAll datasets (data tables) separately discussed\nAll key variables described (name, content, type, coverage (% share missing)\n\nmaybe also: source, extension (csv / xlsx/ parquet)\n\n\n\n\nWhat is a variable dictionary (also called codebook)\n\nmore details of a dataset, often as xlsx\nmetric (euro, %), meaning of values if categorical\nmaybe even mean, min, max\n\nExamples\n\nB√©k√©s-K√©zdi (2021) Bisnode dataset variables\nReif (2022) illinois-wellness-data"
  },
  {
    "objectID": "week02/index - Copy.html#ii.-work-on-data",
    "href": "week02/index - Copy.html#ii.-work-on-data",
    "title": "Week02: Discovery and documentation",
    "section": "II. Work on data",
    "text": "II. Work on data\n\nNo AI\n\nDownload and look at the Random Subset data\nStart collecting some info on the data without AI\nStart thinking about an interesting research question (find \\(y\\) and \\(x\\))\n\n\n\nAI: let AI teach you also about\n\nStart asking for skeleton readme, ask about advice\nDiscussion\n\n\n\nAI: Learning and idea generation\n\nTell AI about your plan and need for a readme\n\nexperiment with one-shot vs interaction\n\nDiscussion\n\n\n\nCyborg mode: create a readme with AI\n\nUpload the codebook + random subset data\nGet AI to design a README TEMPLATE for this task.\nGet a draft\nUnderstand and edit draft\n\n\n\nIII additional idea\n\nSometimes, complicated projects have extensive folder structure. Use A to design a folder structure"
  },
  {
    "objectID": "week02/index - Copy.html#end-of-week-discussion-points",
    "href": "week02/index - Copy.html#end-of-week-discussion-points",
    "title": "Week02: Discovery and documentation",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nWhat was the biggest contribution of AI?\nFirst result vs after iterations ‚Äì what did improve?\nHow do you feel about learning from AI vs human instructor? Pros and cons?"
  },
  {
    "objectID": "week06/index - Copy.html",
    "href": "week06/index - Copy.html",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "",
    "text": "Week 06: Sentiment Analysis with AI\n\n\nUsing API to AI to create data from text"
  },
  {
    "objectID": "week06/index - Copy.html#overview",
    "href": "week06/index - Copy.html#overview",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Overview",
    "text": "Overview\nContinue using text for research with AI\n\nLearning Outcomes\nBy the end of the session, students will:\n\nGain hands-on experience with sentiment analysis.\nHave experience integrating NLP in research\nThink about what is ground truth"
  },
  {
    "objectID": "week06/index - Copy.html#assignment-review",
    "href": "week06/index - Copy.html#assignment-review",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Assignment review",
    "text": "Assignment review\n\nReview of transformation options\n\nlexicon based counting numbers ‚Äì&gt; generate a transparent script\nMachine-learning classifiers, n-grams etc.\nLLM (transformer-based) one-shot: treating the LLM like a giant classifier: you hand it raw text and ask classes of sentiment. (Deep contextual understanding‚Äîword embeddings, attention across the whole sentence etc decide the sentiment.) No separate sentiment lexicon; it‚Äôs all encoded in the model weights.\nYou can also take a pretrained LLM and continue training it on thousands of labeled review. See our example guidelines HERE\n\n\n\n\nLet us discuss the concept ground truth (again): AI vs humans vs ‚Äútruth‚Äù"
  },
  {
    "objectID": "week06/index - Copy.html#using-apis",
    "href": "week06/index - Copy.html#using-apis",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Using APIs",
    "text": "Using APIs\n\nBasics and setup\nHow to use APIs\nGet an API Key (ChatGPT and Claude)\n\n\nExamples\nSimple walkthrough with GDP data ‚Äì uses World Bank and FRED APIs\nBit harder walkthrough with football data ‚Äì uses FBREF soccer data. Guess the club for example.\n\n\nMore advanced stuff\nMore advanced knowledge on APIs ‚Äì how APIs work\n\n\nMaterials\nDatasets\n\ntexts (text_id level)\ngames info (such as results, text_id level)\nclass-ratings (human, AI ratio, text_id*student level)\ndomain-rating (text_id level)\nclass-rating-aggregated (text_id level)\n\ncode"
  },
  {
    "objectID": "week06/index - Copy.html#interview-analysis",
    "href": "week06/index - Copy.html#interview-analysis",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Interview Analysis",
    "text": "Interview Analysis\nCode for sentiment analysis of football manager interviews, here: interview scripts\n\nDomain lexicon creation in R\nscript in R requires API key setup in R\nScript in Pythin ‚Äì make sure you set your own API key"
  },
  {
    "objectID": "week06/index - Copy.html#preparation",
    "href": "week06/index - Copy.html#preparation",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Preparation",
    "text": "Preparation\n\nDownload the combined data from Moodle\n\nNote: win, draw ‚Äì need encode loss"
  },
  {
    "objectID": "week06/index - Copy.html#class-tasks",
    "href": "week06/index - Copy.html#class-tasks",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Class tasks",
    "text": "Class tasks\n\nDiscussion 1\n\nYour experience regarding human vs ai ratings.\nWhat was difficult and easy as human rater\n\n\n\nData Analysis\n\nTake the aggregated file and ask AI for a readme. Discuss what is in the data\nCompare human, domain lexicon and AI rating. For human and AI take the average.\nThink of an interesting comparison using AI rating\nCompare results by human and lexicon rating\n\n\n\nDiscussion 2\n\nWhat is ground truth\n\n\n\nHow to integrate AI into research\n\ncombine data with text\nthink RQ and how you‚Äôd use AI"
  },
  {
    "objectID": "week06/index - Copy.html#additional-tasks-if-time-permits",
    "href": "week06/index - Copy.html#additional-tasks-if-time-permits",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Additional tasks if time permits",
    "text": "Additional tasks if time permits\n\npredict gender and result\n\nShow AI all texts and ask to predict the gender of speaker\nShow AI all texts and ask to predict the result (manager‚Äôs team won, drew, lost)"
  },
  {
    "objectID": "week04/index.html#assignment",
    "href": "week04/index.html#assignment",
    "title": "Week 04: Joining tables",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNote\n\n\n\nAssignment [/assignments/assignment_04.md]",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week04/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week04/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 04: Joining tables",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nYou may wonder why this topic? I‚Äôll tell you. David Card, Nobel-prize winning economist visited me at CEU. I asked what I shall I add to my textbook. Some new fancy econometric method? He said in his experience, people have the hardest time joining tables, and in business applications, it‚Äôs crucial.\nThis is also a place to show how AI can help expanding on key concepts. One must know about the problem of joining tables. But what is left vs right, what is primary vs secondary key ‚Äì well ask AI.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 4: Joining Tables"
    ]
  },
  {
    "objectID": "week03/index.html#assignment",
    "href": "week03/index.html#assignment",
    "title": "Week 03: Reporting your data analysis",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nNote\n\n\n\nAssignment 3: Creating a Report\nCritical Note: Reports will be shared for peer review in Week 4 - upload to Moodle student folder.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Writing Reports"
    ]
  },
  {
    "objectID": "week03/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week03/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 03: Reporting your data analysis",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nThe AI (Claude 4.0) suggested to add ‚ÄúPay special attention to Chapter 4‚Äôs discussion of good vs.¬†bad graphs and Chapter 10 on interpreting regression coefficients precisely.‚Äù Well. We indeed have bits on good graphs in Chapter4 but not really on bad ones. It is Chapter 07 that builds coefficient estimation, albeit we have some stuff in Chapter 10 for multiple coefficients. I asked AI about errors. It noted that it predicted them based on knowing about the core summary of our book + contents of‚Äùtypical econometrics textbook‚Äù. Instructive re half-truth danger.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 3: Writing Reports"
    ]
  },
  {
    "objectID": "week03/index - Copy.html",
    "href": "week03/index - Copy.html",
    "title": "Week 03: Reporting your data analysis",
    "section": "",
    "text": "Week 03: Reporting your data analysis\n\n\nCreating inputs and organizing a short data analytics report with AI"
  },
  {
    "objectID": "week03/index - Copy.html#objectives",
    "href": "week03/index - Copy.html#objectives",
    "title": "Week 03: Reporting your data analysis",
    "section": "Objectives",
    "text": "Objectives\n\nSummary:\nHow to organize a short data analytics report? The job includes choosing and creating relevant plots, running regression. The class will exlopre how we can use AI to assist in these tasks.\n\n\nDetails\n\nUnderstand how to connect an empirical question to data\nCreate relevant visualizations and tables using AI.\nLearn to critically assess reports with the help of AI tools."
  },
  {
    "objectID": "week03/index - Copy.html#preparation-before-class",
    "href": "week03/index - Copy.html#preparation-before-class",
    "title": "Week 03: Reporting your data analysis",
    "section": "Preparation BEFORE class",
    "text": "Preparation BEFORE class\n\nBackground reading: B√©k√©s-K√©zdi (2021) Chapters 3-4, 7-10\nDownload the WVS_GDP_merged_data.csv. This is an aggregated, cleaned subset of the 7th Wave of WVS dataset merged with GDP data from World Bank\n\n\nThe data\n\nThis is aggregated data: country level\nYear: Wave 7 of the WVS ‚Äì survey was conducted at different years.\nCombined with World Bank data: at year when survey was conducted\nGDP: level USD, level USD PPP, level USD PPP per capita\npopulation"
  },
  {
    "objectID": "week03/index - Copy.html#class-plan",
    "href": "week03/index - Copy.html#class-plan",
    "title": "Week 03: Reporting your data analysis",
    "section": "Class plan",
    "text": "Class plan\n\nReview Readme assignment\n\nQ+A\nVerify, verify, verify (AI is good but rarely perfect)\n\n\n\nStarting presentation about prompting\nslideshow\n\n\nHow does a good report look like?\n\nHow to write a good short report: structure\ngood graphs and tables\n\nMake sure precise language. Recap on causal language.\n\n\n\n\nProblems with AI generated reports.\nUse AI as input (like advanced google search) not as output writer\nBecause * Creates ‚Äúaverage‚Äù / generic / bland / repetitive text * Convincing but not precise * Not your style and not your exact plan * Too broad (like adds further research)\n\n\nNO AI\nForm 2-3 member groups freely\n\nEach group: Choose one these pre-defined research questions:\n\n\nIs there a relationship between income level and trust?\nIs there a relationship between income level and happiness?\nIs there a relationship between income level and gender attitudes?\n\n\nChoose the relevant variables to answer your question (you can use AI to understand variables like in week2\nDesign a plan for a report on the topic: list of exhibits (graphs, tables). Do not write code (yet)\nDiscuss plans\n\n\n\nAI 1\nTry get a report with a single prompt. * Hint: translate your plan into a prompt using ideas from the intro.\n\n\nAI 2\n\nShowcase an iterative process where key exhibits are created"
  },
  {
    "objectID": "week03/index - Copy.html#end-of-week-discussion-points",
    "href": "week03/index - Copy.html#end-of-week-discussion-points",
    "title": "Week 03: Reporting your data analysis",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nCompare single and multi-step approach generating reports?\nHow good is AI in creating good enough vs exactly as planned graphs?\nWhat is happiness? :-)"
  },
  {
    "objectID": "week04/index - Copy.html",
    "href": "week04/index - Copy.html",
    "title": "Week 04: Joining tables",
    "section": "",
    "text": "Week 04: Joining tables\n\n\nCombing data tables, and understanding what can go wrong with AI"
  },
  {
    "objectID": "week04/index - Copy.html#about-the-class",
    "href": "week04/index - Copy.html#about-the-class",
    "title": "Week 04: Joining tables",
    "section": "About the class",
    "text": "About the class\nData wrangling is when you prepare the data for the analysis. A key aspect is joining data tables. AI can help design the process, and give you code to do that.\n\nObjectives summary:\nLearn how to organize data in a tidy way, join multiple datasets, choose variables to answer a research question and create a reproducible workflow to analyze data.\n\n\nLearning Objectives\n\nLearn how to store information in a tidy way.\nWork with relational data.\nJoin tables\nUse AI to explain complex concepts\n\n\n\nBefore class\n\nBackground reading: B√©k√©s-K√©zdi (2021) Chapter 2\nDownload data-modified.zip from Here. Unzip. It is a set of csv files such as ‚Äòcities_modified‚Äô\n\nAlso available on Moodle\n\nThe data description is available here"
  },
  {
    "objectID": "week04/index - Copy.html#class-plan",
    "href": "week04/index - Copy.html#class-plan",
    "title": "Week 04: Joining tables",
    "section": "Class Plan",
    "text": "Class Plan\n\nRecap\nDiscuss assignment 03 (20 mins by groups + 10 mins together) * Create 4-member groups. Each groups will read reports by an another team (1‚Äì&gt;2, 2‚Äì&gt;3, N‚Äì&gt;1) * Read the other team‚Äôs submissions with a ‚Äòreader‚Äôs perspective‚Äô and take notes. * which report did you like the most and why * rank reports in terms of how much AI was involved from low to high and note suspicious examples\n\n\nTask 1: Use AI to understand these terms. Ask examples. (Individual)\n\ntidy data table\nrelational datasets,\n\nschema,\nprimary and foreign key\ncomposite key\n\njoining tables\n\ndifferent types of join\n1:1, 1:m\n\njoining tables in your language (python, R, Stata)\n\nThis is followed by a discussion."
  },
  {
    "objectID": "week04/index - Copy.html#task-2-form-2-3-groups-of-people-using-same-coding-language",
    "href": "week04/index - Copy.html#task-2-form-2-3-groups-of-people-using-same-coding-language",
    "title": "Week 04: Joining tables",
    "section": "Task 2: Form 2-3 groups of people using same coding language",
    "text": "Task 2: Form 2-3 groups of people using same coding language\nUse the data you downloaded to carry out joins and inspect results. Use AI but inspect."
  },
  {
    "objectID": "week04/index - Copy.html#section",
    "href": "week04/index - Copy.html#section",
    "title": "Week 04: Joining tables",
    "section": "1:1",
    "text": "1:1\n\nJoin hotels and cities. Compare left, right, inner, outer joins.\n\n\nwhat happens to N?"
  },
  {
    "objectID": "week04/index - Copy.html#m",
    "href": "week04/index - Copy.html#m",
    "title": "Week 04: Joining tables",
    "section": "1:m",
    "text": "1:m\n\nStart: Tabulate the frequency of hotels by city_hotel_counts\nCities to Hotels\n\n\none city joins to multiple hotels\nfilter on 2 cities for easier visibility\n\n\nJoin hotel and occupancy 1\n\n\nm:1\n\n\nJoin hotel and occupancy 2\n\n\nget hotel level\ntrick: aggregate\n\n\nJoin on composite key\n\n\ncreate a data table at city-year-match level showing average occupancy and tourist arrivals"
  },
  {
    "objectID": "week04/index - Copy.html#advice-ideas",
    "href": "week04/index - Copy.html#advice-ideas",
    "title": "Week 04: Joining tables",
    "section": "Advice, ideas",
    "text": "Advice, ideas\n\ndiscuss and collect ideas from AI\nlearn to focus on key suggestions (AI can go nuanced and not important points easily)"
  },
  {
    "objectID": "week04/index - Copy.html#home-assignment",
    "href": "week04/index - Copy.html#home-assignment",
    "title": "Week 04: Joining tables",
    "section": "Home Assignment",
    "text": "Home Assignment\nSuggested assignment [/assignments/assignment_04]"
  },
  {
    "objectID": "week04/index - Copy.html#end-of-week-discussion-points",
    "href": "week04/index - Copy.html#end-of-week-discussion-points",
    "title": "Week 04: Joining tables",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nHow useful was AI in teaching skills?\nHow useful was AI in actually joining tables?\nHow can you debug what AI did in terms of executing code?"
  },
  {
    "objectID": "week05/index - Copy.html",
    "href": "week05/index - Copy.html",
    "title": "Week 05: using text as data",
    "section": "",
    "text": "Week 05: using text as data\n\n\nTurning a series of short texts into tabular data: humans vs AI"
  },
  {
    "objectID": "week05/index - Copy.html#overview",
    "href": "week05/index - Copy.html#overview",
    "title": "Week 05: using text as data",
    "section": "Overview",
    "text": "Overview\nIn this lesson, students will be introduced to sentiment analysis, specifically applied to evaluating general positivity or negativity in football managers‚Äô statements about match outcomes.\n\nLearning Outcomes\nBy the end of the session, students will: - Gain hands-on experience with sentiment analysis. - Understand the complexities and limitations of sentiment analysis.\n\n\nMaterials\n\nGeneral Sentiment (positive/negative) rating scale HERE\nCSV files:\n\nstudent_test_5 download from HERE as xlsx HERE as csv, (or from moodle)"
  },
  {
    "objectID": "week05/index - Copy.html#assignment-review",
    "href": "week05/index - Copy.html#assignment-review",
    "title": "Week 05: using text as data",
    "section": "Assignment review",
    "text": "Assignment review\n\nFancy graphs != good graphs (good graph &lt;- careful design)\nPrecise interpretation &gt;&gt; BS\nLess is more\nShow only what you understand deeply"
  },
  {
    "objectID": "week05/index - Copy.html#lecture-nlp-basics",
    "href": "week05/index - Copy.html#lecture-nlp-basics",
    "title": "Week 05: using text as data",
    "section": "Lecture: NLP basics",
    "text": "Lecture: NLP basics\n\nTopic: Introduction to Sentiment Analysis\nKey points:\n\nImportance of text analysis and its applications\nIntroduction to Natural Language Processing (NLP): definition and applications\nKey concepts in text analysis:\n\nTokenization\nPreprocessing techniques\nFeature extraction\n\nSentiment analysis: detecting emotion and tone in text\nPractical examples from football managers‚Äô post-match interviews\nLimitations and challenges in text analysis, emphasizing contextual interpretation and ambiguity\n\n\nSlides\ndomain lexicon"
  },
  {
    "objectID": "week05/index - Copy.html#practical-activity",
    "href": "week05/index - Copy.html#practical-activity",
    "title": "Week 05: using text as data",
    "section": "Practical Activity",
    "text": "Practical Activity\n\nManual vs AI Sentiment Analysis Activity\n\nObjective: Practice manually rating football manager statements as positive or negative.\nSteps:\n\nReview general sentiment rating scale provided HERE\nIndividually analyze and rate 5 provided test statements from student_test.csv.\nNow use AI to rate them.\nTry have a better domain lexicon.\n\n\nDiscuss experience, how AI helps, what could go wrong.\n\n\nPrediction of score\n\nModeling choices of results\nThink about how you would do it first\nCheck how AI thinks about, rate the examples and look at explanations\ntake the 5 examples, and compare your predictions vs the AI predictions\n\n\n\nDiscussion: Validation and Sentiment Analysis\n\nObjective: Discuss validation techniques used in sentiment analysis.\nTopics for discussion:\n\nDifferences between manual and AI ratings\nGround Truth\nIntroduction to validation methods:\nIf ground truth ‚Äì can do confusion maztric, calculate accuracy\nIf no ground truth ‚Äì measure agreement between humans and AI. test difference.\n\nAI is average, but‚Ä¶\nAI with persona?\nAI biased ?"
  },
  {
    "objectID": "week05/index - Copy.html#end-of-week-discussion-points",
    "href": "week05/index - Copy.html#end-of-week-discussion-points",
    "title": "Week 05: using text as data",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nHow precise is AI in sentiment analysis?\nHow did you compare to AI in terms of scores? How did any difference make you feel?\nCan you think of a past project where AI could have helped you upgrade it?"
  },
  {
    "objectID": "week05/index.html#preparation-before-class",
    "href": "week05/index.html#preparation-before-class",
    "title": "Week 05: using text as data",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\nüìö Background Reading\n\n\nNo specific readings required - we‚Äôll learn by doing\n\nNo need for natural language processing background.\nThinking ahead: How would you manually determine if someone sounds ‚Äúpositive‚Äù or ‚Äúnegative‚Äù about something?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#review",
    "href": "week05/index.html#review",
    "title": "Week 05: using text as data",
    "section": "Review",
    "text": "Review\n\n\nüìà Assignment Review (15 min)\n\n\nKey Learning Points:\n\nFancy graphs != good graphs (good graph ‚Üê careful design)\nPrecise interpretation &gt;&gt; BS\nLess is more\nShow only what you understand deeply\n\nLess is More Principle: Simplicity often reveals insights better than complexity",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#class-material",
    "href": "week05/index.html#class-material",
    "title": "Week 05: using text as data",
    "section": "Class Material",
    "text": "Class Material\n\n\nüìä Materials Setup\n\n\nData Context:\n\nThese are real post-match interviews from Premier League managers discussing game results.\nEach text represents one manager‚Äôs full response to journalists after a match.\nPreprocessing: Replaced player and team names with ‚ÄúPLAYER‚Äù and ‚ÄúTEAM‚Äù - removes potential bias\n\nPractice Dataset: Contains 5 post-match interviews from football managers\n\nStudent test file (CSV)\nStudent test file (Excel)\n\nReference Materials:\n\nGeneral sentiment rating scale - our classification framework\nDomain-specific lexicon - football-specific sentiment terms\n\n\n\n\n\nüìñ NLP Fundamentals Lecture (25 min)\n\n\nSlideshow: Text to Data\nKey Concepts Covered:\n\nTokenization: Breaking text into meaningful units\nPreprocessing: Stemming, lemmatization, stop word removal\n\nBag of Words: Converting text to numerical features\nDomain knowledge: Why football-specific terms matter\nSentiment Analysis: Methods from simple dictionaries to AI models\n\nPractical Focus: How these concepts apply to manager interviews\n\n\n\n\n‚öΩ Manual vs.¬†AI Sentiment Workshop (40 min)\n\n\nPhase 1: Human Rating (15 min)\n\nIndividual work: Rate the 5 provided manager statements using the sentiment scale\n5-unit scale: -2 (very negative) to +2 (very positive)\n\nPhase 2: AI Rating (15 min)\n\nUse your preferred AI to rate the same 5 statements\nTry different prompting approaches: simple vs.¬†detailed instructions\nCompare AI‚Äôs reasoning with your own thought process\n\nPhase 3: Domain Lexicon Exploration (10 min)\n\nExamine the football-specific lexicon\nDiscuss how domain knowledge affects interpretation\n\nDiscussion: Validation and Sentiment Analysis\n\nObjective: Discuss validation techniques used in sentiment analysis.\nTopics for discussion:\n\nDifferences between manual and AI ratings\nGround Truth\nIntroduction to validation methods:\nIf ground truth ‚Äì can do confusion maztric, calculate accuracy\nIf no ground truth ‚Äì measure agreement between humans and AI. test difference.\n\nAI is average, but‚Ä¶\nAI with persona?\nAI biased ?\n\n\n\nExtra: Prediction of score\nIf time permits: look at predicting score\n\nModeling choices of results\nThink about how you would do it first\nCheck how AI thinks about, rate the examples and look at explanations\ntake the 5 examples, and compare your predictions vs the AI predictions",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#discussion-questions",
    "href": "week05/index.html#discussion-questions",
    "title": "Week 05: using text as data",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nEnd of Week Reflection:\n\nHow precise is AI in sentiment analysis?\nHow did you compare to AI in terms of scores? How did any difference make you feel?\nCan you think of a past project where AI could have helped you upgrade it?\n[SUGGESTED ADDITION] How important was football knowledge for accurate rating? Would someone unfamiliar with the sport rate differently?\n[SUGGESTED ADDITION] Did AI provide consistent explanations for its ratings? When did its reasoning seem flawed?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#assignment",
    "href": "week05/index.html#assignment",
    "title": "Week 05: using text as data",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nAssignment 5: Student-Specific Sentiment Analysis\n\n\n\nDue: Before Week 6\nCompare manual ratings with AI-generated ratings on your own random sample of 25 interview texts.\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week05/index.html#background-tools-and-resources",
    "href": "week05/index.html#background-tools-and-resources",
    "title": "Week 05: using text as data",
    "section": "Background, Tools and Resources",
    "text": "Background, Tools and Resources\nUnderstanding Manager Interviews:\n\n[SUGGESTED ADDITION] Post-match context matters: Managers speak differently after wins vs.¬†losses vs.¬†draws\n[SUGGESTED ADDITION] Tactical language: ‚ÄúWe dominated possession‚Äù (positive) vs.¬†‚ÄúThey dominated us‚Äù (negative)\n[SUGGESTED ADDITION] Defensive comments: Managers often protect players while expressing frustration with officiating or luck\n\nText Preprocessing for Football Data:\n\n[SUGGESTED ADDITION] Player names: Replaced with ‚ÄúPLAYER‚Äù in our dataset - why might this matter for sentiment analysis?\n[SUGGESTED ADDITION] Team names: Replaced with ‚ÄúTEAM‚Äù - removes potential bias\n[SUGGESTED ADDITION] Punctuation and tone: Ellipses (‚Ä¶) often indicate pauses or frustration\n\nSentiment Analysis Approaches Comparison:\n\n\n\n\n\n\n\n\nApproach\nStrengths\nBest Use for Football Interviews\n\n\n\n\nManual Rating\nContext-aware, domain expertise\nSmall samples, validation\n\n\nDomain Lexicon\nFast, transparent, detects obvious sentiment\nBaseline analysis, large samples\n\n\nAI Models\nHandles context, mixed sentiments\nComplex emotional nuance",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week06/index.html#preparation-before-class",
    "href": "week06/index.html#preparation-before-class",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\nüîß API Setup\n\n\nAPI Access Setup:\n\nHow to get AI API keys (OpenAI ChatGPT and Anthropic Claude)\nBudget: ~$5 minimum for course exercises\n\nAPI Learning Resources:\n\nIntroduction to APIs - fundamental concepts\nAdvanced API knowledge - how APIs work under the hood\n\n\n\n\n\nüìä Preparation\n\n\nMaterials from Moodle/Course Repository:\n\nCombined interview dataset (text_id level)\nGame information with results (win/draw/loss encoding)\nCombined student ratings from Week 5 assignments (aggregated, anonymized)\nDomain lexicon ratings\n\nData Structure:\n\ntexts: Individual interview transcripts with metadata\ngames: Match results and context information (soccer has 3 outcomes: win, draw, loss ‚Äì impcation for result encoding)\nratings: Human ratings, AI ratings, lexicon scores by text_id\n\nCode Examples Available:\n\nR implementation requires API key setup in R\nPython implementation\n\ntext and prompts\n\nsentiment guidelines\n\nOther\n\nMore advanced knowledge on APIs ‚Äì how APIs work",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#review",
    "href": "week06/index.html#review",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Review",
    "text": "Review\n\n\nüí¨ Assignment 5 Discussion (20 min)\n\n\nReview of transformation options\n\nlexicon based counting numbers ‚Äì&gt; generate a transparent script\nMachine-learning classifiers, n-grams etc.\nLLM (transformer-based) one-shot: treating the LLM like a giant classifier: you hand it raw text and ask classes of sentiment. (Deep contextual understanding‚Äîword embeddings, attention across the whole sentence etc decide the sentiment.) No separate sentiment lexicon; it‚Äôs all encoded in the model weights.\nYou can also take a pretrained LLM and continue training it on thousands of labeled review. See our example guidelines HERE\n\nSharing Experiences:\n\nHuman vs.¬†AI rating differences: Where did you disagree most?\nRating challenges: What aspects of manager interviews were hardest to classify?\nConsistency issues: Did you rate similar texts consistently? Did AI? Was there a consistent gap?\nDomain knowledge impact: How much did football expertise affect your ratings?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#class-material",
    "href": "week06/index.html#class-material",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Class Material",
    "text": "Class Material\n\n\nüîç API Walkthrough Sessions (30 min)\n\n\nIntroduction\n\nHow to use APIs\n\nBeginner-Friendly Examples:\n\nSimple walkthrough with GDP data ‚Äì uses World Bank and FRED APIs\nBit harder walkthrough with football data ‚Äì uses FBREF soccer data. Guess the club for example.\n\nMore advanced stuff\n\nMore advanced knowledge on APIs ‚Äì how APIs work\n\n\n\n\n\nüìà Data Analysis Workshop (30 min)\n\n\nData Integration Task:\n\nTake the aggregated file and ask AI for a readme. Discuss what is in the data\nCompare human, domain lexicon and AI rating. For human and AI take the average.\nThink of an interesting comparison using AI rating\nCompare results by human and lexicon rating\n\nDiscussion 2\n\nWhat is ground truth\n\nHow to integrate AI into research\n\ncombine data with text\nthink RQ and how you‚Äôd use AI\n\n\n\n\n\nüéØ Advanced Applications (20 min)\n\n\nAdditional tasks if time permits\nPredict gender and result\nNote: Men‚Äôs teams have male managers, women‚Äôs teams have female managers in this dataset\n\nShow AI all texts and ask to predict the gender of speaker\nShow AI all texts and ask to predict the result (manager‚Äôs team won, drew, lost)\n\nDiscussion:\nWhat linguistic cues might reveal gender? Are these reliable?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#end-of-week-discussion-points",
    "href": "week06/index.html#end-of-week-discussion-points",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "End of Week Discussion points",
    "text": "End of Week Discussion points\n\nGround Truth Problem: In sentiment analysis, what constitutes the ‚Äúcorrect‚Äù answer? How do we validate when humans disagree?\nAPI Integration: What are the benefits and costs (risks) of using API to scale text analysis?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#assignment",
    "href": "week06/index.html#assignment",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Assignment",
    "text": "Assignment\n\n\n\n\n\n\nAssignment 6: Gender Classification Pipeline\n\n\n\nDue: Optional extension exercise\nCreate a similar pipeline for predicting manager gender from interview text, including AI explanations of classification decisions.\nFull Assignment Details",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week06/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week06/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 06: Sentiment Analysis with AI",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nAI helped writing the Python code and translating to R. But it needed a great deal of debugging: working with tests and building stable pipelines are hard.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 6: Text as Data II"
    ]
  },
  {
    "objectID": "week05/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week05/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 05: using text as data",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nIt‚Äôs worth noting that my knowledge on NLP was bag of words, bigrams and TF-IDF. AI really helped making the jump.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 5: Text as Data I"
    ]
  },
  {
    "objectID": "week00/index.html",
    "href": "week00/index.html",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "",
    "text": "Week 00: AI-Assisted Coding Prep\n\n\nBoosting coding for data analysis with AI tools",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/index.html#preparation-before-class",
    "href": "week00/index.html#preparation-before-class",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Preparation / Before Class",
    "text": "Preparation / Before Class\n\n\nüîß Tool Setup\n\n\nRequired Access:\n\nChatGPT account (free tier sufficient for testing)\nClaude account (free tier sufficient)\n\nRecommended Setup:\n\nGitHub account for Copilot (free for students)\nVSCode, RStudio, or Jupyter Notebook installed\nYour preferred data analysis language ready (R/Python)\nYou can you a great deal of this for other languages like SQL, Stata or Julia.\n\nOptional but Valuable:\n\nCursor AI editor (free trial available)",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/index.html#class-material",
    "href": "week00/index.html#class-material",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Class Material",
    "text": "Class Material\n\n\nü§ñ Why AI + Data Analysis Coding Works (20 min)\n\n\nCovers key set of tasks:\n\nRepetitive patterns: Data analysis has common workflows (load ‚Üí clean/wrangle ‚Üí analyze ‚Üí visualize)\nWell-documented libraries: pandas, scikit-learn, as well as dplyr, ggplot2, are extensively covered in AI training data (had been in use for a while). With updates, it knows more recent libraries from tensorflow to fixest (R)\nClear intentions: ‚ÄúCreate a scatterplot with regression line‚Äù is specific enough for good code generation. Well established ‚Äúgood practice‚Äù\nIterative nature: Data analysis involves lots of tweaking and refinement ‚Äì chat aspect helpful.\n\nWhat AI Excels At:\n\nWell defined code chunks and setup (libraries, dependencies)\nSyntax, how exactly do certain coding tasks (such as regex, loops)\nStandard statistical procedures\nData manipulation and cleaning\nBasic visualization code\n\nHuman Oversight and Decisions are Needed:\n\nMost code\nResearch design decisions\nStatistical / econometric interpretation\nDomain-specific logic\nFinalize question (vs bland suggestions)\nQuality control and validation\n\n\n\n\n\nüìù Effective Prompting for Data Analysis (25 min)\n\n\nModel Recommendations:\n\nChatGPT 4o / Claude Sonnet 4. works about equally fine.\n\n\nPrompting Best Practices\nSome ideas to help get the code do what you want, or at least close enough.\nBe specific\n\nBe specific about libraries: ‚ÄúUsing pandas and seaborn‚Ä¶‚Äù vs.¬†‚ÄúUsing R and ggplot2‚Ä¶‚Äù\nInclude data structure: ‚ÄúDataFrame with columns: date, price, volume‚Äù\nSpecify output format: ‚ÄúSave as PNG for publication‚Äù or ‚ÄúReturn as tidy data table‚Äù or ‚Äúgive back markedown (latex) text with equations‚Äù\n\nDefine language, preferences\n\nI use R and tidyverse, so unless specified, use that\nI use Python, and when possible prefer polars to pandas\nAsk for comments: ‚ÄúInclude detailed comments explaining each step‚Äù\n\nFor frequent tasks, AI will know which library to use in a language\n\nFilter on size&lt;5 ‚Äì will do it in Python Pandas as default.\n\nScatterplot of sales and employment ‚Äì Will do it seaborn / matplotlib or ggplot in R.\n\nAdvice include data structure\n\nIf possible upload the data\nif not, upload a small bit of data, like 1/1000 random sample\nStart with creating a data dictionary (see also class 3)\n\n\n\nExample prompts\nYou can experiment with a vague prompt or being specific.\nExample Prompt 1: broad\n\"Here is some sales data, summarize regional variation.\"\nvs\nExample Prompt 2: with details\n\"Create Python (R) code using pandas (dplyr) to:\n\n1) Load CSV with columns id, date, sales, region\n2) Filter for 2023 data  \n3) Group by region and calculate mean sales\n4) Create a bar chart with plotnine / ggplot2\"\nWhat are the pros and cons of each?\n\n\nSome tasks where AI works well\n1. Data Cleaning Pipeline 2. Exploratory Data Analysis 3. Statistical Modeling 4. Machine Learning Pipeline\n\n\n\n\n\nüìù Creating Graphs exercise (40 min)\n\n\nA full practice exercise\nCreated one example on gender wage gap.\nIt follows the case study on the in Chapter 09\nWe start with actual earnings data from the US, called CPS MORG.\nWe end up with:\n\n\n\nNice heatmap\n\n\nOther ideas\n\nInstructors may have their datasets and exercises. We have plenty here: Case studies, each comes with data",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/index.html#discussion-questions",
    "href": "week00/index.html#discussion-questions",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Discussion Questions",
    "text": "Discussion Questions\nReflection:\n\nWhich AI tool felt most natural for your coding style?\nWhere did AI suggestions surprise you (positively or negatively)?\nHow might this change your typical data analysis workflow?\nWhat validation steps would you add when using AI-generated code?",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/index.html#practice-exercises",
    "href": "week00/index.html#practice-exercises",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nIdeas\n\nInstructors may have their datasets and exercises\nWe have plenty here: Case studies, each comes with data\n\nOne full example\n\nCreated one example following the case study on the gender wage gap",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/index.html#background-tools-and-resources",
    "href": "week00/index.html#background-tools-and-resources",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Background, Tools and Resources",
    "text": "Background, Tools and Resources\nGetting Started:\n\nGitHub Student Pack ‚Äì Free Copilot access\nCursor AI ‚Äì AI-first code editor\nOpenAI Codex Cookbook ‚Äì Advanced prompting examples\n\nKey Insight: AI coding assistance is most powerful when you understand the underlying concepts. Use AI to accelerate implementation, not replace understanding.\nNext Week: Week 1 - LLM Review where we‚Äôll explore broader AI concepts for data analysis.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/index.html#some-personal-comments-on-ai-and-this-class",
    "href": "week00/index.html#some-personal-comments-on-ai-and-this-class",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Some personal comments on AI and this class",
    "text": "Some personal comments on AI and this class\n\nWhile this class is called week00, it was created last. You may have guessed righ.\nAI (Claude 4.0) created a great deal of this class following a detailed prompt. I also asked whether to keep it as week00 or change all numbering. Great answers.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#nameing",
    "href": "week00/assets/week00-ask-ai.html#nameing",
    "title": "The name of week00",
    "section": "Nameing",
    "text": "Nameing\n\n\nüîß Is name week00 OK? ::: ::: {.card-body}\nG√°bor:\n‚ÄúI‚Äôm struggling with this decision. Shall I add it as week00 or rename all weeks (01‚Äì&gt;02 etc) and have this as 01. what do you think?‚Äù**\nClaude:\nI‚Äôd recommend keeping it as Week 00 for several practical and pedagogical reasons:\n\n‚úÖ Arguments for Week 00:\nPractical Benefits: - No disruption: Keeps all existing numbering, assignments, internal links intact - Less work: Avoids the error-prone task of updating references throughout - Flexibility: Students with strong AI coding experience could potentially skip/skim\nPedagogical Logic: - Common convention: Week 00 is standard in CS courses for ‚Äúsetup‚Äù content - Clear signaling: ‚ÄúThis is foundational preparation, not optional‚Äù - Entry point flexibility: Students can assess their own readiness\n\n\nüéØ Make Week 00 Feel Essential:\nIn course description/syllabus: - ‚ÄúWeek 00 (REQUIRED): AI-Assisted Coding Fundamentals‚Äù - ‚ÄúEssential preparation for all subsequent weeks‚Äù\nIn Week 01 intro: - ‚ÄúThis course assumes familiarity with AI coding tools covered in Week 00‚Äù - Brief recap/check that students completed the prep\nCross-references: - Link back to Week 00 techniques throughout other weeks - ‚ÄúAs we covered in Week 00‚Äôs prompting strategies‚Ä¶‚Äù\n\n\n‚öñÔ∏è Alternative Consideration:\nThe only reason I‚Äôd consider renumbering is if you find that students consistently skip Week 00 and then struggle. But you can address this with: - Clear communication about importance - Quick competency check in Week 01 - Making Week 00 content prerequisite for assignments\nRecommendation: Start with Week 00, see how it works in practice, and renumber later if needed. Much easier to go from 00‚Üí01 than to undo a full renumbering.\nWhat‚Äôs your gut feeling on student behavior - would they skip something labeled ‚ÄúWeek 00‚Äù?"
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#arguments-for-week-00",
    "href": "week00/assets/week00-ask-ai.html#arguments-for-week-00",
    "title": "The name of week00",
    "section": "‚úÖ Arguments for Week 00:",
    "text": "‚úÖ Arguments for Week 00:\nPractical Benefits: - No disruption: Keeps all existing numbering, assignments, internal links intact - Less work: Avoids the error-prone task of updating references throughout - Flexibility: Students with strong AI coding experience could potentially skip/skim\nPedagogical Logic: - Common convention: Week 00 is standard in CS courses for ‚Äúsetup‚Äù content - Clear signaling: ‚ÄúThis is foundational preparation, not optional‚Äù - Entry point flexibility: Students can assess their own readiness"
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#make-week-00-feel-essential",
    "href": "week00/assets/week00-ask-ai.html#make-week-00-feel-essential",
    "title": "The name of week00",
    "section": "üéØ Make Week 00 Feel Essential:",
    "text": "üéØ Make Week 00 Feel Essential:\nIn course description/syllabus: - ‚ÄúWeek 00 (REQUIRED): AI-Assisted Coding Fundamentals‚Äù - ‚ÄúEssential preparation for all subsequent weeks‚Äù\nIn Week 01 intro: - ‚ÄúThis course assumes familiarity with AI coding tools covered in Week 00‚Äù - Brief recap/check that students completed the prep\nCross-references: - Link back to Week 00 techniques throughout other weeks - ‚ÄúAs we covered in Week 00‚Äôs prompting strategies‚Ä¶‚Äù"
  },
  {
    "objectID": "week00/assets/week00-ask-ai.html#alternative-consideration",
    "href": "week00/assets/week00-ask-ai.html#alternative-consideration",
    "title": "The name of week00",
    "section": "‚öñÔ∏è Alternative Consideration:",
    "text": "‚öñÔ∏è Alternative Consideration:\nThe only reason I‚Äôd consider renumbering is if you find that students consistently skip Week 00 and then struggle. But you can address this with: - Clear communication about importance - Quick competency check in Week 01 - Making Week 00 content prerequisite for assignments\nRecommendation: Start with Week 00, see how it works in practice, and renumber later if needed. Much easier to go from 00‚Üí01 than to undo a full renumbering.\nWhat‚Äôs your gut feeling on student behavior - would they skip something labeled ‚ÄúWeek 00‚Äù?"
  },
  {
    "objectID": "week00/index.html#learning-objectives",
    "href": "week00/index.html#learning-objectives",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, students will:\n\nUnderstand how AI transforms coding workflows for data analysis tasks\nLearn effective prompting strategies for data analysis code generation\nExperience key AI coding tools: ChatGPT Canvas, Claude Projects, GitHub Copilot\nPractice AI-assisted coding for common data analysis tasks\nSet up integrated development environments with AI assistance\nFocus on R and Python.",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/assets/creating-graphs.html",
    "href": "week00/assets/creating-graphs.html",
    "title": "Creating Graphs",
    "section": "",
    "text": "Is there a difference between how much men and women make? How does it vary by age?\nThe topic is related to the case study on gender gap in earnings.\n\n\n\nAs a case study, consider the earnings case study from the Data Analysis textbook\n\nHave a look at the info readme\nDownload morg-2014-emp.csv from earnings data\nDownload cpsx.pdf, which is an old school codebook\nRead in morg-2014-emp.csv\n\nOne state\nLet us start filtering on the largest state\n1. Create an ordered frequency table of the state, and filter on the largest one\nI used RStudio with GitHub Copilot to write all three code snippets. It automatically guessed the third bit.\n```{r}\n\n# setup\nlibrary(tidyverse)\n\n# ordered freq table of state\nmorg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(state = fct_reorder(state, n)) %&gt;%\n  ggplot(aes(x = state, y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Number of Employees by State in Morg\",\n       x = \"State\",\n       y = \"Number of Employees\") +\n  theme_minimal()\n\n# find the largest state in terms of number of obs\n\nstate_max=morg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(1) %&gt;%\n  pull(state)\n\n\n# filter morg for the largest state\nmorg_largest_state &lt;- morg %&gt;%\n  filter(state == state_max)\n\n```\nHold on, what does max state=5 mean? Ask ChatGPT.\n\nSave this file as morg-2014-emp-state5.csv. You can also find it at the data section here\nGet to know the file\n\nUpload the file and ask for a variable dictionary. After 2 iterations, I used this.\n\nCreate a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download.\n\n\nChatGPT created a table, yes.\nBut look at what Claude did, amazing.\n\n\n\n\nFirst try\nIs there a gender gap in earnings? Show me a graph.\nChatGPT will run Data Analysis ‚Äì create a short Python script and display a graph.\n```{python}\nimport matplotlib.pyplot as plt\n\n# Check for gender and earnings columns\nif \"sex\" in df.columns and \"earnwke\" in df.columns:\n    # Drop rows with missing or zero earnings\n    df_filtered = df[(df[\"earnwke\"].notna()) & (df[\"earnwke\"] &gt; 0)]\n\n    # Group by sex and calculate mean earnings\n    gender_earnings = df_filtered.groupby(\"sex\")[\"earnwke\"].mean().reset_index()\n    gender_earnings[\"earnwke\"] = gender_earnings[\"earnwke\"].round(2)\n\n    # Create a bar plot\n    plt.figure(figsize=(6, 4))\n    plt.bar(gender_earnings[\"sex\"], gender_earnings[\"earnwke\"])\n    plt.title(\"Average Weekly Earnings by Gender\")\n    plt.xlabel(\"Sex (1 = Male, 2 = Female)\")\n    plt.ylabel(\"Mean Weekly Earnings\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\nelse:\n    raise ValueError(\"Required columns 'sex' and 'earnwke' not found in dataset.\")\n\n```\nHere is the chart:\n Followed by this text ‚ÄúHere is the graph showing the gender gap in weekly earnings. Males (1) earn more on average than females (2) in this sample. Let me know if you want to adjust for hours worked or other covariates.‚Äù\nYes, adjust by age.\n\n\n\nThe graph I was created\n\n\nNow what? By now, you have some idea, what you‚Äôd like. For me, it was:\nMaybe binscatter by year, start 21. fit loess for males and females\nTry it!\nFor me, it turns out that it needed some adjustments after several iterations to get this. Add CI. Add Footnote. Increase FN font size. Add N of obs. Change color scheme.\n```{python}\n\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\n# Get viridis colors\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\n# Count observations\nn_obs = len(reg_data)\n\n# Updated footnote\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21‚Äì80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\n\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\n\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\n\n# Updated footnote with larger font\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\n\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nIt took some iterations, starting with:\n\n\n\nFourth iteration\n\n\nTo get to something I liked. It made errors, forgot labels, and made coding errors. Check the process.\n\n\n\nFourth iteration\n\n\n```{python}\nimport statsmodels.api as sm\nfrom sklearn.utils import resample\n\n# Restrict and prepare data for regression\nreg_data = df[\n    ['age', 'earnwke', 'uhours', 'sex', 'grade92', 'occ2012']\n].dropna()\nreg_data = reg_data[\n    (reg_data['age'] &gt;= 21) & (reg_data['age'] &lt;= 62) &\n    (reg_data['uhours'] &gt; 0) & (reg_data['earnwke'] &gt; 0)\n].copy()\n\n# Compute hourly wage\nreg_data['hourly'] = reg_data['earnwke'] / reg_data['uhours']\n\n# Residualize hourly wage on occ and grade\nX = pd.get_dummies(reg_data[['grade92', 'occ2012']].astype(\"category\"), drop_first=True)\nX = sm.add_constant(X)\nmodel = sm.OLS(reg_data['hourly'], X).fit()\nreg_data['resid'] = model.resid\nreg_data['gender'] = reg_data['sex'].map({1: 'Male', 2: 'Female'})\n\n# Define bootstrap function\ndef bootstrap_loess(data, gender, x_col, y_col, span=0.3, n_boot=100):\n    from statsmodels.nonparametric.smoothers_lowess import lowess\n\n    subset = data[data['gender'] == gender]\n    x_vals = np.linspace(21, 80, 60)\n    y_boot = np.zeros((n_boot, len(x_vals)))\n\n    for i in range(n_boot):\n        boot_sample = resample(subset)\n        smoothed = lowess(boot_sample[y_col], boot_sample[x_col], frac=span, return_sorted=True)\n        y_interp = np.interp(x_vals, smoothed[:, 0], smoothed[:, 1])\n        y_boot[i, :] = y_interp\n\n    y_mean = y_boot.mean(axis=0)\n    y_low = np.percentile(y_boot, 2.5, axis=0)\n    y_high = np.percentile(y_boot, 97.5, axis=0)\n    return x_vals, y_mean, y_low, y_high\n\n# Run bootstrap LOESS for both genders\nx_m, y_m, l_m, u_m = bootstrap_loess(reg_data, 'Male', 'age', 'resid')\nx_f, y_f, l_f, u_f = bootstrap_loess(reg_data, 'Female', 'age', 'resid')\n\n# Colors and footnote\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\nn_obs = len(reg_data)\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21‚Äì80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nFinal thoughts \n\nWhile AI is great, it needed some iterations to get what I want, not just something.\nOh, and you can go further. Think about this graph. Is the econometrics of it okay?\n\n\n\n\n Ok now show a heatmap of hourly wages by occupation and education level. Keep viridis. \nNot great.\nCreate a few distinct categories of occupation.\nThen create bins for education.\nAlmost.\nSwap colors\nNow we are talking:\n\n\n\nFourth iteration\n\n\n\n\n\nMany other options. Two extremes.\n\nDescribe age. In a graph. (Hint: think about binsize)\nCreate an interesting and well-done scatterplot (turn o3 on or use Claude Sonnet)"
  },
  {
    "objectID": "week00/assets/creating-graphs.html#an-example",
    "href": "week00/assets/creating-graphs.html#an-example",
    "title": "Creating Graphs",
    "section": "",
    "text": "Is there a difference between how much men and women make? How does it vary by age?\nThe topic is related to the case study on gender gap in earnings.\n\n\n\nAs a case study, consider the earnings case study from the Data Analysis textbook\n\nHave a look at the info readme\nDownload morg-2014-emp.csv from earnings data\nDownload cpsx.pdf, which is an old school codebook\nRead in morg-2014-emp.csv\n\nOne state\nLet us start filtering on the largest state\n1. Create an ordered frequency table of the state, and filter on the largest one\nI used RStudio with GitHub Copilot to write all three code snippets. It automatically guessed the third bit.\n```{r}\n\n# setup\nlibrary(tidyverse)\n\n# ordered freq table of state\nmorg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(state = fct_reorder(state, n)) %&gt;%\n  ggplot(aes(x = state, y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Number of Employees by State in Morg\",\n       x = \"State\",\n       y = \"Number of Employees\") +\n  theme_minimal()\n\n# find the largest state in terms of number of obs\n\nstate_max=morg %&gt;%\n  group_by(state) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(1) %&gt;%\n  pull(state)\n\n\n# filter morg for the largest state\nmorg_largest_state &lt;- morg %&gt;%\n  filter(state == state_max)\n\n```\nHold on, what does max state=5 mean? Ask ChatGPT.\n\nSave this file as morg-2014-emp-state5.csv. You can also find it at the data section here\nGet to know the file\n\nUpload the file and ask for a variable dictionary. After 2 iterations, I used this.\n\nCreate a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download.\n\n\nChatGPT created a table, yes.\nBut look at what Claude did, amazing.\n\n\n\n\nFirst try\nIs there a gender gap in earnings? Show me a graph.\nChatGPT will run Data Analysis ‚Äì create a short Python script and display a graph.\n```{python}\nimport matplotlib.pyplot as plt\n\n# Check for gender and earnings columns\nif \"sex\" in df.columns and \"earnwke\" in df.columns:\n    # Drop rows with missing or zero earnings\n    df_filtered = df[(df[\"earnwke\"].notna()) & (df[\"earnwke\"] &gt; 0)]\n\n    # Group by sex and calculate mean earnings\n    gender_earnings = df_filtered.groupby(\"sex\")[\"earnwke\"].mean().reset_index()\n    gender_earnings[\"earnwke\"] = gender_earnings[\"earnwke\"].round(2)\n\n    # Create a bar plot\n    plt.figure(figsize=(6, 4))\n    plt.bar(gender_earnings[\"sex\"], gender_earnings[\"earnwke\"])\n    plt.title(\"Average Weekly Earnings by Gender\")\n    plt.xlabel(\"Sex (1 = Male, 2 = Female)\")\n    plt.ylabel(\"Mean Weekly Earnings\")\n    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\nelse:\n    raise ValueError(\"Required columns 'sex' and 'earnwke' not found in dataset.\")\n\n```\nHere is the chart:\n Followed by this text ‚ÄúHere is the graph showing the gender gap in weekly earnings. Males (1) earn more on average than females (2) in this sample. Let me know if you want to adjust for hours worked or other covariates.‚Äù\nYes, adjust by age.\n\n\n\nThe graph I was created\n\n\nNow what? By now, you have some idea, what you‚Äôd like. For me, it was:\nMaybe binscatter by year, start 21. fit loess for males and females\nTry it!\nFor me, it turns out that it needed some adjustments after several iterations to get this. Add CI. Add Footnote. Increase FN font size. Add N of obs. Change color scheme.\n```{python}\n\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\n# Get viridis colors\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\n# Count observations\nn_obs = len(reg_data)\n\n# Updated footnote\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21‚Äì80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\n\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\n\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\n\n# Updated footnote with larger font\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\n\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nIt took some iterations, starting with:\n\n\n\nFourth iteration\n\n\nTo get to something I liked. It made errors, forgot labels, and made coding errors. Check the process.\n\n\n\nFourth iteration\n\n\n```{python}\nimport statsmodels.api as sm\nfrom sklearn.utils import resample\n\n# Restrict and prepare data for regression\nreg_data = df[\n    ['age', 'earnwke', 'uhours', 'sex', 'grade92', 'occ2012']\n].dropna()\nreg_data = reg_data[\n    (reg_data['age'] &gt;= 21) & (reg_data['age'] &lt;= 62) &\n    (reg_data['uhours'] &gt; 0) & (reg_data['earnwke'] &gt; 0)\n].copy()\n\n# Compute hourly wage\nreg_data['hourly'] = reg_data['earnwke'] / reg_data['uhours']\n\n# Residualize hourly wage on occ and grade\nX = pd.get_dummies(reg_data[['grade92', 'occ2012']].astype(\"category\"), drop_first=True)\nX = sm.add_constant(X)\nmodel = sm.OLS(reg_data['hourly'], X).fit()\nreg_data['resid'] = model.resid\nreg_data['gender'] = reg_data['sex'].map({1: 'Male', 2: 'Female'})\n\n# Define bootstrap function\ndef bootstrap_loess(data, gender, x_col, y_col, span=0.3, n_boot=100):\n    from statsmodels.nonparametric.smoothers_lowess import lowess\n\n    subset = data[data['gender'] == gender]\n    x_vals = np.linspace(21, 80, 60)\n    y_boot = np.zeros((n_boot, len(x_vals)))\n\n    for i in range(n_boot):\n        boot_sample = resample(subset)\n        smoothed = lowess(boot_sample[y_col], boot_sample[x_col], frac=span, return_sorted=True)\n        y_interp = np.interp(x_vals, smoothed[:, 0], smoothed[:, 1])\n        y_boot[i, :] = y_interp\n\n    y_mean = y_boot.mean(axis=0)\n    y_low = np.percentile(y_boot, 2.5, axis=0)\n    y_high = np.percentile(y_boot, 97.5, axis=0)\n    return x_vals, y_mean, y_low, y_high\n\n# Run bootstrap LOESS for both genders\nx_m, y_m, l_m, u_m = bootstrap_loess(reg_data, 'Male', 'age', 'resid')\nx_f, y_f, l_f, u_f = bootstrap_loess(reg_data, 'Female', 'age', 'resid')\n\n# Colors and footnote\nimport matplotlib.cm as cm\nfrom matplotlib.colors import to_hex\n\nviridis = cm.get_cmap(\"viridis\", 2)\ncolor_male = to_hex(viridis(0.1))\ncolor_female = to_hex(viridis(0.8))\n\nn_obs = len(reg_data)\nfootnote = (\n    f\"Notes: Residual hourly wages from a linear regression controlling for occupation (C(occ2012))\\n\"\n    f\"and education (C(grade92)), using 2014 CPS MORG data from California (N = {n_obs}). Residuals smoothed by\\n\"\n    f\"LOESS (span = 0.3), with 95% CI from 100 bootstrap replications. Sample restricted to individuals aged\\n\"\n    f\"21‚Äì80 with positive usual weekly hours and earnings.\"\n)\n\n# Plot\nplt.figure(figsize=(9, 6))\nplt.plot(x_m, y_m, label=\"Men\", color=color_male, linewidth=2)\nplt.fill_between(x_m, l_m, u_m, color=color_male, alpha=0.2)\nplt.plot(x_f, y_f, label=\"Women\", color=color_female, linewidth=2)\nplt.fill_between(x_f, l_f, u_f, color=color_female, alpha=0.2)\n\nplt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\nplt.title(\"Adjusted Gender Wage Gap Across Age\", fontsize=15)\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Residual Hourly Wage (USD)\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend(title=\"Gender\", labels=[\"Men\", \"Women\"], fontsize=10, title_fontsize=11)\nplt.figtext(0.01, -0.12, footnote, fontsize=10, ha=\"left\", va=\"top\")\nplt.tight_layout(rect=[0, 0.12, 1, 1])\nplt.savefig(\"/mnt/data/gender_wage_gap_viridis_residuals.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n```\nFinal thoughts \n\nWhile AI is great, it needed some iterations to get what I want, not just something.\nOh, and you can go further. Think about this graph. Is the econometrics of it okay?\n\n\n\n\n Ok now show a heatmap of hourly wages by occupation and education level. Keep viridis. \nNot great.\nCreate a few distinct categories of occupation.\nThen create bins for education.\nAlmost.\nSwap colors\nNow we are talking:\n\n\n\nFourth iteration\n\n\n\n\n\nMany other options. Two extremes.\n\nDescribe age. In a graph. (Hint: think about binsize)\nCreate an interesting and well-done scatterplot (turn o3 on or use Claude Sonnet)"
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html",
    "href": "week00/assets/variable-dictionary-claude4.html",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "",
    "text": "This is my data and the codebook. Create a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download."
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#variable-definitions",
    "href": "week00/assets/variable-dictionary-claude4.html#variable-definitions",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Variable Definitions",
    "text": "Variable Definitions\n\nHousehold Identifiers\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\n‚Ä¶1\nRow Index\nInteger\nSequential row identifier\nContinuous\n\n\nhhid\nHousehold ID\nInteger\nUnique household identifier\n12-15 digits\n\n\nintmonth\nInterview Month\nString\nCalendar month of interview\n01-12 (Jan-Dec)\n\n\nstfips\nState FIPS\nString\nState FIPS code\n11-95\n\n\n\n\n\nSample Design\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nweight\nFinal Weight\nFloat\nPopulation weight (√ó100)\n0-20549\n\n\n\n\n\nLabor Market Variables\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nearnwke\nWeekly Earnings\nFloat\nUsual weekly earnings ($)\n0-2884\n\n\nuhours\nUsual Hours\nInteger\nUsual hours worked per week\n0-99\n\n\nlfsr94\nLabor Force Status\nString\nEmployment status recode\n1=Working, 2=With job not at work, 3=Looking, 4=Layoff, 5=Not in labor force\n\n\n\n\n\nDemographics\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nage\nAge\nInteger\nAge in years\n16-99\n\n\nsex\nSex\nInteger\nGender\n1=Male, 2=Female\n\n\nrace\nRace\nInteger\nRace/ethnicity\n1=White, 2=Black, 3=American Indian, 4=Asian/Pacific Islander, 5=Other\n\n\nethnic\nHispanic Origin\nFloat\nHispanic ethnicity\n1=Mexican American, 2=Chicano, 3=Mexicano, 4=Puerto Rican, 5=Cuban, 6=Central/South American, 7=Other Spanish, 8=All other\n\n\nmarital\nMarital Status\nInteger\nMarital status\n1=Married civilian spouse present, 2=Married AF spouse present, 3=Married spouse absent, 4=Widowed, 5=Divorced, 6=Separated, 7=Never married\n\n\n\n\n\nEducation\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\ngrade92\nEducation Level\nInteger\nHighest grade completed\n31=Less than 1st grade, 32=1st-4th grade, 33=5th-6th grade, 34=7th-8th grade, 35=9th grade, 36=10th grade, 37=11th grade, 38=12th grade no diploma, 39=HS graduate, 40=Some college, 41=Associate occupational, 42=Associate academic, 43=Bachelor‚Äôs degree, 44=Master‚Äôs degree, 45=Professional degree, 46=Doctorate\n\n\n\n\n\nFamily Structure\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nownchild\nOwn Children\nInteger\nNumber of own children &lt;18\n0-8\n\n\nchldpres\nChild Presence\nInteger\nPresence of own children &lt;18\nComplex coding scheme (see codebook)\n\n\n\n\n\nCitizenship\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nprcitshp\nCitizenship\nString\nCitizenship status\n1=Native born US, 2=Native born PR/US territory, 3=Native born abroad of US parents, 4=Foreign born naturalized, 5=Foreign born not citizen\n\n\n\n\n\nGeography\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nstate\nState Code\nInteger\n1960 Census state code\n11-95 (see codebook for state mapping)\n\n\n\n\n\nIndustry & Occupation\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nind02\nIndustry 2002\nString\n2002 NAICS industry classification\n170-9890\n\n\nocc2012\nOccupation 2012\nInteger\n2010 SOC occupation code\n10-9840\n\n\nclass\nClass of Worker\nString\nEmployment class\n1=Federal government, 2=State government, 3=Local government, 4=Private for profit, 5=Private non-profit, 6=Self-employed incorporated, 7=Self-employed not incorporated, 8=Without pay\n\n\n\n\n\nUnion Status\n\n\n\n\n\n\n\n\n\n\nVariable\nLabel\nType\nDescription\nValue Labels\n\n\n\n\nunionmme\nUnion Member\nString\nUnion membership status\n1=Yes, 2=No\n\n\nunioncov\nUnion Coverage\nString\nCovered by union contract\n1=Yes, 2=No"
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#data-quality-notes",
    "href": "week00/assets/variable-dictionary-claude4.html#data-quality-notes",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Data Quality Notes",
    "text": "Data Quality Notes\nMissing Data: Unable to calculate precise coverage statistics without data access. Refer to original CPS documentation for universe definitions.\nWeights: Use weight variable for population estimates. The earnings weight earnwt is preferred for earnings-related analyses.\nTime Period: This extract contains data from 2014 MORG files (outgoing rotation groups only).\nTop Coding: Earnings variables may be top-coded. Check for density spikes at maximum values."
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#key-variable-relationships",
    "href": "week00/assets/variable-dictionary-claude4.html#key-variable-relationships",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Key Variable Relationships",
    "text": "Key Variable Relationships\n\nLabor Force Status (lfsr94): Controls universe for many employment variables\nEducation (grade92): Uses post-1992 credential-based classification\nEarnings (earnwke): Available only for wage/salary workers; excludes self-employed\nHours (uhours): Usual hours at main job\nDemographics: Standard CPS demographic variables with consistent coding"
  },
  {
    "objectID": "week00/assets/variable-dictionary-claude4.html#usage-guidelines",
    "href": "week00/assets/variable-dictionary-claude4.html#usage-guidelines",
    "title": "CPS MORG 2014 Variable Dictionary by Claude Sonnet 4.0",
    "section": "Usage Guidelines",
    "text": "Usage Guidelines\n\nWeights: Always use appropriate weights for population estimates\nMissing Values: Check universe definitions in codebook\nTime Series: Be aware of classification changes across years\nEarnings: Exclude self-employed workers for wage analyses\nIndustry/Occupation: Codes based on 2002 NAICS and 2010 SOC systems\n\n\nSource: NBER CPS Labor Extracts, based on Bureau of Labor Statistics Current Population Survey\nDocumentation: See full codebook for detailed variable definitions and historical changes"
  },
  {
    "objectID": "week00/index.html#setting-up-tool",
    "href": "week00/index.html#setting-up-tool",
    "title": "Week 00: AI-Assisted Coding Prep",
    "section": "Setting up tool",
    "text": "Setting up tool\n\n\nüõ†Ô∏è AI Coding Tools Showcase (30 min)\n\n\nChatGPT Tools:\n\nCanvas: Collaborative coding environment for iterative development\nAdvanced Data Analysis: Upload datasets, generate analysis with code execution\nGPT-4.1: Optimized specifically for coding tasks\n\nClaude Tools:\n\nProjects: Upload full datasets and documentation for context-aware coding\nArtifacts: Code generation with real-time preview and editing\nClaude 4 Sonnet: Strong reasoning for complex analytical workflows\n\nSpecialized Coding Tools:\n\nGitHub Copilot: Inline code completion integrated into your existing editor\nCursor AI: AI-first code editor with context-aware suggestions\nReplit: Browser-based coding with AI assistance\n\nHands-on Demo:\n\nUpload sample dataset to ChatGPT Advanced Data Analysis\nCreate Claude Project with course data\nCompare code generation approaches\n\n\n\n\n\n‚ö° GitHub Copilot Integration (20 min)\n\n\nSetup in Different Environments:\nVSCode:\n\nInstall GitHub Copilot extension\nAuthenticate with GitHub account\nUse Ctrl+Space for suggestions, Tab to accept\n\nJupyter Notebook:\n\nInstall via VS Code Jupyter extension or JupyterLab extension\nInline suggestions while typing\nCopilot Chat for longer explanations\n\nRStudio:\n\nEnable GitHub Copilot in Global Options &gt; Code &gt; Completion\nWorks with R scripts and R Markdown\nSuggests tidyverse and base R patterns\n\nWorkflow Best Practices:\n\nWrite descriptive comments before code blocks\nUse meaningful variable names to guide suggestions\nAccept suggestions, then modify as needed\nUse Copilot Chat for explanations and debugging",
    "crumbs": [
      "Home",
      "Course Weeks",
      "Week 0: Coding with AI"
    ]
  },
  {
    "objectID": "week00/assets/variable_dictionary-chatgpt.html",
    "href": "week00/assets/variable_dictionary-chatgpt.html",
    "title": "CPS MORG 2014 Variable Dictionary by ChatGPT 4o",
    "section": "",
    "text": "my single prompt\nThis is my data and the codebook. Create a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download.\n\n\nVariable dictionary\n\n\n\nvarname\ntype\nmissing (%)\nmean\nmode\n\n\n\n\n‚Ä¶1\nint64\n0%\n197435.3\n83378\n\n\nhhid\nint64\n0%\n404953427059356.6\n250109964300640\n\n\nintmonth\nobject\n0%\nNA\nDecember\n\n\nstfips\nobject\n0%\nNA\nFL\n\n\nweight\nfloat64\n0%\n2550.8\n291.4682\n\n\nearnwke\nfloat64\n0%\n933.7\n2884.61\n\n\nuhours\nint64\n0%\n39.4\n40\n\n\ngrade92\nint64\n0%\n41.0\n39\n\n\nrace\nint64\n0%\n1.5\n1\n\n\nethnic\nfloat64\n89%\n3.8\n1.0\n\n\nage\nint64\n0%\n41.0\n44\n\n\nsex\nint64\n0%\n1.5\n2\n\n\nmarital\nint64\n0%\n3.6\n1\n\n\nownchild\nint64\n0%\n0.6\n0\n\n\nchldpres\nint64\n0%\n1.6\n0\n\n\nprcitshp\nobject\n0%\nNA\nNative, Born In US\n\n\nstate\nint64\n0%\n5.0\n5\n\n\nind02\nobject\n0%\nNA\nElementary and secondary schools (6111)\n\n\nocc2012\nint64\n0%\n3917.9\n430\n\n\nclass\nobject\n0%\nNA\nPrivate, For Profit\n\n\nunionmme\nobject\n0%\nNA\nNo\n\n\nunioncov\nobject\n7%\nNA\nNo\n\n\nlfsr94\nobject\n0%\nNA\nEmployed-At Work"
  },
  {
    "objectID": "week00/assets/variable-dictionary-chatgpt.html",
    "href": "week00/assets/variable-dictionary-chatgpt.html",
    "title": "CPS MORG 2014 Variable Dictionary by ChatGPT 4o",
    "section": "",
    "text": "my single prompt\nThis is my data and the codebook. Create a variable dictionary. Use the pdf i shared earlier. Output as markdown. For each variable: varname, labels, type, coverage (% missing), mean and mode. Round up numbers. Look at cps and provide short labels. Get me an .md I can download.\n\n\nVariable dictionary\n\n\n\nvarname\ntype\nmissing (%)\nmean\nmode\n\n\n\n\n‚Ä¶1\nint64\n0%\n197435.3\n83378\n\n\nhhid\nint64\n0%\n404953427059356.6\n250109964300640\n\n\nintmonth\nobject\n0%\nNA\nDecember\n\n\nstfips\nobject\n0%\nNA\nFL\n\n\nweight\nfloat64\n0%\n2550.8\n291.4682\n\n\nearnwke\nfloat64\n0%\n933.7\n2884.61\n\n\nuhours\nint64\n0%\n39.4\n40\n\n\ngrade92\nint64\n0%\n41.0\n39\n\n\nrace\nint64\n0%\n1.5\n1\n\n\nethnic\nfloat64\n89%\n3.8\n1.0\n\n\nage\nint64\n0%\n41.0\n44\n\n\nsex\nint64\n0%\n1.5\n2\n\n\nmarital\nint64\n0%\n3.6\n1\n\n\nownchild\nint64\n0%\n0.6\n0\n\n\nchldpres\nint64\n0%\n1.6\n0\n\n\nprcitshp\nobject\n0%\nNA\nNative, Born In US\n\n\nstate\nint64\n0%\n5.0\n5\n\n\nind02\nobject\n0%\nNA\nElementary and secondary schools (6111)\n\n\nocc2012\nint64\n0%\n3917.9\n430\n\n\nclass\nobject\n0%\nNA\nPrivate, For Profit\n\n\nunionmme\nobject\n0%\nNA\nNo\n\n\nunioncov\nobject\n7%\nNA\nNo\n\n\nlfsr94\nobject\n0%\nNA\nEmployed-At Work"
  },
  {
    "objectID": "week00/assets/creating-graphs.html#now-with-claude",
    "href": "week00/assets/creating-graphs.html#now-with-claude",
    "title": "Creating Graphs",
    "section": "Now with Claude",
    "text": "Now with Claude\nShow me an interesting scatterplot. Make it well done.\nThe Education-Earnings Gradient"
  },
  {
    "objectID": "week00/assets/creating-graphs.html#discussion-points",
    "href": "week00/assets/creating-graphs.html#discussion-points",
    "title": "Creating Graphs",
    "section": "Discussion points",
    "text": "Discussion points\nThink about this for each iteration.\n\nIs the code correct\nWhat do we like and dislike in this graph? Look carefully at all aspects. How would you change it?\nGo through the process and improve each graph to presentation quality.\n\n\nBonus\nYou made it till the end. Your bonus track is California Love"
  },
  {
    "objectID": "week00/assets/creating-graphs.html#key-findings",
    "href": "week00/assets/creating-graphs.html#key-findings",
    "title": "Creating Graphs",
    "section": "Key Findings",
    "text": "Key Findings\nThis visualization reveals several important patterns in the 2014 labor market:\n\nClear Education Gradient: Higher education levels consistently associate with higher earnings\nHours Variation: More educated workers tend to work slightly longer hours\nWithin-Group Variation: Substantial earnings variation exists within each education category\nAge Effects: Older workers (larger circles) often earn more within education levels\n\n\n\n\n\n\n\nNote\n\n\n\nThe college earnings premium shown here (47%) aligns with established labor economics research on returns to education.\n\n\nNot perfect. But amazing."
  }
]